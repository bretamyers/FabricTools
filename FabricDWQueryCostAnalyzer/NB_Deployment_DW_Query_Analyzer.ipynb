{"cells":[{"cell_type":"code","source":["CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n","CapacityMetricsDataset = 'Fabric Capacity Metrics'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"113d6600-09f0-4990-bba5-7197a6cf768a"},{"cell_type":"markdown","source":["### Get the workspace id"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5a72acf-8391-4bfd-bacc-11bee32837d0"},{"cell_type":"code","source":["workspaceId = spark.conf.get('trident.workspace.id')\n","print(f\"{workspaceId=}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6033b2c-aff0-4373-b7a1-1ee99c23b542"},{"cell_type":"code","source":["import requests, json\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}', headers=header)\n","workspaceName = response.json().get('displayName')\n","print(f'{workspaceName=}')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51396ef1-3c5f-4651-983e-0dbf377b0e34"},{"cell_type":"markdown","source":["### Create lakehouses"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a742a06-b57a-4dd7-abca-b59de79dffb4"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","\n","for lakehouseName in ['LH_SampleData', 'LH_QueryResults']:\n","    try:\n","        lakehouseItem = mssparkutils.lakehouse.create(lakehouseName, \"\", workspaceId)\n","    except Exception as e:\n","        # print(e)\n","        pass\n","\n","LH_SampleData = mssparkutils.lakehouse.get(\"LH_SampleData\", workspaceId)\n","LH_QueryResults = mssparkutils.lakehouse.get(\"LH_QueryResults\", workspaceId)\n","print(f'{LH_SampleData=}')\n","print(f'{LH_QueryResults=}')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af01c431-0b74-460d-96be-ffc21f7c3973"},{"cell_type":"markdown","source":["### Get notebook definition from github"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ac84ea7-b8a7-49b9-9028-9bb74b65590a"},{"cell_type":"code","source":["import requests, json, base64\n","url = \"https://raw.githubusercontent.com/bretamyers/FabricTools/dev/FabricDWQueryCostAnalyzer/NB_DW_Load_Cost_Analyzer.ipynb\"\n","response = requests.get(url)\n","\n","b = base64.b64encode(bytes(json.dumps(response.json()), 'utf-8')) # convert to bytes\n","base64_str = b.decode('utf-8') # convert bytes to string\n","notebookDefDict = json.loads(base64.b64decode(base64_str).decode('utf-8')) # convert base64 bytes to string and then to dictionary\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse'] = LH_QueryResults.get('id')\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_name'] = LH_QueryResults.get('displayName')\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_workspace_id'] = LH_QueryResults.get('workspaceId')\n","notebookDefBase64 = base64.b64encode(json.dumps(notebookDefDict).encode('utf-8')).decode('utf-8') # convert dictionary back to base64 string"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80bfd315-3fa8-4f47-b92e-e9b9430db456"},{"cell_type":"markdown","source":["### Create notebook from the definition"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"878a8e71-4e79-418b-83d5-f2859658fcca"},{"cell_type":"code","source":["import requests, json, time, base64\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","body = {\n","    \"displayName\": \"NB_DW_Load_Cost_Analyzer\",\n","    \"description\": \"A notebook to run data warehouse queries and capture the duration and cost of those queries.\",\n","    \"definition\": {\n","        \"format\": \"ipynb\",\n","        \"parts\": [\n","            {\n","                \"path\": \"notebook-content.py\"\n","                ,\"payload\": notebookDefBase64\n","                ,\"payloadType\": \"InlineBase64\"\n","            }\n","        ]\n","    }\n","}\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    time.sleep(int(response.headers.get('Retry-After')))\n","    for retry in range(5):\n","        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n","        print(responseStatus.text)\n","        print(responseStatus.headers)\n","        if responseStatus.headers.get('Location') is not None:\n","            # print(responseStatus.text)\n","            # print(responseStatus.json())\n","            # print(responseStatus.headers)\n","            break\n","        else:\n","            time.sleep(int(responseStatus.headers.get('Retry-After')))\n","else:\n","    print(f\"Error - {response.json()}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ccda6393-9e73-4806-b70c-bd4ed8d7d537"},{"cell_type":"markdown","source":["### Get the notebook id"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d046260-0103-4d5b-995b-163dc2973680"},{"cell_type":"code","source":["import requests, json\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","          \n","# Need to implement for pagination\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n","\n","for notebook in response.json().get('value'):\n","    if notebook.get('displayName') == 'NB_DW_Load_Cost_Analyzer':\n","        notebookId = notebook.get('id')\n","        print(f'{notebookId=}')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f377b78-e1e2-4332-a7c0-a0d2cfff5096"},{"cell_type":"markdown","source":["### Create Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd4ca523-eec3-4fb8-ae9f-3b3492d2411a"},{"cell_type":"code","source":["import requests, json, time\n","\n","body = {\"displayName\": \"WH_SampleData\"}\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    print(f'Creating Warehouse {body.get(\"displayName\")}')\n","    time.sleep(int(response.headers.get('Retry-After')))\n","    for retry in range(5):\n","        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n","        print(responseStatus.text)\n","        print(responseStatus.headers)\n","        if responseStatus.json().get('status') != 'Succeeded':\n","            time.sleep(int(responseStatus.headers.get('Retry-After')))\n","        else:\n","            if responseStatus.headers.get('Location') is None: # no result was return but operation completed\n","                break\n","            else:\n","                responseResult = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n","                print(f\"Succeeded {responseResult.json()}\")\n","                break\n","else:\n","    print(response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a07eef0-25d6-4ba6-99c7-8d73ad2f89b6"},{"cell_type":"code","source":["import requests, json, time\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header)\n","\n","for warehouse in response.json().get('value'):\n","    if warehouse.get('displayName') == 'WH_SampleData':\n","        WH_SampleData = warehouse\n","        break\n","print(WH_SampleData)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"061eb768-5f22-4ba2-b8e3-14f786ef3ded"},{"cell_type":"markdown","source":["### Load data to Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee346c67-7d7b-4f7b-ac19-ecf93d16aae7"},{"cell_type":"code","source":["import azure.storage.blob\n","import pandas as pd\n","from notebookutils import mssparkutils\n","\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite', 'LEGACY')\n","spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', 'LEGACY')\n","\n","account_url = \"https://fabrictutorialdata.blob.core.windows.net\"\n","blob_service_client = azure.storage.blob.BlobServiceClient(account_url)\n","container_client = blob_service_client.get_container_client('sampledata')\n","\n","tableList = list()\n","for blob in container_client.walk_blobs(name_starts_with='WideWorldImportersDW/parquet/tables/', delimiter='/'):\n","    tableName = blob.name.split('/')[-1].split('.parquet')[0]\n","    tableList.append(tableName)\n","\n","for table in tableList:\n","    print(f'Loading Table: {table}')\n","    if mssparkutils.fs.exists(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\"):\n","        mssparkutils.fs.rm(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\", recurse=True)\n","\n","    for blob in container_client.walk_blobs(name_starts_with=f\"WideWorldImportersDW/parquet/tables/\", delimiter='/'):\n","        if f'{table}.parquet' == blob.name.split('/')[-1]:\n","            dfPandas = pd.read_parquet(f'{account_url}/sampledata/{blob.name}', engine='pyarrow', storage_options={'anon': True})\n","\n","            if not dfPandas.empty:\n","                dfSpark = spark.createDataFrame(dfPandas)\n","                dfSpark.write.format('delta').mode('append').option('mergeSchema', \"true\").save(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\")\n","            else:\n","                print('Bad file', blob.name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"bd60b5c3-4354-4442-946d-e5988ffe91d2"},{"cell_type":"markdown","source":["### Load data to Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c484d10d-c021-409a-9016-f977d4c27732"},{"cell_type":"code","source":["from notebookutils import mssparkutils  \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n","import pyodbc, struct, itertools, time, datetime, re, uuid, json\n","\n","connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n","\n","# Use the credentials of the user executing the notebook\n","token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n","encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n","tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n","\n","def get_result_set(cursor):\n","    if cursor.description:\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","    else:\n","        resultList = []\n","        resultColumns = []\n","    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","    with conn.cursor() as cursor:\n","        while retry in range(5):\n","            cursor.execute(f\"\"\"SELECT TABLE_NAME FROM LH_SampleData.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo'\"\"\")\n","            resultSet = get_result_set(cursor)\n","            if len(resultSet) > 0:\n","                break\n","            else:\n","                time.sleep(10)\n","        while cursor.nextset():\n","            queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n","            resultSet.append(get_result_set(cursor))\n","        cursor.commit()\n","\n","    for table in resultSet:\n","        with conn.cursor() as cursor:\n","            query = f\"\"\"IF OBJECT_ID('dbo.{table.get(\"TABLE_NAME\")}', 'U') IS NOT NULL DROP TABLE dbo.{table.get(\"TABLE_NAME\")}; CREATE TABLE dbo.{table.get(\"TABLE_NAME\")} AS SELECT * FROM LH_SampleData.dbo.{table.get(\"TABLE_NAME\")}\"\"\"\n","            print(query)\n","            cursor.execute(query)\n","            cursor.commit()\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba43f623-71ed-4333-b93c-8b680d51a62e"},{"cell_type":"markdown","source":["### Refresh Fabric Capacity Metrics App so that the latest workspace artifacts are in the model"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2906366c-fc4d-465c-9a60-2e782ef4ac15"},{"cell_type":"code","source":["import requests, json, time\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n","\n","capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n","print(f'{capacityWorkspaceId = }')\n","\n","response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n","\n","datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n","print(f'{datasetId = }')\n","\n","# tableList = [{\"table\": \"Capacities\"}\n","#             ,{\"table\": \"Workspaces\"}\n","#             ,{\"table\": \"TimePoints\"}\n","#             ,{\"table\": \"TimePointBackgroundDetail\"}\n","#             ,{\"table\": \"Items\"}\n","#             ]\n","body = {\"objects\": tableList} # Need to ask Pat what tables are import and what are direct query\n","# response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header, data=json.dumps(body))\n","response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n","\n","refreshId = response.headers.get('RequestId')\n","print(f'{refreshId = }')\n","\n","for attempt in range(12): \n","    # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n","    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n","    if response.json().get('value')[0].get('status') != 'Unknown':\n","        print(f'Refresh Complete')\n","        break\n","    else:\n","        print(f'Refreshing tables ...')\n","        time.sleep(10)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"280025e5-283a-45aa-bb74-bd8ea4e77b39"},{"cell_type":"markdown","source":["### Run the notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18b9a783-80e9-4026-bb5c-9d5169b9271f"},{"cell_type":"code","source":["import requests, json, time\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","body = {\n","    \"executionData\": {\n","        \"parameters\": {\n","            \"FabricDWWorkspaceName\": {\"value\": workspaceName, \"type\": \"string\"}\n","            ,\"FabricDWName\": {\"value\": \"WH_SampleData\", \"type\": \"string\"}\n","            ,\"ConcurrencyNum\": {\"value\": \"1\", \"type\": \"int\"}\n","            ,\"CapacityMetricsWorkspace\": {\"value\": CapacityMetricsWorkspace, \"type\": \"string\"}\n","            ,\"CapacityMetricsDataset\": {\"value\": CapacityMetricsDataset, \"type\": \"string\"}\n","        }\n","    }\n","}\n","\n","# print(json.dumps(body, indent=4))\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{notebookId}/jobs/instances?jobType=RunNotebook', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    print('Started Execution of Notebook')\n","    time.sleep(10) # Sleep for 10 seconds to wait for job to get started. TODO Change in the future to a retry\n","    print(f\"{response.headers=}\")\n","    # jobId = response.headers.get('Location')\n","    while True:\n","        # print(f\"{response.headers.get('Location')=}\")\n","        responseStatus = requests.request(method=\"get\", url=response.headers.get('Location'), headers=header)\n","        print(f\"Status: {responseStatus.json().get('status')} {\"|\" + responseStatus.json().get('failureReason') if responseStatus.json().get('failureReason') != None else ''}\")\n","        if responseStatus.json().get('status') in ['Succeeded', 'Failed', 'Completed']:\n","            print(responseStatus.json().get('status'))\n","            break\n","        else:\n","            time.sleep(120) # Wait 2 minutes to check the status of the run\n","else:\n","    print('Error trying to execute')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55a76cf0-1dcb-4bc1-a7bb-b88481135831"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ddff3519-dad5-496f-80b1-77fbb15c0bf3"},{"cell_type":"code","source":["# import requests\n","\n","# response = requests.request(method=\"get\", url=f'https://api.fabric.microsoft.com/v1/workspaces/a3b40eca-afde-448e-bc54-af3d318657b0/items/d20ea153-dbf1-4821-8243-e823bf95bdbb/jobs/instances/237f073e-f403-4d12-a2ae-63377fbe0335', headers=header)\n","# print(response.status_code)\n","# print(response.json())\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd2dcdd2-b53e-47db-a40d-ee0f382bed85"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"129fe76b-c99a-47ea-b574-c0972791ff9c"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}