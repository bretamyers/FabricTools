{"cells":[{"cell_type":"code","source":["CapacityMetricsWorkspace = 'WS_Fabric_Capacity_Metrics'\n","CapacityMetricsDataset = 'Fabric Capacity Metrics'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:16:21.4192693Z","session_start_time":"2024-09-24T21:16:21.60552Z","execution_start_time":"2024-09-24T21:16:30.5787318Z","execution_finish_time":"2024-09-24T21:16:33.2620904Z","parent_msg_id":"01024669-d01c-4f4c-85eb-743385035aaf"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 3, Finished, Available, Finished)"},"metadata":{}}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"113d6600-09f0-4990-bba5-7197a6cf768a"},{"cell_type":"markdown","source":["### Get the workspace id"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f5a72acf-8391-4bfd-bacc-11bee32837d0"},{"cell_type":"code","source":["workspaceId = spark.conf.get('trident.workspace.id')\n","print(f\"{workspaceId=}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:16:21.4199993Z","session_start_time":null,"execution_start_time":"2024-09-24T21:16:33.622447Z","execution_finish_time":"2024-09-24T21:16:33.864184Z","parent_msg_id":"aa512449-77fc-4f37-9e98-7b4d47570ddc"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["workspaceId='41a063a3-ef2a-4938-8570-3f5b2331f584'\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c6033b2c-aff0-4373-b7a1-1ee99c23b542"},{"cell_type":"code","source":["import requests, json\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}', headers=header)\n","workspaceName = response.json().get('displayName')\n","print(f'{workspaceName=}')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:16:21.4206351Z","session_start_time":null,"execution_start_time":"2024-09-24T21:16:34.2235859Z","execution_finish_time":"2024-09-24T21:16:34.996883Z","parent_msg_id":"dbd69a62-e2da-476b-b15a-41fe5d8cd95b"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["workspaceName='WS_DW_Query_Cost_Test_Take_Nine'\n"]}],"execution_count":3,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51396ef1-3c5f-4651-983e-0dbf377b0e34"},{"cell_type":"markdown","source":["### Create lakehouses"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0a742a06-b57a-4dd7-abca-b59de79dffb4"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","\n","for lakehouseName in ['LH_SampleData', 'LH_QueryResults']:\n","    try:\n","        lakehouseItem = mssparkutils.lakehouse.create(lakehouseName, \"\", workspaceId)\n","    except Exception as e:\n","        # print(e)\n","        pass\n","\n","LH_SampleData = mssparkutils.lakehouse.get(\"LH_SampleData\", workspaceId)\n","LH_QueryResults = mssparkutils.lakehouse.get(\"LH_QueryResults\", workspaceId)\n","print(f'{LH_SampleData=}')\n","print(f'{LH_QueryResults=}')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"af01c431-0b74-460d-96be-ffc21f7c3973"},{"cell_type":"markdown","source":["### Get notebook definition from github"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1ac84ea7-b8a7-49b9-9028-9bb74b65590a"},{"cell_type":"code","source":["import requests, json, base64\n","url = \"https://raw.githubusercontent.com/bretamyers/FabricTools/dev/FabricDWQueryCostAnalyzer/NB_DW_Load_Cost_Analyzer.ipynb\"\n","response = requests.get(url)\n","\n","b = base64.b64encode(bytes(json.dumps(response.json()), 'utf-8')) # convert to bytes\n","base64_str = b.decode('utf-8') # convert bytes to string\n","notebookDefDict = json.loads(base64.b64decode(base64_str).decode('utf-8')) # convert base64 bytes to string and then to dictionary\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse'] = LH_QueryResults.get('id')\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_name'] = LH_QueryResults.get('displayName')\n","notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_workspace_id'] = LH_QueryResults.get('workspaceId')\n","notebookDefBase64 = base64.b64encode(json.dumps(notebookDefDict).encode('utf-8')).decode('utf-8') # convert dictionary back to base64 string"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80bfd315-3fa8-4f47-b92e-e9b9430db456"},{"cell_type":"markdown","source":["### Create notebook from the definition"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"878a8e71-4e79-418b-83d5-f2859658fcca"},{"cell_type":"code","source":["import requests, json, time, base64\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","body = {\n","    \"displayName\": \"NB_DW_Load_Cost_Analyzer\",\n","    \"description\": \"A notebook to run data warehouse queries and capture the duration and cost of those queries.\",\n","    \"definition\": {\n","        \"format\": \"ipynb\",\n","        \"parts\": [\n","            {\n","                \"path\": \"notebook-content.py\"\n","                ,\"payload\": notebookDefBase64\n","                ,\"payloadType\": \"InlineBase64\"\n","            }\n","        ]\n","    }\n","}\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    time.sleep(int(response.headers.get('Retry-After')))\n","    for retry in range(5):\n","        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n","        print(responseStatus.text)\n","        print(responseStatus.headers)\n","        if responseStatus.headers.get('Location') is not None:\n","            break\n","        else:\n","            time.sleep(int(responseStatus.headers.get('Retry-After')))\n","else:\n","    print(f\"Error - {response.json()}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ccda6393-9e73-4806-b70c-bd4ed8d7d537"},{"cell_type":"markdown","source":["### Get the notebook id"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6d046260-0103-4d5b-995b-163dc2973680"},{"cell_type":"code","source":["import requests, json\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","          \n","# Need to implement for pagination\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n","\n","for notebook in response.json().get('value'):\n","    if notebook.get('displayName') == 'NB_DW_Load_Cost_Analyzer':\n","        notebookId = notebook.get('id')\n","        print(f'{notebookId=}')\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f377b78-e1e2-4332-a7c0-a0d2cfff5096"},{"cell_type":"markdown","source":["### Create Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd4ca523-eec3-4fb8-ae9f-3b3492d2411a"},{"cell_type":"code","source":["import requests, json, time\n","\n","body = {\"displayName\": \"WH_SampleData\"}\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    print(f'Creating Warehouse {body.get(\"displayName\")}')\n","    time.sleep(int(response.headers.get('Retry-After')))\n","    for retry in range(5):\n","        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n","        print(responseStatus.text)\n","        print(responseStatus.headers)\n","        if responseStatus.json().get('status') != 'Succeeded':\n","            time.sleep(int(responseStatus.headers.get('Retry-After')))\n","        else:\n","            if responseStatus.headers.get('Location') is None: # no result was return but operation completed\n","                break\n","            else:\n","                responseResult = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n","                print(f\"Succeeded {responseResult.json()}\")\n","                break\n","else:\n","    print(response.text)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4a07eef0-25d6-4ba6-99c7-8d73ad2f89b6"},{"cell_type":"code","source":["import requests, json, time\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header)\n","\n","for warehouse in response.json().get('value'):\n","    if warehouse.get('displayName') == 'WH_SampleData':\n","        WH_SampleData = warehouse\n","        break\n","print(WH_SampleData)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:20:02.131893Z","session_start_time":null,"execution_start_time":"2024-09-24T21:20:02.51995Z","execution_finish_time":"2024-09-24T21:20:03.2832523Z","parent_msg_id":"7d11105c-3d13-4499-a043-2a62b7d494a9"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 7, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["{'id': '179a51e7-8282-4172-9e2e-63aa5e0f8a5f', 'type': 'Warehouse', 'displayName': 'WH_SampleData', 'description': '', 'workspaceId': '41a063a3-ef2a-4938-8570-3f5b2331f584', 'properties': {'connectionInfo': 'n52dzjnhphme5jslxpytuo62ni-unr2aqjk544etblqh5nsgmpvqq.datawarehouse.fabric.microsoft.com', 'connectionString': 'n52dzjnhphme5jslxpytuo62ni-unr2aqjk544etblqh5nsgmpvqq.datawarehouse.fabric.microsoft.com', 'createdDate': '2024-09-24T18:45:11.307', 'lastUpdatedTime': '2024-09-24T18:45:24.9'}}\n"]}],"execution_count":5,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"061eb768-5f22-4ba2-b8e3-14f786ef3ded"},{"cell_type":"markdown","source":["### Load data to Lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ee346c67-7d7b-4f7b-ac19-ecf93d16aae7"},{"cell_type":"code","source":["import azure.storage.blob\n","import pandas as pd\n","from notebookutils import mssparkutils\n","\n","spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite', 'LEGACY')\n","spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', 'LEGACY')\n","\n","account_url = \"https://fabrictutorialdata.blob.core.windows.net\"\n","blob_service_client = azure.storage.blob.BlobServiceClient(account_url)\n","container_client = blob_service_client.get_container_client('sampledata')\n","\n","tableList = list()\n","for blob in container_client.walk_blobs(name_starts_with='WideWorldImportersDW/parquet/tables/', delimiter='/'):\n","    tableName = blob.name.split('/')[-1].split('.parquet')[0]\n","    tableList.append(tableName)\n","\n","for table in tableList:\n","    print(f'Loading Table: {table}')\n","    if mssparkutils.fs.exists(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\"):\n","        mssparkutils.fs.rm(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\", recurse=True)\n","\n","    for blob in container_client.walk_blobs(name_starts_with=f\"WideWorldImportersDW/parquet/tables/\", delimiter='/'):\n","        if f'{table}.parquet' == blob.name.split('/')[-1]:\n","            dfPandas = pd.read_parquet(f'{account_url}/sampledata/{blob.name}', engine='pyarrow', storage_options={'anon': True})\n","\n","            if not dfPandas.empty:\n","                dfSpark = spark.createDataFrame(dfPandas)\n","                dfSpark.write.format('delta').mode('append').option('mergeSchema', \"true\").save(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\")\n","            else:\n","                print('Bad file', blob.name)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"bd60b5c3-4354-4442-946d-e5988ffe91d2"},{"cell_type":"markdown","source":["### Load data to Warehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c484d10d-c021-409a-9016-f977d4c27732"},{"cell_type":"code","source":["from notebookutils import mssparkutils  \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n","import pyodbc, struct, itertools, time, datetime, re, uuid, json\n","\n","connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n","\n","# Use the credentials of the user executing the notebook\n","token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n","encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n","tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n","\n","def get_result_set(cursor):\n","    if cursor.description:\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","    else:\n","        resultList = []\n","        resultColumns = []\n","    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","    with conn.cursor() as cursor:\n","        for retry in range(5):\n","            cursor.execute(f\"\"\"SELECT TABLE_NAME FROM LH_SampleData.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo'\"\"\")\n","            resultSet = get_result_set(cursor)\n","            if len(resultSet) == 14:\n","                break\n","            else:\n","                time.sleep(30) # wait 30 seconds for the schema sync to occur\n","        while cursor.nextset():\n","            queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n","            resultSet.append(get_result_set(cursor))\n","        cursor.commit()\n","\n","    for table in resultSet:\n","        with conn.cursor() as cursor:\n","            query = f\"\"\"IF OBJECT_ID('dbo.{table.get(\"TABLE_NAME\")}', 'U') IS NOT NULL DROP TABLE dbo.{table.get(\"TABLE_NAME\")}; CREATE TABLE dbo.{table.get(\"TABLE_NAME\")} AS SELECT * FROM LH_SampleData.dbo.{table.get(\"TABLE_NAME\")}\"\"\"\n","            print(query)\n","            cursor.execute(query)\n","            cursor.commit()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:43:00.1177512Z","session_start_time":null,"execution_start_time":"2024-09-24T21:43:00.4629311Z","execution_finish_time":"2024-09-24T21:43:21.4608182Z","parent_msg_id":"7918fa39-2966-48f3-943d-e2c9f35c09e9"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["IF OBJECT_ID('dbo.DimCity', 'U') IS NOT NULL DROP TABLE dbo.DimCity; CREATE TABLE dbo.DimCity AS SELECT * FROM LH_SampleData.dbo.DimCity\nIF OBJECT_ID('dbo.DimCustomer', 'U') IS NOT NULL DROP TABLE dbo.DimCustomer; CREATE TABLE dbo.DimCustomer AS SELECT * FROM LH_SampleData.dbo.DimCustomer\nIF OBJECT_ID('dbo.DimPaymentMethod', 'U') IS NOT NULL DROP TABLE dbo.DimPaymentMethod; CREATE TABLE dbo.DimPaymentMethod AS SELECT * FROM LH_SampleData.dbo.DimPaymentMethod\nIF OBJECT_ID('dbo.DimDate', 'U') IS NOT NULL DROP TABLE dbo.DimDate; CREATE TABLE dbo.DimDate AS SELECT * FROM LH_SampleData.dbo.DimDate\nIF OBJECT_ID('dbo.DimSupplier', 'U') IS NOT NULL DROP TABLE dbo.DimSupplier; CREATE TABLE dbo.DimSupplier AS SELECT * FROM LH_SampleData.dbo.DimSupplier\nIF OBJECT_ID('dbo.FactMovement', 'U') IS NOT NULL DROP TABLE dbo.FactMovement; CREATE TABLE dbo.FactMovement AS SELECT * FROM LH_SampleData.dbo.FactMovement\nIF OBJECT_ID('dbo.DimStockItem', 'U') IS NOT NULL DROP TABLE dbo.DimStockItem; CREATE TABLE dbo.DimStockItem AS SELECT * FROM LH_SampleData.dbo.DimStockItem\nIF OBJECT_ID('dbo.FactOrder', 'U') IS NOT NULL DROP TABLE dbo.FactOrder; CREATE TABLE dbo.FactOrder AS SELECT * FROM LH_SampleData.dbo.FactOrder\nIF OBJECT_ID('dbo.DimEmployee', 'U') IS NOT NULL DROP TABLE dbo.DimEmployee; CREATE TABLE dbo.DimEmployee AS SELECT * FROM LH_SampleData.dbo.DimEmployee\nIF OBJECT_ID('dbo.DimTransactionType', 'U') IS NOT NULL DROP TABLE dbo.DimTransactionType; CREATE TABLE dbo.DimTransactionType AS SELECT * FROM LH_SampleData.dbo.DimTransactionType\nIF OBJECT_ID('dbo.FactStockHolding', 'U') IS NOT NULL DROP TABLE dbo.FactStockHolding; CREATE TABLE dbo.FactStockHolding AS SELECT * FROM LH_SampleData.dbo.FactStockHolding\nIF OBJECT_ID('dbo.FactSale', 'U') IS NOT NULL DROP TABLE dbo.FactSale; CREATE TABLE dbo.FactSale AS SELECT * FROM LH_SampleData.dbo.FactSale\nIF OBJECT_ID('dbo.FactTransaction', 'U') IS NOT NULL DROP TABLE dbo.FactTransaction; CREATE TABLE dbo.FactTransaction AS SELECT * FROM LH_SampleData.dbo.FactTransaction\nIF OBJECT_ID('dbo.FactPurchase', 'U') IS NOT NULL DROP TABLE dbo.FactPurchase; CREATE TABLE dbo.FactPurchase AS SELECT * FROM LH_SampleData.dbo.FactPurchase\n"]}],"execution_count":9,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba43f623-71ed-4333-b93c-8b680d51a62e"},{"cell_type":"code","source":["from notebookutils import mssparkutils  \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n","import pyodbc, struct, itertools, time, datetime, re, uuid, json\n","\n","connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n","\n","# Use the credentials of the user executing the notebook\n","token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n","encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n","tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n","\n","def get_result_set(cursor):\n","    if cursor.description:\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","    else:\n","        resultList = []\n","        resultColumns = []\n","    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","    with conn.cursor() as cursor:\n","        cursor.execute('''IF OBJECT_ID('dbo.sp_Query', 'P') IS NOT NULL\n","\tDROP PROCEDURE dbo.sp_Query\n",";''')\n","        cursor.execute('''CREATE PROCEDURE dbo.sp_Query AS\n","\tSELECT COUNT(*) AS cnt_FactOrder FROM FactOrder\n","\n","    SELECT COUNT(*) AS cnt_FactMovement FROM FactMovement\n","\n","    SELECT COUNT(*) AS cnt_FactPurchase FROM FactPurchase\n","\n",";''')\n","        cursor.execute('''IF OBJECT_ID('dbo.sp_Ingest', 'P') IS NOT NULL\n","\tDROP PROCEDURE dbo.sp_Ingest\n",";''')\n","        cursor.execute('''CREATE PROCEDURE dbo.sp_Ingest AS\n","        IF OBJECT_ID('dbo.DimCity', 'U') IS NOT NULL DROP TABLE dbo.DimCity; CREATE TABLE dbo.DimCity AS SELECT * FROM LH_SampleData.dbo.DimCity;\n","\n","        SELECT COUNT(*) FROM DimCity\n",";''')\n","        cursor.commit()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"ed85e2ae-a53f-4382-a26e-f4781ab76e95","normalized_state":"finished","queued_time":"2024-09-24T21:43:31.9898532Z","session_start_time":null,"execution_start_time":"2024-09-24T21:43:32.3451964Z","execution_finish_time":"2024-09-24T21:43:33.8005733Z","parent_msg_id":"45420da0-5846-4138-9020-1c97b2e8c2c1"},"text/plain":"StatementMeta(, ed85e2ae-a53f-4382-a26e-f4781ab76e95, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c0d90134-f7c8-4a55-a79a-e32271afcda0"},{"cell_type":"markdown","source":["### Refresh Fabric Capacity Metrics App so that the latest workspace artifacts are in the model"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2906366c-fc4d-465c-9a60-2e782ef4ac15"},{"cell_type":"code","source":["import requests, json, time\n","\n","def model_refresh():\n","    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","            ,\"Content-Type\": \"application/json\"\n","            }\n","\n","    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n","\n","    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n","    print(f'{capacityWorkspaceId = }')\n","\n","    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n","\n","    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n","    print(f'{datasetId = }')\n","\n","    tableList = [{\"table\": \"Capacities\"}\n","                ,{\"table\": \"Workspaces\"}\n","                ,{\"table\": \"TimePoints\"}\n","                ,{\"table\": \"TimePointBackgroundDetail\"}\n","                ,{\"table\": \"Items\"}\n","                ]\n","    # body = {\"objects\": tableList} # Need to ask Pat what tables are import and what are direct query\n","    # body = {\"type\": \"Full\", \"objects\": tableList} # Need to ask Pat what tables are import and what are direct query\n","    # response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header, data=json.dumps(body))\n","    response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n","\n","    refreshId = response.headers.get('RequestId')\n","    print(f'{refreshId = } | {response.status_code = }')\n","\n","    if response.status_code == 202:\n","        for attempt in range(12): \n","            # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n","            response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n","            if response.status_code == 200:\n","                if response.json().get('value')[0].get('status') != 'Unknown':\n","                    print(f'Refresh Complete')\n","                    break\n","                else:\n","                    print(f'Refreshing tables ...')\n","                    time.sleep(20)\n","            else:\n","                time.sleep(10)\n","    else:\n","        print(f'Refreshed failed - {response.text}')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"280025e5-283a-45aa-bb74-bd8ea4e77b39"},{"cell_type":"code","source":["\n","import requests, math\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","            ,\"Content-Type\": \"application/json\"\n","            }\n","\n","response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n","\n","capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n","print(f'{capacityWorkspaceId = }')\n","\n","response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n","\n","datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n","print(f'{datasetId = }')\n","\n","body = {\n","  \"queries\": [\n","    {\n","      \"query\": f\"\"\"\n","        DEFINE\n","            VAR __DS0Core = \n","                SUMMARIZE('Workspaces', 'Workspaces'[WorkspaceName], 'Workspaces'[WorkspaceId])\n","        EVALUATE\n","            __DS0Core\n","    \"\"\"\n","    }\n","  ]\n","}\n","\n","response = requests.post(f'https://api.powerbi.com/v1.0/myorg/datasets/{datasetId}/executeQueries', headers=header, json=body )\n","\n","for retry in range(10):\n","    model_refresh()\n","    workspaceFound = False\n","    for results in response.json().get('results'):\n","        for table in results.get('tables'):\n","            for row in table.get('rows'):\n","                print(row.get('Workspaces[WorkspaceName]'))\n","                if row.get('Workspaces[WorkspaceName]') == workspaceName:\n","                    workspaceFound = True\n","    if workspaceFound:\n","        print('Workspace was found in the capacity metrics app model')\n","        break\n","    else:\n","        print('Workspace was not found. Waiting 5 minutes and will refresh the model and try again...')\n","        time.sleep(300)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"56e97c3e-1263-4760-ac21-d95755077b29"},{"cell_type":"markdown","source":["### Run the notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18b9a783-80e9-4026-bb5c-9d5169b9271f"},{"cell_type":"code","source":["import requests, json, time\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","body = {\n","    \"executionData\": {\n","        \"parameters\": {\n","            \"FabricDWWorkspaceName\": {\"value\": workspaceName, \"type\": \"string\"}\n","            ,\"FabricDWName\": {\"value\": \"WH_SampleData\", \"type\": \"string\"}\n","            ,\"ConcurrencyNum\": {\"value\": \"1\", \"type\": \"int\"}\n","            ,\"CapacityMetricsWorkspace\": {\"value\": CapacityMetricsWorkspace, \"type\": \"string\"}\n","            ,\"CapacityMetricsDataset\": {\"value\": CapacityMetricsDataset, \"type\": \"string\"}\n","        }\n","    }\n","}\n","\n","# print(json.dumps(body, indent=4))\n","\n","response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{notebookId}/jobs/instances?jobType=RunNotebook', headers=header, data=json.dumps(body))\n","\n","if response.status_code == 202:\n","    print('Started Execution of Notebook')\n","    time.sleep(10) # Sleep for 10 seconds to wait for job to get started. TODO Change in the future to a retry\n","    print(f\"{response.headers=}\")\n","    # jobId = response.headers.get('Location')\n","    while True:\n","        # print(f\"{response.headers.get('Location')=}\")\n","        responseStatus = requests.request(method=\"get\", url=response.headers.get('Location'), headers=header)\n","        print(f\"Status: {responseStatus.json().get('status')} | {responseStatus.json().get('failureReason') if responseStatus.json().get('failureReason') != None else ''}\")\n","        if responseStatus.json().get('status') in ['Succeeded', 'Failed', 'Completed']:\n","            print(responseStatus.json().get('status'))\n","            break\n","        else:\n","            time.sleep(120) # Wait 2 minutes to check the status of the run\n","else:\n","    print('Error trying to execute')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55a76cf0-1dcb-4bc1-a7bb-b88481135831"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ddff3519-dad5-496f-80b1-77fbb15c0bf3"},{"cell_type":"code","source":["# import requests\n","\n","# response = requests.request(method=\"get\", url=f'https://api.fabric.microsoft.com/v1/workspaces/a3b40eca-afde-448e-bc54-af3d318657b0/items/d20ea153-dbf1-4821-8243-e823bf95bdbb/jobs/instances/237f073e-f403-4d12-a2ae-63377fbe0335', headers=header)\n","# print(response.status_code)\n","# print(response.json())\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fd2dcdd2-b53e-47db-a40d-ee0f382bed85"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{}},"nbformat":4,"nbformat_minor":5}