{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94e7d5b-db72-4777-b700-297f8baaaf13",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Parameters to update\n",
    "- **FabricDWWorkspaceName**: The name of the workspace that the warehouse exists.\n",
    "- **FabricDWName**: The name of the warehouse.\n",
    "- **ConcurrnecyNum**: The number of queries that will be executings in parallel.\n",
    "- **CapacityMetricsWorkspace**: The name of the workspace that the capacity metrics semantic model exists.\n",
    "- **CapacityMetricsDataset**: The name of the capacity metrics app semenatic model.\n",
    "- **StoreQueryResults**: Flag to set if the results of the queries will be stored in the query results table.\n",
    "- **QueryRepeatCount**: Number of times a query will run (should be between 1 and 4) eg. QueryRepeatCount = 4 and queryList = [query1, query2] will become [query1, query1, query1, query1, query2, query2, query2]\n",
    "- **RunName**: The name of the run. If not specified, one will be generated with the following format. '*Run_{yyyyMMdd}_{hhmmss}*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be0399-9430-4b52-a3ab-1c911fc0d69a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FabricDWWorkspaceName = ''\n",
    "FabricDWName = 'WH_SampleData'\n",
    "ConcurrencyNum = 1 # This should be equal or less than the length of the list with the queries defined below\n",
    "CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n",
    "CapacityMetricsDataset = 'Fabric Capacity Metrics'\n",
    "StoreQueryResults = False\n",
    "QueryRepeatCount = 4 # Number of times a query will run eg. queryRepeatCount = 4, queryList = [query1, query2] will become [query1, query1, query1, query1, query2, query2, query2]\n",
    "RunName = '' # The name of the run. If not specified, one will be generated with the following format. '*Run_{yyyyMMdd}_{hhmmss}*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493545a-4048-4721-a9ef-eeed8e883a28",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "RunName = RunName if RunName else f\"Run_{datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"{FabricDWWorkspaceName=}\")\n",
    "print(f\"{FabricDWName=}\")\n",
    "print(f\"{ConcurrencyNum=}\")\n",
    "print(f\"{CapacityMetricsWorkspace=}\")\n",
    "print(f\"{CapacityMetricsDataset=}\")\n",
    "print(f\"{StoreQueryResults=}\")\n",
    "print(f\"{RunName=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b5a5a-dfa7-43b0-b8a7-b4b3849a7559",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "queryList = [\n",
    "    # Transaction\n",
    "    'SELECT COUNT(*) FROM FactTransaction'\n",
    "    ,'''SELECT\tCOUNT(*)\n",
    "FROM\tFactTransaction AS ft\n",
    "JOIN\tDimDate AS d\n",
    "ON\t\td.Date = ft.DateKey\n",
    "JOIN\tDimPaymentMethod AS pm\n",
    "ON\t\tpm.PaymentMethodKey= ft.PaymentMethodKey\n",
    "JOIN\tDimTransactionType AS tt\n",
    "ON\t\ttt.TransactionTypeKey = ft.TransactionTypeKey\n",
    "JOIN\tDimSupplier AS s\n",
    "ON\t\ts.SupplierKey = ft.SupplierKey\n",
    "JOIN\tDimCustomer AS cu\n",
    "ON\t\tcu.CustomerKey = ft.CustomerKey\n",
    "JOIN\tDimCustomer AS cuBill\n",
    "ON\t\tcuBill.CustomerKey = ft.BillToCustomerKey'''\n",
    "    # Order\n",
    "    ,'SELECT COUNT(*) cnt_FactOrder FROM FactOrder'\n",
    "    ,'''SELECT\tCOUNT(*)\n",
    "FROM\tFactOrder AS fo\n",
    "JOIN\tDimDate AS dOrder\n",
    "ON\t\tdOrder.Date = fo.OrderDateKey\n",
    "LEFT JOIN\tDimDate AS dPicked\n",
    "ON\t\tdPicked.Date = fo.PickedDateKey\n",
    "JOIN\tDimStockItem AS si\n",
    "ON\t\tsi.StockItemKey = fo.StockItemKey\n",
    "JOIN\tDimCity AS c\n",
    "ON\t\tc.CityKey = fo.CityKey\n",
    "JOIN\tDimEmployee AS e\n",
    "ON\t\te.EmployeeKey = fo.SalespersonKey\n",
    "JOIN\tDimEmployee AS ePicker\n",
    "ON\t\tePicker.EmployeeKey = fo.PickerKey\n",
    "JOIN\tDimCustomer AS cu\n",
    "ON\t\tcu.CustomerKey = fo.CustomerKey'''\n",
    "    # FactMovement\n",
    "    ,'SELECT COUNT(*) cnt_FactMovement FROM FactMovement'\n",
    "    ,'''SELECT\tCOUNT(*)\n",
    "FROM\tFactMovement AS fm\n",
    "JOIN\tDimDate AS d\n",
    "ON\t\td.Date = fm.DateKey\n",
    "JOIN\tDimStockItem AS si\n",
    "ON\t\tsi.StockItemKey = fm.StockItemKey\n",
    "JOIN\tDimTransactionType AS tt\n",
    "ON\t\ttt.TransactionTypeKey = fm.TransactionTypeKey\n",
    "JOIN\tDimSupplier AS s\n",
    "ON\t\ts.SupplierKey = fm.SupplierKey\n",
    "JOIN\tDimCustomer AS c\n",
    "ON\t\tc.CustomerKey = fm.CustomerKey'''\n",
    "    # FactPurchase\n",
    "    ,'SELECT COUNT(*) cnt_FactPurchase FROM FactPurchase'\n",
    "    ,'''SELECT\tCOUNT(*)\n",
    "FROM\tFactPurchase AS fp\n",
    "JOIN\tDimDate AS d\n",
    "ON\t\td.Date = fp.DateKey\n",
    "JOIN\tDimStockItem AS si\n",
    "ON\t\tsi.StockItemKey = fp.StockItemKey\n",
    "JOIN\tDimSupplier AS s\n",
    "ON\t\ts.SupplierKey = fp.SupplierKey'''\n",
    "    # StockHolding\n",
    "    ,'SELECT COUNT(*) AS cnt_FactStockHolding FROM FactStockHolding'\n",
    "    ,'''SELECT\tCOUNT(*)\n",
    "FROM\tFactStockHolding AS fsh\n",
    "JOIN\tDimStockItem AS si\n",
    "ON\t\tsi.StockItemKey = fsh.StockItemKey'''\n",
    "    ,'''SELECT\tTOP 100 fsh.*\n",
    "FROM\tFactStockHolding AS fsh\n",
    "JOIN\tDimStockItem AS si\n",
    "ON\t\tsi.StockItemKey = fsh.StockItemKey'''\n",
    "    # Stored Procedures\n",
    "    ,'''EXEC sp_Ingest'''\n",
    "    ,'''EXEC sp_Query'''\n",
    "    # Query with multiple statements\n",
    "    ,'''IF OBJECT_ID('dbo.DimDate', 'U') IS NOT NULL DROP TABLE dbo.DimDate; CREATE TABLE dbo.DimDate AS SELECT * FROM LH_SampleData.dbo.DimDate'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ed022-d969-45cd-8fd2-93b34357f16f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "executorCoreCnt = int(spark.conf.get('spark.executor.cores', '0'))\n",
    "executorInstances = len(spark._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "maxConcurrency = ConcurrencyNum if ConcurrencyNum < (executorCoreCnt * executorInstances) else (executorCoreCnt * executorInstances)\n",
    "\n",
    "# Adding queries to the queryList so that each queries executes 4 times sequentially.\n",
    "# Example, original queryList = [query_1, query_2] becomes [query_1, query_1, query_1, query_1, query_2, query_2, query_2, query_2]\n",
    "queryListWithRepeat = []\n",
    "for query in queryList:\n",
    "    queryListWithRepeat.extend([query] * QueryRepeatCount)\n",
    "\n",
    "# Multipling the number of queries by the maxConcurrency to create a pool of queries to be executed.\n",
    "# This will make it so that each number of concurrency will execute every query in the queryList.\n",
    "# Eg. If the queryList has 10 queries and the maxConcurrency is 5, then the queryPool will have 50 queries.\n",
    "queryPool = queryListWithRepeat * maxConcurrency\n",
    "\n",
    "rddQueries = sc.parallelize(queryPool, maxConcurrency)\n",
    "rddQueriesWithId = rddQueries.zipWithUniqueId().map(lambda x: [x[1], (x[1], x[0])] )\n",
    "rddQueriesWithId = rddQueriesWithId.partitionBy(maxConcurrency, lambda k: k ) \n",
    "print(rddQueriesWithId.glom().map(len).collect())  # Check the length of each partition to check for even distribution of rows in the partitions. This will tell us if the number of queries are evenly distributed\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<p><span style=\"font-size:20px;\"><strong>Max concurrency of spark session is </strong><i><strong>{executorCoreCnt * executorInstances}</strong></i></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>Defined concurrency is </strong><i><strong>{ConcurrencyNum}</strong></i></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>Will run </strong><i><strong>{maxConcurrency}</strong></i><strong> queries concurrently for this spark session</strong></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>A total of <i>{len(queryPool)}</i> queries will be executed (queryList size <i>{len(queryList)}</i> * max concurrency <i>{maxConcurrency}</i> * query repeat count <i>{QueryRepeatCount}</i>)</strong></span></p>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f726386-0439-4ceb-9f96-b6f562e01135",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "while True:\n",
    "    workspaceFound = False\n",
    "    for workspace in response.json().get('value'):\n",
    "        if workspace.get('displayName') == FabricDWWorkspaceName:\n",
    "            fabricDWWorkspaceId = workspace.get('id')\n",
    "            workspaceFound = True\n",
    "            break\n",
    "    \n",
    "    if workspaceFound:\n",
    "        break\n",
    "    elif workspaceFound == False and response.json().get('continuationToken'):\n",
    "        responseStatus = requests.request(method='get', url=response.json().get('continuationUri'), headers=header)\n",
    "    else:\n",
    "        print(f\"Workspace was not found and no contination token found - {response.json()}\")\n",
    "        break\n",
    "\n",
    "print(f'{fabricDWWorkspaceId = }\\n{FabricDWWorkspaceName = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c86e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/items', headers=header)\n",
    "\n",
    "for item in response.json().get('value'):\n",
    "    if item.get('displayName') == FabricDWName:\n",
    "        itemType = item.get('type')\n",
    "        \n",
    "print(f'{itemType = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd34b-c182-47ff-8d87-d8fdd550ad3e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n",
    "workspaceName = response.json().get('displayName')\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n",
    "capacityId = response.json().get('capacityId')\n",
    "capacityRegion = response.json().get('capacityRegion')\n",
    "capacityName = response.json().get('displayName')\n",
    "capacitySku = 'F0' #Default value of F0\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/capacities', headers=header)\n",
    "for capacity in response.json().get('value'):\n",
    "    if capacity.get('id') == capacityId:\n",
    "        capacitySku = capacity.get('sku')\n",
    "\n",
    "if itemType == 'Lakehouse':\n",
    "    response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/lakehouses', headers=header)\n",
    "    warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == FabricDWName][0]\n",
    "    fabricDWServer = warehouse.get('properties').get('sqlEndpointProperties').get('connectionString')\n",
    "else:\n",
    "    response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/warehouses', headers=header)\n",
    "    warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == FabricDWName][0]\n",
    "    fabricDWServer = warehouse.get('properties').get('connectionString')\n",
    "    \n",
    "warehouseId = warehouse.get('id')\n",
    "\n",
    "print(f'{warehouseId = }\\n{fabricDWServer = }\\n{workspaceName = }\\n{capacityId = }\\n{capacityRegion = }\\n{capacityName = }\\n{capacitySku = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509449a-48a5-4da1-a7c7-43a75ecd7325",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "response = requests.request(method='get', url=\"https://prices.azure.com/api/retail/prices?$filter=skuName eq 'Fabric Capacity'\", headers=header)\n",
    "for capacity in response.json().get('Items'):\n",
    "    if capacity.get('armRegionName') == capacityRegion.replace(' ', '').lower():\n",
    "        costReserved = capacity.get('retailPrice') / 12 / 730 / 60 / 60 # get the amount per CU second\n",
    "        costPayGo = costReserved / (156.334/262.80) # constant saving of ~41%. 156.334 is the resevered price of a region. 262.80 is the paygo price of a region\n",
    "print(f'{costReserved = :.10f}\\n{costPayGo = :.10f}') # per CU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bcecd-ac6c-45bf-9ebb-66fe8949ab54",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Define the queries to be executed. These are single line queries so use /* */ for commenting out code vs --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4596a-e954-4ccc-aa49-0d32c1500013",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={fabricDWServer};Database={FabricDWName};APP=QueryCostAnalyzer'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "runId = str(uuid.uuid4()).upper()\n",
    "\n",
    "print(f'{runId = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1633b8f-665d-4393-a63c-174effbb1ba9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''SELECT @@VERSION AS DWVersion\n",
    "                            ,@@SERVERNAME AS ServerGuid\n",
    "                            ,DB_NAME() AS DWName\n",
    "        ''')\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = [column[0] for column in cursor.description]\n",
    "        cursor.commit()\n",
    "        resultSet = [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "        cursor.execute(f'''SELECT [is_vorder_enabled] AS IsVOrderEnabled, [data_lake_log_publishing_desc] AS DataLakeLogPublishingDesc\n",
    "                            ,[data_lake_log_publishing] AS DataLakeLogPublishing, [create_date] AS DWCreateDate, [compatibility_level] AS CompatibilityLevel\n",
    "                            FROM sys.databases \n",
    "                            WHERE [name] = '{FabricDWName}'\n",
    "            ''')\n",
    "\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = [column[0] for column in cursor.description]\n",
    "        cursor.commit()\n",
    "        resultSet = resultSet[0] | [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList][0]\n",
    "        df = spark.createDataFrame([resultSet])\n",
    "\n",
    "dfRun = (df.withColumn('RunStartDateTimeUTC', F.lit(None).cast(TimestampType()))\n",
    "            .withColumn('RunStartTimeEpochMS', F.lit(None).cast(LongType()))\n",
    "            .withColumn('RunName', F.lit(RunName).cast(StringType()))\n",
    "            .withColumn('RunId', F.lit(runId).cast(StringType()))\n",
    "            .withColumn('DWConnectionString', F.lit(fabricDWServer).cast(StringType()))\n",
    "            .withColumn('QueriesExecutedCnt', F.lit(len(queryPool)).cast(IntegerType()))\n",
    "            .withColumn('RunConcurrency', F.lit(maxConcurrency).cast(IntegerType()))\n",
    "            .withColumn('QueryRepeatCount', F.lit(QueryRepeatCount).cast(IntegerType()))\n",
    "            .withColumn('StoreQueryResults', F.lit(StoreQueryResults).cast(StringType()))\n",
    "\n",
    "            .withColumn('DWGuid', F.lit(warehouseId).cast(StringType())) \n",
    "            .withColumn('WorkspaceName', F.lit(workspaceName).cast(StringType())) \n",
    "            .withColumn('WorkspaceGuid', F.lit(fabricDWWorkspaceId).cast(StringType())) \n",
    "            .withColumn('CapacityName', F.lit(capacityName).cast(StringType())) \n",
    "            .withColumn('CapacityGuid', F.lit(capacityId).cast(StringType())) \n",
    "            .withColumn('CapacitySKU', F.lit(capacitySku).cast(StringType())) \n",
    "            .withColumn('CapacityRegion', F.lit(capacityRegion).cast(StringType())) \n",
    "\n",
    "            .withColumn('RunEndDatetimeUTC', F.lit(None).cast(TimestampType()))\n",
    "            .withColumn('RunEndTimeEpochMS', F.lit(None).cast(LongType()))\n",
    "            .withColumn('RunDurationMS', F.lit(None).cast(LongType()))\n",
    "            .withColumn('RunCUSeconds', F.lit(None).cast(DoubleType()))\n",
    "            .withColumn('RunCostPayGo', F.lit(None).cast(DoubleType()))\n",
    "            .withColumn('RunCostReserved', F.lit(None).cast(DoubleType()))\n",
    "            \n",
    "            .withColumn('CapacityDailyCUSeconds', F.lit(60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ))\n",
    "            .withColumn('CapacityDailyCostPayGo', F.lit(costPayGo * 60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ))\n",
    "            .withColumn('CapacityDailyCostReserved', F.lit(costReserved * 60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ))\n",
    "        )\n",
    "\n",
    "dfRunOrdered = dfRun.select(\"RunName\", \"RunId\", \"DWConnectionString\", \"QueriesExecutedCnt\", \"RunConcurrency\", \"QueryRepeatCount\", \"StoreQueryResults\", \"ServerGuid\"\n",
    "        , \"DWGuid\", \"DWName\", \"DWVersion\", \"WorkspaceName\", \"WorkspaceGuid\", \"CapacityName\", \"CapacityGuid\", \"CapacitySKU\", \"CapacityRegion\", \"CompatibilityLevel\"\n",
    "        , \"DWCreateDate\", \"DataLakeLogPublishing\", \"DataLakeLogPublishingDesc\", \"IsVOrderEnabled\"\n",
    "        , \"RunStartDateTimeUTC\", \"RunStartTimeEpochMS\", \"RunEndDatetimeUTC\", \"RunEndTimeEpochMS\", \"RunDurationMS\", \"RunCUSeconds\"\n",
    "        , \"RunCostPayGo\", \"RunCostReserved\", \"CapacityDailyCUSeconds\", \"CapacityDailyCostPayGo\", \"CapacityDailyCostReserved\"\n",
    "    )\n",
    "\n",
    "if spark.catalog.tableExists(\"RunResults\"):\n",
    "\n",
    "    dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n",
    "    \n",
    "    (dtRunResults.alias('t')\n",
    "        .merge(dfRunOrdered.alias('s')\n",
    "            ,f't.runId = s.RunId'\n",
    "            )\n",
    "        .whenNotMatchedInsertAll()\n",
    "    ).execute() \n",
    "else:\n",
    "    dfRunOrdered.write.format('delta').mode('append').saveAsTable('RunResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a25284-d19a-4120-b2bf-9326efd16033",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pyodbc \n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultRowCnt = len(resultList)\n",
    "        resultColumns = []\n",
    "        if StoreQueryResults:\n",
    "            resultColumns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "        resultRowCnt = 0\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList], resultRowCnt\n",
    "\n",
    "def execute_query(iterator):\n",
    "    queryMetrics = []\n",
    "    for queryInfo in iterator:\n",
    "        queryIndex = queryInfo[1][0]\n",
    "        queryStatement = queryInfo[1][1]\n",
    "        with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                queryStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "                startTime = int(round(time.time() * 1000))\n",
    "\n",
    "                cursor.execute(queryStatement)\n",
    "                \n",
    "                queryMessage = str(cursor.messages) if cursor.messages else \"\"\n",
    "                resultSetList = list()\n",
    "                resultRowCntList = list()\n",
    "                resultSet, resultRowCnt = get_result_set(cursor)\n",
    "                resultSetList.append(resultSet)\n",
    "                resultRowCntList.append(resultRowCnt)\n",
    "\n",
    "                while cursor.nextset():\n",
    "                    queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n",
    "                    resultSet, resultRowCnt = get_result_set(cursor)\n",
    "                    resultSetList.append(resultSet)\n",
    "                    resultRowCntList.append(resultRowCnt)\n",
    "                    \n",
    "                endTime = int(round(time.time() * 1000))\n",
    "                queryEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "                \n",
    "                statementId = ','.join(re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n",
    "                queryHash = ','.join(re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage)) if re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage) else \"\"\n",
    "                distributionRequestId = ','.join(re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n",
    "                resultSetJsonString = json.dumps(resultSetList)\n",
    "\n",
    "                cursor.commit()\n",
    "\n",
    "                queryId = str(uuid.uuid4()).upper()\n",
    "                queryMetrics.append([runId, queryId, queryStatement, queryStartDateTimeUTC, queryEndDateTimeUTC\n",
    "                        ,queryMessage, startTime, endTime, endTime - startTime\n",
    "                        ,statementId, queryHash, distributionRequestId, resultSetJsonString, resultRowCntList\n",
    "                        ])\n",
    "    return queryMetrics\n",
    "\n",
    "queriesExecuted = rddQueriesWithId.mapPartitions(execute_query)\n",
    "\n",
    "runStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "runStartTimeEpoch = int(runStartDateTimeUTC.timestamp()*1000)\n",
    "\n",
    "queriesExecuted.cache().count()\n",
    "\n",
    "runEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "runEndTimeEpoch = int(runEndDateTimeUTC.timestamp()*1000)\n",
    "\n",
    "dfQueriesExecuted = queriesExecuted.toDF(schema=StructType([\n",
    "    StructField(\"RunId\", StringType(), False),\n",
    "    StructField(\"QueryId\", StringType(), False),\n",
    "    StructField(\"QueryStatement\", StringType(), False),\n",
    "    StructField(\"QueryStartDateTimeUTC\", TimestampType(), False),\n",
    "    StructField(\"QueryEndDateTimeUTC\", TimestampType(), False),\n",
    "    StructField(\"ReturnMessage\", StringType(), False),\n",
    "    StructField(\"QueryStartTimeEpochMS\", LongType(), False),\n",
    "    StructField(\"QueryEndTimeEpochMS\", LongType(), False),\n",
    "    StructField(\"QueryDurationMS\", LongType(), False),\n",
    "    StructField(\"StatementId\", StringType(), False),\n",
    "    StructField(\"QueryHash\", StringType(), False),\n",
    "    StructField(\"DistributionRequestId\", StringType(), False),\n",
    "    StructField(\"ResultSet\", StringType(), False),\n",
    "    StructField(\"ResultRowCnt\", StringType(), False)\n",
    "    ]))\n",
    "\n",
    "dfFinal = dfQueriesExecuted.withColumn('QueryCUSeconds', F.lit(None).cast(DoubleType())).withColumn('QueryCostPayGo', F.lit(None).cast(DoubleType())).withColumn('QueryCostReserved', F.lit(None).cast(DoubleType()))\n",
    "dfFinal.write.format('delta').mode('append').saveAsTable('QueryResults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf916df-8027-4011-a8ec-dd3fedc1c2b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "statementList = spark.sql(f'SELECT ARRAY_JOIN(COLLECT_SET(CONCAT(\"\\\\\"\", StatementId, \"\\\\\"\")), \", \") AS Statements FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('Statements')\n",
    "# We have to explode by statement ids since a sql query may have multiple queries within it\n",
    "queriesExecutedCnt = spark.sql(f'SELECT COUNT(StatementId) AS QueryCnt FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('QueryCnt') \n",
    "print(f'{runId = }\\n{statementList = }\\n{queriesExecutedCnt = }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44baaf-dc59-433f-8b7e-c5a2a124f3fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n",
    "\n",
    "df_final = spark.createDataFrame(data=[(runStartDateTimeUTC, runStartTimeEpoch, runEndDateTimeUTC, runEndTimeEpoch, )], schema=['runStartDateTimeUTC', 'runStartTimeEpochMS', 'RunEndDateTimeUTC', 'RunEndTimeEpochMS'])\n",
    "\n",
    "(dtRunResults.alias('t')\n",
    "    .merge(df_final.alias('s')\n",
    "        ,f't.RunId = \"{runId}\"'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'RunStartDateTimeUTC': 's.RunStartDateTimeUTC'\n",
    "        ,'RunStartTimeEpochMS': 's.RunStartTimeEpochMS'\n",
    "        ,'RunEndDatetimeUTC': 's.RunEndDateTimeUTC'\n",
    "        ,'RunEndTimeEpochMS': 's.RunEndTimeEpochMS'\n",
    "        ,'RunDurationMS': 's.RunEndTimeEpochMS - s.RunStartTimeEpochMS'\n",
    "        }\n",
    "        )\n",
    ").execute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fd036-6768-4fe9-b1f9-fea79ed5e7b0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, datetime\n",
    "from pyspark.sql.functions import to_timestamp, lit, col\n",
    "\n",
    "def get_capacity_metrics_usage(time_point:datetime, operation_id_list:str):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Items[ItemId]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"Items[ItemKind]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"Items[ItemName]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationStartTime]\", \tStringType(), \tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationEndTime]\", \t\tStringType(), \tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationId]\", \t\t\tStringType(), \t\tTrue),\n",
    "        StructField(\"[Sum_CUs]\", \t\tDoubleType(), \t\tTrue),\n",
    "        StructField(\"[Sum_Duration]\", IntegerType(), \t\tTrue)\n",
    "    ])\n",
    "\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "                ,\"Content-Type\": \"application/json\"\n",
    "                }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "    \n",
    "    body = {\n",
    "        \"queries\": [\n",
    "        {\n",
    "            \"query\": f\"\"\"\n",
    "                DEFINE\n",
    "                    MPARAMETER 'CapacityID' \t= \"{capacityId}\"\n",
    "                    MPARAMETER 'TimePoint' \t\t= (DATE({time_point.year}, {time_point.month}, {time_point.day}) + TIME({time_point.hour}, {time_point.minute}, {time_point.second}))\n",
    "\n",
    "                    VAR __Var_CapacityId\t= {{\"{capacityId}\"}}\n",
    "                    VAR __Var_OperationId\t= {{{statementList}}}\n",
    "\n",
    "                    VAR __Filter_OperationId \t= TREATAS(__Var_OperationId, 'TimePointBackgroundDetail'[OperationId])\n",
    "                    VAR __Filter_CapacityId \t= TREATAS(__Var_CapacityId, 'Capacities'[capacityId])\n",
    "\n",
    "                    VAR OperationCUs = \n",
    "                        SUMMARIZECOLUMNS(\n",
    "                            'Items'[ItemId],\n",
    "                            'Items'[ItemKind],\n",
    "                            'Items'[ItemName],\n",
    "                            'TimePointBackgroundDetail'[OperationStartTime],\n",
    "                            'TimePointBackgroundDetail'[OperationEndTime],\n",
    "                            'TimePointBackgroundDetail'[OperationId],\n",
    "                            __Filter_OperationId,\n",
    "                            __Filter_CapacityId,\n",
    "                            \"Sum_CUs\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n",
    "                            \"Sum_Duration\", CALCULATE(SUM('TimePointBackgroundDetail'[Duration (s)]))\n",
    "                        )\n",
    "\n",
    "                EVALUATE\n",
    "                    OperationCUs\n",
    "                \"\"\"\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'https://api.powerbi.com/v1.0/myorg/datasets/{datasetId}/executeQueries', headers=header, json=body )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        rowsList = response.json()[\"results\"][0][\"tables\"][0][\"rows\"]\n",
    "        \n",
    "        df_dax = spark.createDataFrame(data=rowsList, schema=schema)\n",
    "        df_dax = df_dax.select('*', to_timestamp('TimePointBackgroundDetail[OperationStartTime]'), to_timestamp('TimePointBackgroundDetail[OperationEndTime]'))\n",
    "        \n",
    "        df = (df_dax.withColumn(\"TimePoint\", lit(time_point)).select(\n",
    "            col(\"TimePoint\")\n",
    "            ,col(\"Items[ItemId]\").alias(\"ItemId\")\n",
    "            ,col(\"Items[ItemKind]\").alias(\"ItemKind\")\n",
    "            ,col(\"Items[ItemName]\").alias(\"ItemName\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationStartTime]\").alias(\"StartTime\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationEndTime]\").alias(\"EndTime\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationId]\").alias(\"OperationId\")\n",
    "            ,col(\"[Sum_CUs]\").cast(DoubleType()).alias(\"Sum_CUs\")\n",
    "            ,col(\"[Sum_Duration]\").cast(IntegerType()).alias(\"Sum_Duration\"))\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f'{response.json()}\\nCheck that user has as least contributor access to the workspace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "def model_refresh():\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "\n",
    "    response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n",
    "\n",
    "    refreshId = response.headers.get('RequestId')\n",
    "    print(f'{refreshId = } | {response.status_code = }')\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        for attempt in range(12): \n",
    "            # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n",
    "            response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n",
    "            if response.status_code == 200:\n",
    "                if response.json().get('value')[0].get('status') != 'Unknown':\n",
    "                    print(f'Refresh Complete')\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Refreshing tables ...')\n",
    "                    time.sleep(20)\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "    else:\n",
    "        print(f'Refreshed failed - {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68bfa2-1f97-47af-a21c-230459507da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Get the capacity usage for the queries that were executed.\n",
    "This ensures that the background operations are captured no matter what time they are run as they are smoothed over a 24 hour time period.\n",
    "Next, filter that down to the distinct records. This is necessary because a record may show up in the today and tomorrow datasets depending on the time it was run.\n",
    "Nest, aggregate the records into a single record. This is necessary becuase some operations will have two entries, one under the executing user and one under the user \"System\".\n",
    "'''\n",
    "from pyspark.sql.functions import min, max, sum\n",
    "\n",
    "# # Continues to query the metrica app to get the data. Data can delayed by a few minutes.\n",
    "# # We retry every minute until 15 minutes has passed.\n",
    "for retryCnt in range(15):\n",
    "    df_today = get_capacity_metrics_usage(runStartDateTimeUTC, statementList)\n",
    "    df_tomorrow = get_capacity_metrics_usage(runStartDateTimeUTC + datetime.timedelta(hours = 23), statementList)\n",
    "    df_all_days = df_today.unionAll(df_tomorrow)\n",
    "    df_count = df_all_days.select('OperationId').distinct()\n",
    "    \n",
    "    print(f'{df_count.count()} statements of the {queriesExecutedCnt} that have been found in the capacity metrics model. ', end='')\n",
    "    if df_count.count() == queriesExecutedCnt:\n",
    "        df_final = df_all_days.groupBy('ItemId', 'ItemKind', 'ItemName', 'OperationId').agg(min(\"StartTime\").alias(\"StartTime\"), max(\"EndTime\").alias(\"EndTime\"), sum(\"Sum_CUs\").alias(\"QueryCUSeconds\"), sum(\"Sum_Duration\").alias(\"SumDuration\"))\n",
    "        break\n",
    "    else:\n",
    "        if retryCnt%5 == (5-1):\n",
    "            print('Refreshing the capacity metircs app semantic model...')\n",
    "            model_refresh()\n",
    "            continue\n",
    "        print('Sleeping for a minute...')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffd41b-12e4-4c5a-aeae-0dccf30add3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfQueryResults = spark.table(\"QueryResults\")\n",
    "\n",
    "dfQueryResultsCleansed = (dfQueryResults.join(df_final, dfQueryResults.StatementId.contains(df_final.OperationId)) \n",
    "    .filter(dfQueryResults.RunId == runId) \n",
    "    .groupBy(dfQueryResults.RunId, dfQueryResults.StatementId) \n",
    "    .agg(sum(df_final.QueryCUSeconds).alias(\"QueryCUSeconds\"), sum(df_final.SumDuration).alias(\"SumDuration\")\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa53bc-c865-491d-881a-3206e8cfbfad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Update the QueryResults table with the CUSeconds and QueryCost derived from the Capacity Metrics App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c6bdf-7984-489a-b3bf-c3de6080aa60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtQueryResults = DeltaTable.forName(spark, \"QueryResults\")\n",
    "\n",
    "(dtQueryResults.alias('t')\n",
    "    .merge(dfQueryResultsCleansed.alias('s')\n",
    "        ,f't.runId = s.RunId AND t.StatementId = s.StatementId'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'QueryCUSeconds': 's.QueryCUSeconds'\n",
    "        ,'QueryCostPayGo': f's.QueryCUSeconds * {costPayGo}'\n",
    "        ,'QueryCostReserved': f's.QueryCUSeconds * {costReserved}'\n",
    "        }\n",
    "        )\n",
    ").execute() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343c8b6-52c9-414d-8bf7-962982409336",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Update the RunResults table with cost of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12730f91-3044-4f4c-908c-87d657f3ace5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n",
    "dtRunResultsCleansed = spark.sql(f'SELECT SUM(COALESCE(QueryCUSeconds, 0)) AS RunCUSeconds, SUM(COALESCE(QueryCostPayGo, 0)) AS RunCostPayGo, SUM(COALESCE(QueryCostReserved, 0)) AS RunCostReserved FROM QueryResults WHERE RunId = \"{runId}\"')\n",
    "\n",
    "(dtRunResults.alias('t')\n",
    "    .merge(dtRunResultsCleansed.alias('s')\n",
    "        ,f't.RunId = \"{runId}\"'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'RunCUSeconds': 's.RunCUSeconds'\n",
    "        ,'RunCostPayGo': f's.RunCUSeconds * {costPayGo}'  # This could be different looking at the tables separately due to rounding\n",
    "        ,'RunCostReserved': f's.RunCUSeconds * {costReserved}'  # This could be different looking at the tables separately due to rounding\n",
    "        }\n",
    "        )\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081661ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM RunResults WHERE RunId = '{runId}'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql(f\"SELECT * FROM QueryResults WHERE RunId = '{runId}'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff438663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)\n",
    "\n",
    "for tableName in ['QueryResults', 'RunResults']:\n",
    "    dt = DeltaTable.forName(spark, tableName)\n",
    "    dt.optimize().executeCompaction()\n",
    "    dt.vacuum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487dfa72-4492-47c5-b5c3-b1fbc595f985",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-09-26T16:59:25.702326Z",
       "execution_start_time": "2024-09-26T16:59:22.4859083Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b17ae74c-52ae-48ab-96be-2631ef23ef27",
       "queued_time": "2024-09-26T16:59:11.5504861Z",
       "session_id": "56ed4fc1-00c2-4248-bba5-123781108efd",
       "session_start_time": "2024-09-26T16:59:11.7989781Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 56ed4fc1-00c2-4248-bba5-123781108efd, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n",
       "<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n",
       "<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n",
       "\t,s.login_name\n",
       "\t,r.[session_id]\n",
       "\t,r.start_time\n",
       "\t,r.STATUS\n",
       "\t,r.total_elapsed_time\n",
       "\t,r.command\n",
       "\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n",
       "\t\tWHEN r.[statement_start_offset] > 0\n",
       "\t\t\tTHEN\n",
       "\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n",
       "\t\t\t\tCASE r.[statement_end_offset]\n",
       "\t\t\t\t\tWHEN - 1\n",
       "\t\t\t\t\t\tTHEN\n",
       "\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
       "\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n",
       "\t\t\t\t\tELSE\n",
       "\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
       "\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n",
       "\t\t\t\t\tEND\n",
       "\t\tELSE\n",
       "\t\t\t/* 1st part of full command is running */\n",
       "\t\t\tCASE r.[statement_end_offset]\n",
       "\t\t\t\tWHEN - 1\n",
       "\t\t\t\t\tTHEN\n",
       "\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
       "\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n",
       "\t\t\t\tELSE\n",
       "\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
       "\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n",
       "\t\t\t\tEND\n",
       "\t\tEND AS [executing_statement]\n",
       "\t,t.[text] AS [parent_batch]\n",
       "\t,s.[program_name]\n",
       "\t,r.query_hash\n",
       "\t,r.query_plan_hash\n",
       "\t,r.dist_statement_id\n",
       "\t,r.[label]\n",
       "\t,s.client_interface_name\n",
       "\t,r.[sql_handle]\n",
       "\t,c.client_net_address\n",
       "\t,c.connection_id\n",
       "FROM\tsys.dm_exec_requests r\n",
       "CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n",
       "JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n",
       "JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n",
       "JOIN\tsys.databases d ON d.database_id = r.database_id\n",
       "WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n",
       "AND\tr.session_id <> @@SPID\n",
       "AND\ts.program_name NOT IN ('QueryInsights','DMS')\n",
       "</pre></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML(f\"\"\"<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n",
    "<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n",
    "<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n",
    "\t,s.login_name\n",
    "\t,r.[session_id]\n",
    "\t,r.start_time\n",
    "\t,r.STATUS\n",
    "\t,r.total_elapsed_time\n",
    "\t,r.command\n",
    "\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n",
    "\t\tWHEN r.[statement_start_offset] > 0\n",
    "\t\t\tTHEN\n",
    "\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n",
    "\t\t\t\tCASE r.[statement_end_offset]\n",
    "\t\t\t\t\tWHEN - 1\n",
    "\t\t\t\t\t\tTHEN\n",
    "\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
    "\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n",
    "\t\t\t\t\tELSE\n",
    "\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
    "\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n",
    "\t\t\t\t\tEND\n",
    "\t\tELSE\n",
    "\t\t\t/* 1st part of full command is running */\n",
    "\t\t\tCASE r.[statement_end_offset]\n",
    "\t\t\t\tWHEN - 1\n",
    "\t\t\t\t\tTHEN\n",
    "\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
    "\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n",
    "\t\t\t\tELSE\n",
    "\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
    "\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n",
    "\t\t\t\tEND\n",
    "\t\tEND AS [executing_statement]\n",
    "\t,t.[text] AS [parent_batch]\n",
    "\t,s.[program_name]\n",
    "\t,r.query_hash\n",
    "\t,r.query_plan_hash\n",
    "\t,r.dist_statement_id\n",
    "\t,r.[label]\n",
    "\t,s.client_interface_name\n",
    "\t,r.[sql_handle]\n",
    "\t,c.client_net_address\n",
    "\t,c.connection_id\n",
    "FROM\tsys.dm_exec_requests r\n",
    "CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n",
    "JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n",
    "JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n",
    "JOIN\tsys.databases d ON d.database_id = r.database_id\n",
    "WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n",
    "AND\tr.session_id <> @@SPID\n",
    "AND\ts.program_name NOT IN ('QueryInsights','DMS')\n",
    "</pre></p>\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "8b967643-7f60-441f-a4ac-ddc74d812efb",
    "default_lakehouse_name": "LH_QueryResults",
    "default_lakehouse_workspace_id": "8f8cfc19-0610-4b57-9fe9-8abe8c70ce4d"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
