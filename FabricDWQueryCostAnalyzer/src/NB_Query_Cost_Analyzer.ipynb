{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d94e7d5b-db72-4777-b700-297f8baaaf13",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Parameters to update\n",
    "- **FabricDWWorkspaceName**: The name of the workspace that the warehouse exists.\n",
    "- **FabricDWName**: The name of the warehouse.\n",
    "- **ConcurrnecyNum**: The number of queries that will be executings in parallel.\n",
    "- **CapacityMetricsWorkspace**: The name of the workspace that the capacity metrics semantic model exists.\n",
    "- **CapacityMetricsDataset**: The name of the capacity metrics app semenatic model.\n",
    "- **StoreQueryResults**: Flag to set if the results of the queries will be stored in the query results table.\n",
    "- **QueryRepeatCount**: Number of times a query will run (should be between 1 and 4) eg. QueryRepeatCount = 4 and queryList = [query1, query2] will become [query1, query1, query1, query1, query2, query2, query2]\n",
    "- **RunName**: The name of the run. If not specified, one will be generated with the following format. '*Run_{yyyyMMdd}_{hhmmss}*'\n",
    "- **QueryList**: A list of queries to be executed against the sql endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20be0399-9430-4b52-a3ab-1c911fc0d69a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "FabricDWWorkspaceName = ''\n",
    "FabricDWName = 'WH_SampleData'\n",
    "ConcurrencyNum = 1 # The number of workers that will be executing queries at once. Every worker will execute each query defined in the query list.\n",
    "CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n",
    "CapacityMetricsDataset = 'Fabric Capacity Metrics'\n",
    "StoreQueryResults = False\n",
    "QueryRepeatCount = 4 # Number of times a query will run eg. queryRepeatCount = 4, queryList = [query1, query2] will become [query1, query1, query1, query1, query2, query2, query2]\n",
    "RunName = '' # The name of the run. If not specified, one will be generated with the following format. '*Run_{yyyyMMdd}_{hhmmss}*'\n",
    "QueryList = '' # A list of queries to be executed. Eg. \"['SELECT COUNT(*) FROM tblA', 'SELECT 1 AS a']\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493545a-4048-4721-a9ef-eeed8e883a28",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "RunName = RunName if RunName else f\"Run_{datetime.datetime.now(datetime.timezone.utc).strftime('%Y%m%d_%H%M%S')}\"\n",
    "print(f\"{FabricDWWorkspaceName=}\")\n",
    "print(f\"{FabricDWName=}\")\n",
    "print(f\"{ConcurrencyNum=}\")\n",
    "print(f\"{CapacityMetricsWorkspace=}\")\n",
    "print(f\"{CapacityMetricsDataset=}\")\n",
    "print(f\"{StoreQueryResults=}\")\n",
    "print(f\"{RunName=}\")\n",
    "print(f\"{QueryList=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4b5a5a-dfa7-43b0-b8a7-b4b3849a7559",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "if len(QueryList) == 0: \n",
    "    queryList = [\n",
    "        # Example - Query\n",
    "        'SELECT COUNT(*) FROM FactTransaction'\n",
    "        ,'''SELECT\tCOUNT(*) AS TotalTransactions\n",
    "            FROM\tFactTransaction AS ft\n",
    "            JOIN\tDimDate AS d\n",
    "            ON\t\td.Date = ft.DateKey\n",
    "            JOIN\tDimPaymentMethod AS pm\n",
    "            ON\t\tpm.PaymentMethodKey= ft.PaymentMethodKey\n",
    "            JOIN\tDimTransactionType AS tt\n",
    "            ON\t\ttt.TransactionTypeKey = ft.TransactionTypeKey\n",
    "            JOIN\tDimSupplier AS s\n",
    "            ON\t\ts.SupplierKey = ft.SupplierKey\n",
    "            JOIN\tDimCustomer AS cu\n",
    "            ON\t\tcu.CustomerKey = ft.CustomerKey\n",
    "            JOIN\tDimCustomer AS cuBill\n",
    "            ON\t\tcuBill.CustomerKey = ft.BillToCustomerKey'''\n",
    "        # Example - Stored Procedures\n",
    "        ,'''EXEC sp_Ingest'''\n",
    "        ,'''EXEC sp_Query'''\n",
    "        # Example - Query with multiple statements\n",
    "        ,'''IF OBJECT_ID('dbo.DimDate', 'U') IS NOT NULL DROP TABLE dbo.DimDate; CREATE TABLE dbo.DimDate AS SELECT * FROM LH_SampleData.dbo.DimDate'''\n",
    "    ]\n",
    "else:\n",
    "    queryList = ast.literal_eval(QueryList.replace('\\\\n', ' ').replace('\\\\t', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ed022-d969-45cd-8fd2-93b34357f16f",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "executorCoreCnt = int(spark.conf.get('spark.executor.cores', '4'))\n",
    "executorInstances = len(spark._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "executorsRequired = math.ceil(ConcurrencyNum / (executorCoreCnt * executorInstances))\n",
    "maxConcurrency = ConcurrencyNum if ConcurrencyNum < (executorCoreCnt * executorInstances) else (executorCoreCnt * executorInstances)\n",
    "\n",
    "# Adding queries to the queryList so that each queries executes 4 times sequentially.\n",
    "# Example, original queryList = [query_1, query_2] becomes [query_1, query_1, query_1, query_1, query_2, query_2, query_2, query_2]\n",
    "queryListWithRepeat = []\n",
    "for i, query in enumerate(queryList):\n",
    "    for n in range(QueryRepeatCount):\n",
    "        queryListWithRepeat.extend([{'QueryUniqueNum': i+1, 'QueryRepeatNum': n+1, 'Query': query}])\n",
    "\n",
    "# Multipling the number of queries by the maxConcurrency to create a pool of queries to be executed.\n",
    "# This will make it so that each number of concurrency will execute every query in the queryList.\n",
    "# Eg. If the queryList has 10 queries and the maxConcurrency is 5, then the queryPool will have 50 queries.\n",
    "queryPool = queryListWithRepeat * maxConcurrency\n",
    "\n",
    "rddQueries = sc.parallelize(queryPool, maxConcurrency)\n",
    "rddQueriesWithId = rddQueries.zipWithUniqueId().map(lambda x: [x[1], (x[1], x[0].get('QueryUniqueNum'), x[0].get('QueryRepeatNum'), x[0].get('Query'))] )\n",
    "rddQueriesWithId = rddQueriesWithId.partitionBy(maxConcurrency, lambda k: k ) \n",
    "print(rddQueriesWithId.glom().map(len).collect())  # Check the length of each partition to check for even distribution of rows in the partitions. This will tell us if the number of queries are evenly distributed\n",
    "\n",
    "displayHTML(f\"\"\"\n",
    "<p><span style=\"font-size:20px;\"><strong>Max concurrency of spark session is </strong><i><strong>{executorCoreCnt * executorInstances}</strong></i></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>Defined concurrency is </strong><i><strong>{ConcurrencyNum}</strong></i></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>Will run </strong><i><strong>{maxConcurrency}</strong></i><strong> queries concurrently for this spark session</strong></span></p>\n",
    "<p><span style=\"font-size:20px;\"><strong>A total of <i>{len(queryPool)}</i> queries will be executed (queryList size <i>{len(queryList)}</i> * max concurrency <i>{maxConcurrency}</i> * query repeat count <i>{QueryRepeatCount}</i>)</strong></span></p>\n",
    "\"\"\")\n",
    "\n",
    "if executorsRequired < executorInstances:\n",
    "    displayHTML(f\"\"\"\n",
    "    <p><span style=\"font-size:17px;\"><strong><i>cell 1</i><br><pre><code>%%configure -f<br>{{\"conf\": {{\"spark.dynamicAllocation.minExecutors\": {executorsRequired}}}}}</code></pre>\n",
    "        <br><hr style=\"border-bottom: dotted 1px #000\" /><i>cell 2</i><br><br><pre><code>import time, datetime\n",
    "\n",
    "ExecutorsRequired = {executorsRequired}\n",
    "endDateTime = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=10)\n",
    "executorsReadyIterCnt = 0\n",
    "while datetime.datetime.now(datetime.timezone.utc) < endDateTime:\n",
    "    executorAvailableCnt = len([executor.host() for executor in spark._jsc.sc().statusTracker().getExecutorInfos()]) -1\n",
    "    print(f'{{datetime.datetime.now().replace(microsecond=0)}} | {{ExecutorsRequired=}} | {{executorAvailableCnt=}} | {{executorsReadyIterCnt=}}', end='\\\\r')\n",
    "    if executorAvailableCnt == ExecutorsRequired: # check if the required number of executors are ready\n",
    "        executorsReadyIterCnt += 1\n",
    "        if executorsReadyIterCnt == int(30/5)+1: # check 6 times (30 seconds) for executors to be ready and stable\n",
    "            break\n",
    "    else:\n",
    "        executorsReadyIterCnt = 0\n",
    "    time.sleep(5)</code></pre>\n",
    "    </strong></span></p>\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d537b",
   "metadata": {},
   "source": [
    "##### Get the workspace Id and sql endpoint name to run the queries against"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f726386-0439-4ceb-9f96-b6f562e01135",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "while True:\n",
    "    workspaceFound = False\n",
    "    for workspace in response.json().get('value'):\n",
    "        if workspace.get('displayName') == FabricDWWorkspaceName:\n",
    "            fabricDWWorkspaceId = workspace.get('id')\n",
    "            workspaceFound = True\n",
    "            break\n",
    "    \n",
    "    if workspaceFound:\n",
    "        break\n",
    "    elif workspaceFound == False and response.json().get('continuationToken'):\n",
    "        responseStatus = requests.request(method='get', url=response.json().get('continuationUri'), headers=header)\n",
    "    else:\n",
    "        print(f\"Workspace was not found and no contination token found - {response.json()}\")\n",
    "        break\n",
    "\n",
    "print(f'{fabricDWWorkspaceId = }\\n{FabricDWWorkspaceName = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0a2938",
   "metadata": {},
   "source": [
    "##### Get the artifact type of the sql endpoint (lakehouse or warehouse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c86e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/items', headers=header)\n",
    "\n",
    "for item in response.json().get('value'):\n",
    "    if item.get('displayName') == FabricDWName:\n",
    "        itemType = item.get('type')\n",
    "        \n",
    "print(f'{itemType = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c235532",
   "metadata": {},
   "source": [
    "##### The capacity information that the sql endpoint is associated to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1fd34b-c182-47ff-8d87-d8fdd550ad3e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n",
    "workspaceName = response.json().get('displayName')\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n",
    "capacityId = response.json().get('capacityId')\n",
    "capacityRegion = response.json().get('capacityRegion')\n",
    "capacityName = response.json().get('displayName')\n",
    "capacitySku = 'F0' #Default value of F0\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/capacities', headers=header)\n",
    "for capacity in response.json().get('value'):\n",
    "    if capacity.get('id') == capacityId:\n",
    "        capacitySku = capacity.get('sku')\n",
    "\n",
    "if itemType == 'Lakehouse':\n",
    "    response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/lakehouses', headers=header)\n",
    "    warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == FabricDWName][0]\n",
    "    fabricDWServer = warehouse.get('properties').get('sqlEndpointProperties').get('connectionString')\n",
    "else:\n",
    "    response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/warehouses', headers=header)\n",
    "    warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == FabricDWName][0]\n",
    "    fabricDWServer = warehouse.get('properties').get('connectionString')\n",
    "    \n",
    "warehouseId = warehouse.get('id')\n",
    "\n",
    "print(f'{warehouseId = }\\n{fabricDWServer = }\\n{workspaceName = }\\n{capacityId = }\\n{capacityRegion = }\\n{capacityName = }\\n{capacitySku = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff692088",
   "metadata": {},
   "source": [
    "##### Get the latest Fabric capacity cost for the region the sql endpoint exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509449a-48a5-4da1-a7c7-43a75ecd7325",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "response = requests.request(method='get', url=\"https://prices.azure.com/api/retail/prices?$filter=skuName eq 'Fabric Capacity'\", headers=header)\n",
    "for capacity in response.json().get('Items'):\n",
    "    if capacity.get('armRegionName') == capacityRegion.replace(' ', '').lower():\n",
    "        costReserved = capacity.get('retailPrice') / 12 / 730 / 60 / 60 # get the amount per CU second\n",
    "        costPayGo = costReserved / (156.334/262.80) # constant saving of ~41%. 156.334 is the resevered price of a region. 262.80 is the paygo price of a region\n",
    "print(f'{costReserved = :.10f}\\n{costPayGo = :.10f}') # per CU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5bcecd-ac6c-45bf-9ebb-66fe8949ab54",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Define the queries to be executed. These are single line queries so use /* */ for commenting out code vs --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4596a-e954-4ccc-aa49-0d32c1500013",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DecimalType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "def sqlendpoint_get_token():\n",
    "    # Use the credentials of the user executing the notebook\n",
    "    token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "    encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "    tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "    \n",
    "    return tokenstruct\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={fabricDWServer};Database={FabricDWName};APP=QueryCostAnalyzer'\n",
    "\n",
    "runId = str(uuid.uuid4()).upper()\n",
    "\n",
    "print(f'{runId = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04b459d",
   "metadata": {},
   "source": [
    "##### Get information from the sql endpoint and as into the RunResults table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1633b8f-665d-4393-a63c-174effbb1ba9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DecimalType\n",
    "\n",
    "tokenstruct = sqlendpoint_get_token()\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''SELECT @@VERSION AS DWVersion\n",
    "                            ,@@SERVERNAME AS ServerGuid\n",
    "                            ,DB_NAME() AS DWName\n",
    "        ''')\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = [column[0] for column in cursor.description]\n",
    "        cursor.commit()\n",
    "        resultSet = [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "        cursor.execute(f'''SELECT [is_vorder_enabled] AS IsVOrderEnabled, [data_lake_log_publishing_desc] AS DataLakeLogPublishingDesc\n",
    "                            ,[data_lake_log_publishing] AS DataLakeLogPublishing, [create_date] AS DWCreateDate, [compatibility_level] AS CompatibilityLevel\n",
    "                            FROM sys.databases \n",
    "                            WHERE [name] = '{FabricDWName}'\n",
    "            ''')\n",
    "\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = [column[0] for column in cursor.description]\n",
    "        cursor.commit()\n",
    "        resultSet = resultSet[0] | [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList][0]\n",
    "        df = spark.createDataFrame([resultSet])\n",
    "\n",
    "dfRunOrdered = df.select(\n",
    "    F.lit(RunName).alias('RunName').cast(StringType())\n",
    "    ,F.lit(runId).alias('RunId').cast(StringType())\n",
    "    ,F.lit(len(queryPool)).alias('QueriesExecutedCnt').cast(IntegerType())\n",
    "    ,F.lit(maxConcurrency).alias('RunConcurrency').cast(IntegerType())\n",
    "    ,F.lit(QueryRepeatCount).alias('QueryRepeatCount').cast(IntegerType())\n",
    "    ,F.lit(StoreQueryResults).alias('StoreQueryResults').cast(StringType())\n",
    "    ,F.lit(itemType).alias('ItemType').cast(StringType())\n",
    "    ,col('DWName').cast(StringType())\n",
    "    ,col('ServerGuid').cast(StringType())\n",
    "    ,F.lit(warehouseId).alias('DWGuid').cast(StringType())\n",
    "    ,F.lit(fabricDWServer).alias('DWConnectionString').cast(StringType())\n",
    "    ,col('DWVersion').cast(StringType())\n",
    "    ,col('CompatibilityLevel').cast(StringType())\n",
    "    ,col('DWCreateDate').cast(StringType())\n",
    "    ,col('DataLakeLogPublishing').cast(StringType())\n",
    "    ,col('DataLakeLogPublishingDesc').cast(StringType())\n",
    "    ,col('IsVOrderEnabled').cast(StringType())\n",
    "    ,F.lit(workspaceName).alias('WorkspaceName').cast(StringType())\n",
    "    ,F.lit(fabricDWWorkspaceId).alias('WorkspaceGuid').cast(StringType())\n",
    "    ,F.lit(capacityName).alias('CapacityName').cast(StringType())\n",
    "    ,F.lit(capacityId).alias('CapacityGuid').cast(StringType())\n",
    "    ,F.lit(capacitySku).alias('CapacitySKU').cast(StringType())\n",
    "    ,F.lit(capacityRegion).alias('CapacityRegion').cast(StringType())\n",
    "    ,F.lit(None).alias('RunStartDateTimeUTC').cast(TimestampType())\n",
    "    ,F.lit(None).alias('RunStartDateTimeEpochMS').cast(LongType())\n",
    "    ,F.lit(None).alias('RunEndDateTimeUTC').cast(TimestampType())\n",
    "    ,F.lit(None).alias('RunEndDateTimeEpochMS').cast(LongType())\n",
    "    ,F.lit(None).alias('RunDurationMS').cast(LongType())\n",
    "    ,F.lit(None).alias('RunCUSeconds').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunCostPayGo').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunCostReserved').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunDataScannedDiskMB').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunDataScannedMemoryMB').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunDataScannedRemoteStorageMB').cast(DecimalType(38,19))\n",
    "    ,F.lit(None).alias('RunAllocatedCpuTimeMS').cast(LongType())\n",
    "    ,F.lit(60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ).alias('CapacityDailyCUSeconds').cast(IntegerType())\n",
    "    ,F.lit(costPayGo * 60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ).alias('CapacityDailyCostPayGo').cast(DecimalType(38,19))\n",
    "    ,F.lit(costReserved * 60*60*24*int(64 if ''.join([str(i) for i in capacitySku if i.isdigit()]) == 1 else ''.join([str(i) for i in capacitySku if i.isdigit()])) ).alias('CapacityDailyCostReserved').cast(DecimalType(38,19))\n",
    ")\n",
    "\n",
    "if mssparkutils.fs.exists(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults'):\n",
    "\n",
    "    dtRunResults = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults')\n",
    "\n",
    "    (dtRunResults.alias('t')\n",
    "        .merge(dfRunOrdered.alias('s')\n",
    "            ,f't.runId = s.RunId'\n",
    "            )\n",
    "        .whenNotMatchedInsertAll()\n",
    "    ).execute() \n",
    "else:\n",
    "    dfRunOrdered.write.format('delta').mode('append').save(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef5a306",
   "metadata": {},
   "source": [
    "##### Run the queries against the sql endpoint and store results into the QueryResults table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a25284-d19a-4120-b2bf-9326efd16033",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import pyodbc \n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "tokenstruct = sqlendpoint_get_token()\n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultRowCnt = len(resultList)\n",
    "        resultColumns = []\n",
    "        if StoreQueryResults:\n",
    "            resultColumns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "        resultRowCnt = 0\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList], resultRowCnt\n",
    "\n",
    "def execute_query(partition_index, iterator):\n",
    "    workerNum = partition_index + 1\n",
    "    queryMetrics = []\n",
    "    with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "        for i, queryInfo in enumerate(iterator, start=1):\n",
    "            queryIndex = queryInfo[1][0]\n",
    "            workerQueryNum = i\n",
    "            queryUniqueNum = queryInfo[1][1]\n",
    "            queryRepeatNum = queryInfo[1][2]\n",
    "            queryStatement = queryInfo[1][3]\n",
    "            with conn.cursor() as cursor:\n",
    "                queryStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "                startTime = int(round(time.time() * 1000))\n",
    "\n",
    "                cursor.execute(f'/* {workerNum=} | {workerQueryNum=} | {queryUniqueNum=} | {queryRepeatNum=} */ {queryStatement}')\n",
    "                \n",
    "                queryMessage = str(cursor.messages) if cursor.messages else \"\"\n",
    "                resultSetList = list()\n",
    "                resultRowCntList = list()\n",
    "                resultSet, resultRowCnt = get_result_set(cursor)\n",
    "                resultSetList.append(resultSet)\n",
    "                resultRowCntList.append(resultRowCnt)\n",
    "\n",
    "                while cursor.nextset():\n",
    "                    queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n",
    "                    resultSet, resultRowCnt = get_result_set(cursor)\n",
    "                    resultSetList.append(resultSet)\n",
    "                    resultRowCntList.append(resultRowCnt)\n",
    "                    \n",
    "                endTime = int(round(time.time() * 1000))\n",
    "                queryEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "                \n",
    "                statementId = ','.join(re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n",
    "                queryHash = ','.join(re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage)) if re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage) else \"\"\n",
    "                distributionRequestId = ','.join(re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n",
    "                resultSetJsonString = json.dumps(resultSetList)\n",
    "\n",
    "                cursor.commit()\n",
    "\n",
    "                queryId = str(uuid.uuid4()).upper()\n",
    "                queryMetrics.append([queryId, workerNum, workerQueryNum, queryUniqueNum, queryRepeatNum, queryStatement, queryStartDateTimeUTC, queryEndDateTimeUTC\n",
    "                        ,queryMessage, startTime, endTime, endTime - startTime\n",
    "                        ,statementId, queryHash, distributionRequestId, resultSetJsonString, resultRowCntList\n",
    "                        ])\n",
    "    return queryMetrics\n",
    "\n",
    "queriesExecuted = rddQueriesWithId.mapPartitionsWithIndex(execute_query)\n",
    "\n",
    "runStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "runStartTimeEpoch = int(runStartDateTimeUTC.timestamp()*1000)\n",
    "\n",
    "dfQueriesExecuted = spark.createDataFrame(queriesExecuted.collect(), schema=StructType([\n",
    "    StructField(\"QueryId\", StringType(), False),\n",
    "    StructField(\"WorkerNum\", IntegerType(), False),\n",
    "    StructField(\"WorkerQueryNum\", IntegerType(), False),\n",
    "    StructField(\"QueryUniqueNum\", IntegerType(), False),\n",
    "    StructField(\"QueryRepeatNum\", IntegerType(), False),\n",
    "    StructField(\"QueryStatement\", StringType(), False),\n",
    "    StructField(\"QueryStartDateTimeUTC\", TimestampType(), False),\n",
    "    StructField(\"QueryEndDateTimeUTC\", TimestampType(), False),\n",
    "    StructField(\"ReturnMessage\", StringType(), False),\n",
    "    StructField(\"QueryStartDateTimeEpochMS\", LongType(), False),\n",
    "    StructField(\"QueryEndDateTimeEpochMS\", LongType(), False),\n",
    "    StructField(\"QueryDurationMS\", LongType(), False),\n",
    "    StructField(\"StatementId\", StringType(), False),\n",
    "    StructField(\"QueryHash\", StringType(), False),\n",
    "    StructField(\"DistributionRequestId\", StringType(), False),\n",
    "    StructField(\"ResultSet\", StringType(), False),\n",
    "    StructField(\"ResultRowCnt\", StringType(), False)\n",
    "    ]))\n",
    "\n",
    "runEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n",
    "runEndTimeEpoch = int(runEndDateTimeUTC.timestamp()*1000)\n",
    "\n",
    "dfFinal = dfQueriesExecuted.select(F.lit(RunName).alias(\"RunName\"), F.lit(runId).alias(\"RunId\"), \"*\")\\\n",
    "    .withColumns(\n",
    "        {\n",
    "            \"QueryCUSeconds\": F.lit(None).cast(DecimalType(38, 19))\n",
    "            ,\"QueryCostPayGo\": F.lit(None).cast(DecimalType(38, 19))\n",
    "            ,\"QueryCostReserved\": F.lit(None).cast(DecimalType(38, 19))\n",
    "\n",
    "            ,\"DataScannedDiskMB\": F.lit(None).cast(DecimalType(38, 19))\n",
    "            ,\"DataScannedMemoryMB\": F.lit(None).cast(DecimalType(38, 19))\n",
    "            ,\"DataScannedRemoteStorageMB\": F.lit(None).cast(DecimalType(38, 19))\n",
    "            ,\"ResultCacheHit\": F.lit(None).cast(IntegerType())\n",
    "            ,\"AllocatedCpuTimeMS\": F.lit(None).cast(LongType())\n",
    "        }\n",
    "    )\n",
    "\n",
    "dfFinal.write.format('delta').mode('append').save(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf916df-8027-4011-a8ec-dd3fedc1c2b2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults').createOrReplaceTempView('vwqueryresults')\n",
    "statementList = spark.sql(f'SELECT ARRAY_JOIN(COLLECT_SET(CONCAT(\"\\\\\"\", StatementId, \"\\\\\"\")), \", \") AS Statements FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM vwqueryresults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('Statements')\n",
    "# We have to explode by statement ids since a sql query may have multiple queries within it\n",
    "queriesExecutedCnt = spark.sql(f'SELECT COUNT(StatementId) AS QueryCnt FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM vwqueryresults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('QueryCnt') \n",
    "print(f'{runId = }\\n{statementList = }\\n{queriesExecutedCnt = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2202b22c",
   "metadata": {},
   "source": [
    "##### Update the RunResults table with the end time of the run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d44baaf-dc59-433f-8b7e-c5a2a124f3fe",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtRunResults = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults')\n",
    "\n",
    "df_final = spark.createDataFrame(data=[(runStartDateTimeUTC, runStartTimeEpoch, runEndDateTimeUTC, runEndTimeEpoch, )], schema=['runStartDateTimeUTC', 'runStartDateTimeEpochMS', 'RunEndDateTimeUTC', 'RunEndDateTimeEpochMS'])\n",
    "\n",
    "(dtRunResults.alias('t')\n",
    "    .merge(df_final.alias('s')\n",
    "        ,f't.RunId = \"{runId}\"'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'RunStartDateTimeUTC': 's.RunStartDateTimeUTC'\n",
    "        ,'RunStartDateTimeEpochMS': 's.RunStartDateTimeEpochMS'\n",
    "        ,'RunEndDateTimeUTC': 's.RunEndDateTimeUTC'\n",
    "        ,'RunEndDateTimeEpochMS': 's.RunEndDateTimeEpochMS'\n",
    "        ,'RunDurationMS': 's.RunEndDateTimeEpochMS - s.RunStartDateTimeEpochMS'\n",
    "        }\n",
    "    )\n",
    ").execute() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45636737",
   "metadata": {},
   "source": [
    "##### Define the get_capacity_metrics_usage function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09fd036-6768-4fe9-b1f9-fea79ed5e7b0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, datetime\n",
    "from pyspark.sql.functions import to_timestamp, lit, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "def get_capacity_metrics_usage(time_point:datetime, operation_id_list:str):\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"Items[ItemId]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"Items[ItemKind]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"Items[ItemName]\", \tStringType(), \t\tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationStartTime]\", \tStringType(), \tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationEndTime]\", \t\tStringType(), \tTrue),\n",
    "        StructField(\"TimePointBackgroundDetail[OperationId]\", \t\t\tStringType(), \t\tTrue),\n",
    "        StructField(\"[Sum_CUs]\", \t\tDoubleType(), \t\tTrue),\n",
    "        StructField(\"[Sum_Duration]\", IntegerType(), \t\tTrue)\n",
    "    ])\n",
    "\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "                ,\"Content-Type\": \"application/json\"\n",
    "                }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "    \n",
    "    body = {\n",
    "        \"queries\": [\n",
    "        {\n",
    "            \"query\": f\"\"\"\n",
    "                DEFINE\n",
    "                    MPARAMETER 'CapacityID' \t= \"{capacityId}\"\n",
    "                    MPARAMETER 'TimePoint' \t\t= (DATE({time_point.year}, {time_point.month}, {time_point.day}) + TIME({time_point.hour}, {time_point.minute}, {time_point.second}))\n",
    "\n",
    "                    VAR __Var_CapacityId\t= {{\"{capacityId}\"}}\n",
    "                    VAR __Var_OperationId\t= {{{statementList}}}\n",
    "\n",
    "                    VAR __Filter_OperationId \t= TREATAS(__Var_OperationId, 'TimePointBackgroundDetail'[OperationId])\n",
    "                    VAR __Filter_CapacityId \t= TREATAS(__Var_CapacityId, 'Capacities'[capacityId])\n",
    "\n",
    "                    VAR OperationCUs = \n",
    "                        SUMMARIZECOLUMNS(\n",
    "                            'Items'[ItemId],\n",
    "                            'Items'[ItemKind],\n",
    "                            'Items'[ItemName],\n",
    "                            'TimePointBackgroundDetail'[OperationStartTime],\n",
    "                            'TimePointBackgroundDetail'[OperationEndTime],\n",
    "                            'TimePointBackgroundDetail'[OperationId],\n",
    "                            __Filter_OperationId,\n",
    "                            __Filter_CapacityId,\n",
    "                            \"Sum_CUs\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n",
    "                            \"Sum_Duration\", CALCULATE(SUM('TimePointBackgroundDetail'[Duration (s)]))\n",
    "                        )\n",
    "\n",
    "                EVALUATE\n",
    "                    OperationCUs\n",
    "                \"\"\"\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    response = requests.post(f'https://api.powerbi.com/v1.0/myorg/datasets/{datasetId}/executeQueries', headers=header, json=body )\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        rowsList = response.json()[\"results\"][0][\"tables\"][0][\"rows\"]\n",
    "        \n",
    "        df_dax = spark.createDataFrame(data=rowsList, schema=schema)\n",
    "        df_dax = df_dax.select('*', to_timestamp('TimePointBackgroundDetail[OperationStartTime]'), to_timestamp('TimePointBackgroundDetail[OperationEndTime]'))\n",
    "        \n",
    "        df = (df_dax.select(\n",
    "            lit(time_point).alias(\"TimePoint\")\n",
    "            ,col(\"Items[ItemId]\").alias(\"ItemId\")\n",
    "            ,col(\"Items[ItemKind]\").alias(\"ItemKind\")\n",
    "            ,col(\"Items[ItemName]\").alias(\"ItemName\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationStartTime]\").alias(\"StartTime\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationEndTime]\").alias(\"EndTime\")\n",
    "            ,col(\"TimePointBackgroundDetail[OperationId]\").alias(\"OperationId\")\n",
    "            ,col(\"[Sum_CUs]\").cast(DecimalType(38, 19)).alias(\"Sum_CUs\")\n",
    "            ,col(\"[Sum_Duration]\").cast(IntegerType()).alias(\"Sum_Duration\"))\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    else:\n",
    "        raise Exception(f'{response.json()}\\nCheck that user has as least contributor access to the workspace')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa9c0e5",
   "metadata": {},
   "source": [
    "##### Define the model_refresh function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0294f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time\n",
    "\n",
    "def model_refresh():\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "\n",
    "    response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n",
    "\n",
    "    refreshId = response.headers.get('RequestId')\n",
    "    print(f'{refreshId = } | {response.status_code = }')\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        for attempt in range(12): \n",
    "            # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n",
    "            response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n",
    "            if response.status_code == 200:\n",
    "                if response.json().get('value')[0].get('status') != 'Unknown':\n",
    "                    print(f'Refresh Complete')\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Refreshing tables ...')\n",
    "                    time.sleep(20)\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "    else:\n",
    "        print(f'Refreshed failed - {response.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190a3219",
   "metadata": {},
   "source": [
    "##### Query the capacity metrics app to get the CUs consumed for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d68bfa2-1f97-47af-a21c-230459507da7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, sum\n",
    "\n",
    "# # Continues to query the metrica app to get the data. Data can delayed by a few minutes.\n",
    "# # We retry every minute until 15 minutes has passed.\n",
    "for retryCnt in range(15):\n",
    "    df_today = get_capacity_metrics_usage(runStartDateTimeUTC, statementList)\n",
    "    df_tomorrow = get_capacity_metrics_usage(runStartDateTimeUTC + datetime.timedelta(hours = 23), statementList)\n",
    "    df_all_days = df_today.unionAll(df_tomorrow)\n",
    "    df_count = df_all_days.select('OperationId').distinct()\n",
    "    \n",
    "    print(f'{df_count.count()} statements of the {queriesExecutedCnt} that have been found in the capacity metrics model. ', end='')\n",
    "    if df_count.count() == queriesExecutedCnt:\n",
    "        df_final = df_all_days.groupBy('ItemId', 'ItemKind', 'ItemName', 'OperationId').agg(min(\"StartTime\").alias(\"StartTime\"), max(\"EndTime\").alias(\"EndTime\"), sum(\"Sum_CUs\").alias(\"QueryCUSeconds\"), sum(\"Sum_Duration\").alias(\"SumDuration\"))\n",
    "        break\n",
    "    else:\n",
    "        if retryCnt%10 == (10-1):\n",
    "            print('Refreshing the capacity metircs app semantic model...')\n",
    "            model_refresh()\n",
    "            continue\n",
    "        print('Sleeping for a minute...')\n",
    "        time.sleep(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9030f904",
   "metadata": {},
   "source": [
    "##### Query the sql endpoint queryinsights exec_request_history table to get CPU and memory usage per query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ffd41b-12e4-4c5a-aeae-0dccf30add3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "statementIdList = [statement.replace('\"', \"'\") for statement in statementList.split(', ')]\n",
    "\n",
    "\"\"\"\n",
    "Query the queryinsights view until all statment ids have been populated. If more than 5 minutes have past, exit the log what has been found. \n",
    "\"\"\"\n",
    "for retryCnt in range(5):\n",
    "        \n",
    "    tokenstruct = sqlendpoint_get_token()\n",
    "\n",
    "    with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(f'''SELECT \n",
    "                                distributed_statement_id\n",
    "                                ,submit_time\n",
    "                                ,start_time\n",
    "                                ,end_time\n",
    "                                ,total_elapsed_time_ms\n",
    "                                ,login_name\n",
    "                                ,row_count\n",
    "                                ,status\n",
    "                                ,session_id\n",
    "                                ,connection_id\n",
    "                                ,program_name\n",
    "                                ,batch_id\n",
    "                                ,root_batch_id\n",
    "                                ,query_hash\n",
    "                                ,label\n",
    "                                ,result_cache_hit\n",
    "                                ,allocated_cpu_time_ms\n",
    "                                ,data_scanned_remote_storage_mb\n",
    "                                ,data_scanned_memory_mb\n",
    "                                ,data_scanned_disk_mb\n",
    "                                ,command \n",
    "                        FROM    queryinsights.exec_requests_history\n",
    "                        WHERE   submit_time BETWEEN '{runStartDateTimeUTC}' AND '{runEndDateTimeUTC}'\n",
    "                        AND     distributed_statement_id IN\n",
    "                        (\n",
    "                            {','.join(statementIdList)}\n",
    "                        )\n",
    "                ''')\n",
    "\n",
    "            resultList = cursor.fetchall()\n",
    "            resultColumns = [column[0] for column in cursor.description]\n",
    "            cursor.commit()\n",
    "            resultSet = [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "            df_queryinsights = spark.createDataFrame(resultSet)\n",
    "\n",
    "    if df_queryinsights.count() == len(statementIdList):\n",
    "        print(f'{df_queryinsights.count()} out of {len(statementIdList)} found.')\n",
    "        break\n",
    "    else:\n",
    "        print(f'{df_queryinsights.count()} out of {len(statementIdList)} found. Sleeping for a minute...')\n",
    "        time.sleep(60)      \n",
    "\n",
    "df_queryinsights = (df_queryinsights.select(\n",
    "                F.lit(RunName).alias('RunName')\n",
    "                ,F.lit(runId).alias('RunId')\n",
    "                ,F.col('allocated_cpu_time_ms').cast(LongType()).alias('allocated_cpu_time_ms')\n",
    "                ,F.col('batch_id').cast(StringType()).alias('batch_id')\n",
    "                ,F.col('connection_id').cast(StringType()).alias('connection_id')\n",
    "                ,F.col('data_scanned_disk_mb').cast(DecimalType(18, 3)).alias('data_scanned_disk_mb')\n",
    "                ,F.col('data_scanned_memory_mb').cast(DecimalType(18, 3)).alias('data_scanned_memory_mb')\n",
    "                ,F.col('data_scanned_remote_storage_mb').cast(DecimalType(18, 3)).alias('data_scanned_remote_storage_mb')\n",
    "                ,F.col('distributed_statement_id').cast(StringType()).alias('distributed_statement_id')\n",
    "                ,F.col('end_time').cast(TimestampType()).alias('end_time')\n",
    "                ,F.col('label').cast(StringType()).alias('label')\n",
    "                ,F.col('login_name').cast(StringType()).alias('login_name')\n",
    "                ,F.col('program_name').cast(StringType()).alias('program_name')\n",
    "                ,F.col('query_hash').cast(StringType()).alias('query_hash')\n",
    "                ,F.col('result_cache_hit').cast(IntegerType()).alias('result_cache_hit')\n",
    "                ,F.col('root_batch_id').cast(StringType()).alias('root_batch_id')\n",
    "                ,F.col('row_count').cast(LongType()).alias('row_count')\n",
    "                ,F.col('session_id').cast(IntegerType()).alias('session_id')\n",
    "                ,F.col('start_time').cast(TimestampType()).alias('start_time')\n",
    "                ,F.col('status').cast(StringType()).alias('status')\n",
    "                ,F.col('submit_time').cast(TimestampType()).alias('submit_time')\n",
    "                ,F.col('total_elapsed_time_ms').cast(IntegerType()).alias('total_elapsed_time_ms')\n",
    "                ,F.col('command').cast(StringType()).alias('command')\n",
    "            )\n",
    "        )\n",
    "\n",
    "if mssparkutils.fs.exists(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryinsightsresults'):\n",
    "\n",
    "    dtQueryInsightsResults = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryinsightsresults')\n",
    "\n",
    "    (dtQueryInsightsResults.alias('t')\n",
    "        .merge(df_queryinsights.alias('s')\n",
    "            ,f't.runId = s.RunId'\n",
    "            )\n",
    "        .whenNotMatchedInsertAll()\n",
    "    ).execute() \n",
    "else:\n",
    "    df_queryinsights.write.format('delta').mode('append').save(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryinsightsresults')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72af45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView('vwCapacityMetricsApp')\n",
    "df_queryinsights.createOrReplaceTempView('vwQueryInsights')\n",
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults').createOrReplaceTempView('vwQueryResults')\n",
    "\n",
    "dfQueryResultsCleansedWithQueryInsights = spark.sql(f\"\"\"\n",
    "SELECT qr.RunId, qr.StatementId\n",
    "        ,SUM(b.QueryCUSeconds) AS QueryCUSeconds\n",
    "        ,SUM(b.DataScannedMemoryMB) AS DataScannedMemoryMB, SUM(b.DataScannedDiskMB) AS DataScannedDiskMB\n",
    "        ,SUM(b.DataScannedRemoteStorageMB) AS DataScannedRemoteStorageMB, MAX(b.ResultCacheHit) AS ResultCacheHit\n",
    "        ,SUM(b.AllocatedCpuTimeMS) AS AllocatedCpuTimeMS\n",
    "FROM vwQueryResults AS qr\n",
    "JOIN \n",
    "(\n",
    "    SELECT cma.OperationId AS StatementId, cma.QueryCUSeconds\n",
    "            ,qi.data_scanned_memory_mb AS DataScannedMemoryMB, qi.data_scanned_disk_mb AS DataScannedDiskMB\n",
    "            ,qi.data_scanned_remote_storage_mb AS DataScannedRemoteStorageMB\n",
    "            ,qi.result_cache_hit AS ResultCacheHit, qi.allocated_cpu_time_ms AS AllocatedCpuTimeMS\n",
    "    FROM vwCapacityMetricsApp AS cma\n",
    "    JOIN vwQueryInsights AS qi\n",
    "    ON qi.distributed_statement_id = cma.OperationId\n",
    ") AS b\n",
    "ON qr.StatementId LIKE '%'||b.StatementId||'%'\n",
    "WHERE qr.RunId = '{runId}'\n",
    "GROUP BY qr.RunId, qr.StatementId\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13aa53bc-c865-491d-881a-3206e8cfbfad",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Update the QueryResults table with the CUSeconds and QueryCost derived from the Capacity Metrics App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c6bdf-7984-489a-b3bf-c3de6080aa60",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtQueryResults = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults')\n",
    "\n",
    "(dtQueryResults.alias('t')\n",
    "    .merge(dfQueryResultsCleansedWithQueryInsights.alias('s')\n",
    "        ,f't.runId = s.RunId AND t.StatementId = s.StatementId'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'QueryCUSeconds': 's.QueryCUSeconds'\n",
    "        ,'QueryCostPayGo': f's.QueryCUSeconds * {costPayGo}'\n",
    "        ,'QueryCostReserved': f's.QueryCUSeconds * {costReserved}'\n",
    "        ,'DataScannedDiskMB': 's.DataScannedDiskMB'\n",
    "        ,'DataScannedMemoryMB': 's.DataScannedMemoryMB'\n",
    "        ,'DataScannedRemoteStorageMB': 's.DataScannedRemoteStorageMB'\n",
    "        ,'ResultCacheHit': 's.ResultCacheHit'\n",
    "        ,'AllocatedCpuTimeMS': 's.AllocatedCpuTimeMS'\n",
    "        }\n",
    "    )\n",
    ").execute() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8343c8b6-52c9-414d-8bf7-962982409336",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Update the RunResults table with cost of run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12730f91-3044-4f4c-908c-87d657f3ace5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "dtRunResults = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults')\n",
    "\n",
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults').createOrReplaceTempView('vwqueryresults')\n",
    "\n",
    "dtRunResultsCleansed = spark.sql(f'''SELECT SUM(COALESCE(QueryCUSeconds, 0)) AS RunCUSeconds\n",
    "        ,SUM(COALESCE(QueryCostPayGo, 0)) AS RunCostPayGo\n",
    "        ,SUM(COALESCE(QueryCostReserved, 0)) AS RunCostReserved \n",
    "        ,SUM(COALESCE(DataScannedDiskMB, 0)) AS RunDataScannedDiskMB\n",
    "        ,SUM(COALESCE(DataScannedMemoryMB, 0)) AS RunDataScannedMemoryMB\n",
    "        ,SUM(COALESCE(DataScannedRemoteStorageMB, 0)) AS RunDataScannedRemoteStorageMB \n",
    "        ,SUM(COALESCE(AllocatedCpuTimeMS, 0)) AS RunAllocatedCpuTimeMS \n",
    "        FROM vwqueryresults WHERE RunId = \"{runId}\"\n",
    "    ''')\n",
    "\n",
    "(dtRunResults.alias('t')\n",
    "    .merge(dtRunResultsCleansed.alias('s')\n",
    "        ,f't.RunId = \"{runId}\"'\n",
    "        )\n",
    "    .whenMatchedUpdate(set=\n",
    "        {'RunCUSeconds': 's.RunCUSeconds'\n",
    "        ,'RunCostPayGo': f's.RunCUSeconds * {costPayGo}'  # This could be different looking at the tables separately due to rounding\n",
    "        ,'RunCostReserved': f's.RunCUSeconds * {costReserved}'  # This could be different looking at the tables separately due to rounding\n",
    "        ,'RunDataScannedDiskMB': 's.RunDataScannedDiskMB'\n",
    "        ,'RunDataScannedMemoryMB': 's.RunDataScannedMemoryMB'\n",
    "        ,'RunDataScannedRemoteStorageMB': 's.RunDataScannedRemoteStorageMB'\n",
    "        ,'RunAllocatedCpuTimeMS': 's.RunAllocatedCpuTimeMS'\n",
    "        }\n",
    "    )\n",
    ").execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081661ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/runresults').createOrReplaceTempView('vwrunresults')\n",
    "display(spark.sql(f\"SELECT * FROM vwrunresults WHERE RunId = '{runId}'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76252d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryresults').createOrReplaceTempView('vwqueryresults')\n",
    "display(spark.sql(f\"SELECT * FROM vwqueryresults WHERE RunId = '{runId}'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7fd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format('delta').load(f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/queryinsightsresults').createOrReplaceTempView('vwqueryinsightsresults')\n",
    "display(spark.sql(f\"SELECT * FROM vwqueryinsightsresults WHERE RunId = '{runId}'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff438663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled', False)\n",
    "\n",
    "for tableName in ['queryresults', 'runresults', 'queryinsightsresults']:\n",
    "    dt = DeltaTable.forPath(spark, f'abfss://{notebookutils.mssparkutils.env.getWorkspaceName()}@onelake.dfs.fabric.microsoft.com/LH_QueryResults.Lakehouse/Tables/{tableName}')\n",
    "    dt.optimize().executeCompaction()\n",
    "    dt.vacuum(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4c742f",
   "metadata": {},
   "source": [
    "##### Trigger the sql endpoint metadata sync process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918f756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import time\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "workspaceId = spark.conf.get(\"trident.workspace.id\")\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses', headers=header)\n",
    "\n",
    "lakehouseId = [lakehouse.get('id') for lakehouse in response.json().get('value') if lakehouse.get('displayName') == 'LH_QueryResults'][0]\n",
    "sqlendpointId = requests.request(method='get', url=f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\", headers=header).json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "response = requests.request(method='post', url=f\"https://api.fabric.microsoft.com/v1.0/myorg/lhdatamarts/{sqlendpointId}\", data=json.dumps(payload), headers=header)\n",
    "\n",
    "batchId = response.json()[\"batchId\"]\n",
    "progressState = response.json()[\"progressState\"]\n",
    "\n",
    "iterationCnt = 0\n",
    "while progressState == 'inProgress':\n",
    "    time.sleep(5)\n",
    "    statusresponsedata = requests.request(method='get', url=f\"https://api.fabric.microsoft.com/v1.0/myorg/lhdatamarts/{sqlendpointId}/batches/{batchId}\", headers=header)\n",
    "    progressState = statusresponsedata.json()[\"progressState\"]\n",
    "    print(f\"Sync state: {progressState}\")\n",
    "    if iterationCnt > 24:\n",
    "        break # A sync should not take more than 2 minutes. If it does, then break our the loop\n",
    "\n",
    "table_details = [\n",
    "        {\n",
    "        'tableName': table['tableName'],\n",
    "         'sqlSyncState':  table['sqlSyncState']\n",
    "        }\n",
    "        for table in statusresponsedata.json()['operationInformation'][0]['progressDetail']['tablesSyncStatus']\n",
    "    ]\n",
    "\n",
    "print(json.dumps(table_details, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "487dfa72-4492-47c5-b5c3-b1fbc595f985",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": true
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "run_control": {
     "frozen": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2024-09-26T16:59:25.702326Z",
       "execution_start_time": "2024-09-26T16:59:22.4859083Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b17ae74c-52ae-48ab-96be-2631ef23ef27",
       "queued_time": "2024-09-26T16:59:11.5504861Z",
       "session_id": "56ed4fc1-00c2-4248-bba5-123781108efd",
       "session_start_time": "2024-09-26T16:59:11.7989781Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 56ed4fc1-00c2-4248-bba5-123781108efd, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n",
       "<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n",
       "<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n",
       "\t,s.login_name\n",
       "\t,r.[session_id]\n",
       "\t,r.start_time\n",
       "\t,r.STATUS\n",
       "\t,r.total_elapsed_time\n",
       "\t,r.command\n",
       "\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n",
       "\t\tWHEN r.[statement_start_offset] > 0\n",
       "\t\t\tTHEN\n",
       "\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n",
       "\t\t\t\tCASE r.[statement_end_offset]\n",
       "\t\t\t\t\tWHEN - 1\n",
       "\t\t\t\t\t\tTHEN\n",
       "\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
       "\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n",
       "\t\t\t\t\tELSE\n",
       "\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
       "\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n",
       "\t\t\t\t\tEND\n",
       "\t\tELSE\n",
       "\t\t\t/* 1st part of full command is running */\n",
       "\t\t\tCASE r.[statement_end_offset]\n",
       "\t\t\t\tWHEN - 1\n",
       "\t\t\t\t\tTHEN\n",
       "\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
       "\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n",
       "\t\t\t\tELSE\n",
       "\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
       "\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n",
       "\t\t\t\tEND\n",
       "\t\tEND AS [executing_statement]\n",
       "\t,t.[text] AS [parent_batch]\n",
       "\t,s.[program_name]\n",
       "\t,r.query_hash\n",
       "\t,r.query_plan_hash\n",
       "\t,r.dist_statement_id\n",
       "\t,r.[label]\n",
       "\t,s.client_interface_name\n",
       "\t,r.[sql_handle]\n",
       "\t,c.client_net_address\n",
       "\t,c.connection_id\n",
       "FROM\tsys.dm_exec_requests r\n",
       "CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n",
       "JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n",
       "JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n",
       "JOIN\tsys.databases d ON d.database_id = r.database_id\n",
       "WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n",
       "AND\tr.session_id <> @@SPID\n",
       "AND\ts.program_name NOT IN ('QueryInsights','DMS')\n",
       "</pre></p>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displayHTML(f\"\"\"<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n",
    "<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n",
    "<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n",
    "\t,s.login_name\n",
    "\t,r.[session_id]\n",
    "\t,r.start_time\n",
    "\t,r.STATUS\n",
    "\t,r.total_elapsed_time\n",
    "\t,r.command\n",
    "\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n",
    "\t\tWHEN r.[statement_start_offset] > 0\n",
    "\t\t\tTHEN\n",
    "\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n",
    "\t\t\t\tCASE r.[statement_end_offset]\n",
    "\t\t\t\t\tWHEN - 1\n",
    "\t\t\t\t\t\tTHEN\n",
    "\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
    "\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n",
    "\t\t\t\t\tELSE\n",
    "\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
    "\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n",
    "\t\t\t\t\tEND\n",
    "\t\tELSE\n",
    "\t\t\t/* 1st part of full command is running */\n",
    "\t\t\tCASE r.[statement_end_offset]\n",
    "\t\t\t\tWHEN - 1\n",
    "\t\t\t\t\tTHEN\n",
    "\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n",
    "\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n",
    "\t\t\t\tELSE\n",
    "\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n",
    "\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n",
    "\t\t\t\tEND\n",
    "\t\tEND AS [executing_statement]\n",
    "\t,t.[text] AS [parent_batch]\n",
    "\t,s.[program_name]\n",
    "\t,r.query_hash\n",
    "\t,r.query_plan_hash\n",
    "\t,r.dist_statement_id\n",
    "\t,r.[label]\n",
    "\t,s.client_interface_name\n",
    "\t,r.[sql_handle]\n",
    "\t,c.client_net_address\n",
    "\t,c.connection_id\n",
    "FROM\tsys.dm_exec_requests r\n",
    "CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n",
    "JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n",
    "JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n",
    "JOIN\tsys.databases d ON d.database_id = r.database_id\n",
    "WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n",
    "AND\tr.session_id <> @@SPID\n",
    "AND\ts.program_name NOT IN ('QueryInsights','DMS')\n",
    "</pre></p>\n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "8b967643-7f60-441f-a4ac-ddc74d812efb",
    "default_lakehouse_name": "LH_QueryResults",
    "default_lakehouse_workspace_id": "8f8cfc19-0610-4b57-9fe9-8abe8c70ce4d"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
