{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7632378f",
   "metadata": {},
   "source": [
    "### Parameters to Update\n",
    "**CapacityMetricsWorkspace**: The name of the workspace where the capacity metrics app exists.\n",
    "\n",
    "**CapacityMetricsDataset**: The name of the capacity metrics app dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d6600-09f0-4990-bba5-7197a6cf768a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n",
    "CapacityMetricsDataset = 'Fabric Capacity Metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a72acf-8391-4bfd-bacc-11bee32837d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get the workspace id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6033b2c-aff0-4373-b7a1-1ee99c23b542",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "workspaceId = spark.conf.get('trident.workspace.id')\n",
    "print(f\"{workspaceId=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51396ef1-3c5f-4651-983e-0dbf377b0e34",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}', headers=header)\n",
    "workspaceName = response.json().get('displayName')\n",
    "print(f'{workspaceName=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a742a06-b57a-4dd7-abca-b59de79dffb4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create lakehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01c431-0b74-460d-96be-ffc21f7c3973",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "for lakehouseName in ['LH_SampleData', 'LH_QueryResults']:\n",
    "    try:\n",
    "        lakehouseItem = mssparkutils.lakehouse.create(lakehouseName, \"\", workspaceId)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        pass\n",
    "\n",
    "LH_SampleData = mssparkutils.lakehouse.get(\"LH_SampleData\", workspaceId)\n",
    "LH_QueryResults = mssparkutils.lakehouse.get(\"LH_QueryResults\", workspaceId)\n",
    "print(f'{LH_SampleData=}')\n",
    "print(f'{LH_QueryResults=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac84ea7-b8a7-49b9-9028-9bb74b65590a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get notebook definition from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfd315-3fa8-4f47-b92e-e9b9430db456",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, base64\n",
    "url = \"https://raw.githubusercontent.com/bretamyers/FabricTools/main/FabricDWQueryCostAnalyzer/src/NB_DW_Load_Cost_Analyzer.ipynb\"\n",
    "response = requests.get(url)\n",
    "\n",
    "b = base64.b64encode(bytes(json.dumps(response.json()), 'utf-8')) # convert to bytes\n",
    "base64_str = b.decode('utf-8') # convert bytes to string\n",
    "notebookDefDict = json.loads(base64.b64decode(base64_str).decode('utf-8')) # convert base64 bytes to string and then to dictionary\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse'] = LH_QueryResults.get('id')\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_name'] = LH_QueryResults.get('displayName')\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_workspace_id'] = LH_QueryResults.get('workspaceId')\n",
    "\n",
    "for cell in notebookDefDict['cells']:\n",
    "    if cell['id'] == \"20be0399-9430-4b52-a3ab-1c911fc0d69a\":\n",
    "        cell['source'] = [\n",
    "                f\"FabricDWWorkspaceName = '{workspaceName}'\\n\",\n",
    "                \"FabricDWName = 'WH_SampleData'\\n\",\n",
    "                \"ConcurrencyNum = 1 # This should be equal or less than the length of the dataframe with the queryies defined below\\n\",\n",
    "                f\"CapacityMetricsWorkspace = '{CapacityMetricsWorkspace}'\\n\",\n",
    "                f\"CapacityMetricsDataset = '{CapacityMetricsDataset}'\\n\",\n",
    "                f\"StoreQueryResults = False\\n\",\n",
    "                f\"RunName = '' # The name of the test run. If not specified, one will be generated with the following format. 'TestRun_YYYYMMDD_HHMMSS'\"\n",
    "            ]\n",
    "notebookDefBase64 = base64.b64encode(json.dumps(notebookDefDict).encode('utf-8')).decode('utf-8') # convert dictionary back to base64 string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a8e71-4e79-418b-83d5-f2859658fcca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create or update notebook from the definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259741b-87d2-4a81-a336-6d766b592860",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time, base64\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n",
    "\n",
    "notebookFound = False\n",
    "if response.status_code == 200:\n",
    "    # Check if notebook already exists\n",
    "    for notebook in response.json().get('value'):\n",
    "        if notebook.get('displayName') == 'NB_DW_Load_Cost_Analyzer':\n",
    "            notebookFound = True\n",
    "            body = {\"definition\": {\n",
    "                \"format\": \"ipynb\",\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"path\": \"notebook-content.py\"\n",
    "                        ,\"payload\": notebookDefBase64\n",
    "                        ,\"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "                }}\n",
    "            response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks/{notebook.get(\"id\")}/updateDefinition',  headers=header, data=json.dumps(body))\n",
    "\n",
    "            if response.status_code == 202:\n",
    "                responseStatus = response\n",
    "                for retry in range(5):\n",
    "                    time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "                    responseStatus = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n",
    "                    if responseStatus.json().get('status') == 'Succeeded':\n",
    "                        break\n",
    "                \n",
    "if not notebookFound:\n",
    "    print('Not found')\n",
    "    body = {\n",
    "        \"displayName\": \"NB_DW_Load_Cost_Analyzer\",\n",
    "        \"description\": \"A notebook to run data warehouse queries and capture the duration and cost of those queries.\",\n",
    "        \"definition\": {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"notebook-content.py\"\n",
    "                    ,\"payload\": notebookDefBase64\n",
    "                    ,\"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header, data=json.dumps(body))\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        time.sleep(int(response.headers.get('Retry-After')))\n",
    "        for retry in range(5):\n",
    "            responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n",
    "            print(responseStatus.text)\n",
    "            print(responseStatus.headers)\n",
    "            if responseStatus.headers.get('Location') is not None:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "    else:\n",
    "        print(f\"Error - {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046260-0103-4d5b-995b-163dc2973680",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get the notebook id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f377b78-e1e2-4332-a7c0-a0d2cfff5096",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "          \n",
    "# Need to implement for pagination\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n",
    "\n",
    "for notebook in response.json().get('value'):\n",
    "    if notebook.get('displayName') == 'NB_DW_Load_Cost_Analyzer':\n",
    "        notebookId = notebook.get('id')\n",
    "        print(f'{notebookId=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ca523-eec3-4fb8-ae9f-3b3492d2411a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07eef0-25d6-4ba6-99c7-8d73ad2f89b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "body = {\"displayName\": \"WH_SampleData\"}\n",
    "\n",
    "response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header, data=json.dumps(body))\n",
    "\n",
    "if response.status_code == 202:\n",
    "    print(f'Creating Warehouse {body.get(\"displayName\")}')\n",
    "    time.sleep(int(response.headers.get('Retry-After')))\n",
    "    for retry in range(5):\n",
    "        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n",
    "        print(responseStatus.text)\n",
    "        print(responseStatus.headers)\n",
    "        if responseStatus.json().get('status') != 'Succeeded':\n",
    "            time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "        else:\n",
    "            if responseStatus.headers.get('Location') is None: # no result was return but operation completed\n",
    "                break\n",
    "            else:\n",
    "                responseResult = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n",
    "                print(f\"Succeeded {responseResult.json()}\")\n",
    "                break\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eb768-5f22-4ba2-b8e3-14f786ef3ded",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header)\n",
    "\n",
    "for warehouse in response.json().get('value'):\n",
    "    if warehouse.get('displayName') == 'WH_SampleData':\n",
    "        WH_SampleData = warehouse\n",
    "        break\n",
    "print(WH_SampleData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee346c67-7d7b-4f7b-ac19-ecf93d16aae7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load data to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60b5c3-4354-4442-946d-e5988ffe91d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import azure.storage.blob\n",
    "import pandas as pd\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite', 'LEGACY')\n",
    "spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', 'LEGACY')\n",
    "\n",
    "account_url = \"https://fabrictutorialdata.blob.core.windows.net\"\n",
    "blob_service_client = azure.storage.blob.BlobServiceClient(account_url)\n",
    "container_client = blob_service_client.get_container_client('sampledata')\n",
    "\n",
    "tableList = list()\n",
    "for blob in container_client.walk_blobs(name_starts_with='WideWorldImportersDW/parquet/tables/', delimiter='/'):\n",
    "    tableName = blob.name.split('/')[-1].split('.parquet')[0]\n",
    "    tableList.append(tableName)\n",
    "\n",
    "for table in tableList:\n",
    "    print(f'Loading Table: {table}')\n",
    "    if mssparkutils.fs.exists(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\"):\n",
    "        mssparkutils.fs.rm(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\", recurse=True)\n",
    "\n",
    "    for blob in container_client.walk_blobs(name_starts_with=f\"WideWorldImportersDW/parquet/tables/\", delimiter='/'):\n",
    "        if f'{table}.parquet' == blob.name.split('/')[-1]:\n",
    "            dfPandas = pd.read_parquet(f'{account_url}/sampledata/{blob.name}', engine='pyarrow', storage_options={'anon': True})\n",
    "\n",
    "            if not dfPandas.empty:\n",
    "                dfSpark = spark.createDataFrame(dfPandas)\n",
    "                dfSpark.write.format('delta').mode('append').option('mergeSchema', \"true\").save(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\")\n",
    "            else:\n",
    "                print('Bad file', blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484d10d-c021-409a-9016-f977d4c27732",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load data to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43f623-71ed-4333-b93c-8b680d51a62e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = columns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        for retry in range(5):\n",
    "            cursor.execute(f\"\"\"SELECT TABLE_NAME FROM LH_SampleData.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo'\"\"\")\n",
    "            resultSet = get_result_set(cursor)\n",
    "            if len(resultSet) == 14:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(30) # wait 30 seconds for the schema sync to occur\n",
    "        cursor.commit()\n",
    "\n",
    "    for table in resultSet:\n",
    "        with conn.cursor() as cursor:\n",
    "            query = f\"\"\"IF OBJECT_ID('dbo.{table.get(\"TABLE_NAME\")}', 'U') IS NOT NULL DROP TABLE dbo.{table.get(\"TABLE_NAME\")}; CREATE TABLE dbo.{table.get(\"TABLE_NAME\")} AS SELECT * FROM LH_SampleData.dbo.{table.get(\"TABLE_NAME\")}\"\"\"\n",
    "            print(f\"{table.get('TABLE_NAME')} - {query}\")\n",
    "            cursor.execute(query)\n",
    "            cursor.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d90134-f7c8-4a55-a79a-e32271afcda0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = columns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''IF OBJECT_ID('dbo.sp_Query', 'P') IS NOT NULL\n",
    "\tDROP PROCEDURE dbo.sp_Query\n",
    ";''')\n",
    "        cursor.execute('''CREATE PROCEDURE dbo.sp_Query AS\n",
    "\tSELECT COUNT(*) AS cnt_FactOrder FROM FactOrder\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactMovement FROM FactMovement\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactPurchase FROM FactPurchase\n",
    "\n",
    ";''')\n",
    "        cursor.execute('''IF OBJECT_ID('dbo.sp_Ingest', 'P') IS NOT NULL\n",
    "\tDROP PROCEDURE dbo.sp_Ingest\n",
    ";''')\n",
    "        cursor.execute('''CREATE PROCEDURE dbo.sp_Ingest AS\n",
    "        IF OBJECT_ID('dbo.DimCity', 'U') IS NOT NULL DROP TABLE dbo.DimCity; CREATE TABLE dbo.DimCity AS SELECT * FROM LH_SampleData.dbo.DimCity;\n",
    "\n",
    "        SELECT COUNT(*) FROM DimCity\n",
    ";''')\n",
    "        cursor.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906366c-fc4d-465c-9a60-2e782ef4ac15",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Refresh Fabric Capacity Metrics App so that the latest workspace artifacts are in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280025e5-283a-45aa-bb74-bd8ea4e77b39",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "def model_refresh():\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "    print(f'{capacityWorkspaceId = }')\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "    print(f'{datasetId = }')\n",
    "\n",
    "    response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n",
    "\n",
    "    refreshId = response.headers.get('RequestId')\n",
    "    print(f'{refreshId = } | {response.status_code = }')\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        for attempt in range(12): \n",
    "            # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n",
    "            response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n",
    "            if response.status_code == 200:\n",
    "                if response.json().get('value')[0].get('status') != 'Unknown':\n",
    "                    print(f'Refresh Complete')\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Refreshing tables ...')\n",
    "                    time.sleep(20)\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "    else:\n",
    "        print(f'Refreshed failed - {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e97c3e-1263-4760-ac21-d95755077b29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests, math\n",
    "\n",
    "def check_if_workspace_exists():\n",
    "  header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "  response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "  capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "  print(f'{capacityWorkspaceId = }')\n",
    "\n",
    "  response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "  datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "  print(f'{datasetId = }')\n",
    "\n",
    "  body = {\n",
    "    \"queries\": [\n",
    "      {\n",
    "        \"query\": f\"\"\"\n",
    "            DEFINE\n",
    "              VAR __DS0Core = \n",
    "                DISTINCT('Items'[WorkspaceName])\n",
    "                \n",
    "            EVALUATE\n",
    "              __DS0Core\n",
    "      \"\"\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  response = requests.post(f'https://api.powerbi.com/v1.0/myorg/datasets/{datasetId}/executeQueries', headers=header, json=body )\n",
    "\n",
    "  return response\n",
    "\n",
    "workspaceFound = False\n",
    "response = check_if_workspace_exists()\n",
    "for results in response.json().get('results'):\n",
    "  for table in results.get('tables'):\n",
    "    for row in table.get('rows'):\n",
    "      if row.get('Items[WorkspaceName]') == workspaceName:\n",
    "        print(f'Workspace \"{workspaceName}\" was found in the capacity metrics app model')\n",
    "        workspaceFound = True\n",
    "\n",
    "if not workspaceFound:\n",
    "  for retry in range(10):\n",
    "      model_refresh()\n",
    "      response = check_if_workspace_exists()\n",
    "      for results in response.json().get('results'):\n",
    "          for table in results.get('tables'):\n",
    "              for row in table.get('rows'):\n",
    "                  if row.get('Items[WorkspaceName]') == workspaceName:\n",
    "                      workspaceFound = True\n",
    "      if workspaceFound:\n",
    "          print(f'Workspace \"{workspaceName}\" was found in the capacity metrics app model after doing a model refresh.')\n",
    "          break\n",
    "      else:\n",
    "          print(f'Workspace \"{workspaceName}\" was not found. Waiting 2 minutes and will refresh the model and try again...')\n",
    "          time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9a783-80e9-4026-bb5c-9d5169b9271f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run the *NB_DW_Load_Cost_Analyzer* notebook. This will execute a sample set of queries against the WH_SampleData to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a76cf0-1dcb-4bc1-a7bb-b88481135831",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "body = {\n",
    "    \"executionData\": {\n",
    "        \"parameters\": {\n",
    "            \"FabricDWWorkspaceName\": {\"value\": workspaceName, \"type\": \"string\"}\n",
    "            ,\"FabricDWName\": {\"value\": \"WH_SampleData\", \"type\": \"string\"}\n",
    "            ,\"ConcurrencyNum\": {\"value\": \"1\", \"type\": \"int\"}\n",
    "            ,\"CapacityMetricsWorkspace\": {\"value\": CapacityMetricsWorkspace, \"type\": \"string\"}\n",
    "            ,\"CapacityMetricsDataset\": {\"value\": CapacityMetricsDataset, \"type\": \"string\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{notebookId}/jobs/instances?jobType=RunNotebook', headers=header, data=json.dumps(body))\n",
    "\n",
    "startTime= time.time()\n",
    "if response.status_code == 202:\n",
    "    print('Started Execution of Notebook')\n",
    "    time.sleep(10) # Sleep for 10 seconds to wait for job to get started.\n",
    "    while True:\n",
    "        responseStatus = requests.request(method=\"get\", url=response.headers.get('Location'), headers=header)\n",
    "        print(f\"Status: {responseStatus.json().get('status')} | {int(time.time()-startTime)} seconds {'|' + str(responseStatus.json().get('failureReason')) if responseStatus.json().get('failureReason') != None else ''}\", end='\\r')\n",
    "        if responseStatus.json().get('status') in ['Succeeded', 'Failed', 'Completed']:\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(120) # Wait 2 minutes to check the status of the run\n",
    "else:\n",
    "    print('Error trying to execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load(f'{LH_QueryResults.properties.get(\"abfsPath\")}/Tables/runresults'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load(f'{LH_QueryResults.properties.get(\"abfsPath\")}/Tables/queryresults'))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
