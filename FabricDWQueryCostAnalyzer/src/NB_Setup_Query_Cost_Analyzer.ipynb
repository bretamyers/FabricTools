{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7632378f",
   "metadata": {},
   "source": [
    "### Parameters to Update\n",
    "**CapacityMetricsWorkspace**: The name of the workspace where the capacity metrics app exists.\n",
    "\n",
    "**CapacityMetricsDataset**: The name of the capacity metrics app dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d6600-09f0-4990-bba5-7197a6cf768a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n",
    "CapacityMetricsDataset = 'Fabric Capacity Metrics'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a72acf-8391-4bfd-bacc-11bee32837d0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get the workspace id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6033b2c-aff0-4373-b7a1-1ee99c23b542",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "workspaceId = spark.conf.get('trident.workspace.id')\n",
    "print(f\"{workspaceId=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51396ef1-3c5f-4651-983e-0dbf377b0e34",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}', headers=header)\n",
    "workspaceName = response.json().get('displayName')\n",
    "print(f'{workspaceName=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a742a06-b57a-4dd7-abca-b59de79dffb4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create lakehouses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01c431-0b74-460d-96be-ffc21f7c3973",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "\n",
    "for lakehouseName in ['LH_SampleData', 'LH_QueryResults']:\n",
    "    try:\n",
    "        lakehouseItem = mssparkutils.lakehouse.create(lakehouseName, \"\", workspaceId)\n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        pass\n",
    "\n",
    "LH_SampleData = mssparkutils.lakehouse.get(\"LH_SampleData\", workspaceId)\n",
    "LH_QueryResults = mssparkutils.lakehouse.get(\"LH_QueryResults\", workspaceId)\n",
    "print(f'{LH_SampleData=}')\n",
    "print(f'{LH_QueryResults=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac84ea7-b8a7-49b9-9028-9bb74b65590a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get notebook definition from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfd315-3fa8-4f47-b92e-e9b9430db456",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, base64\n",
    "url = \"https://raw.githubusercontent.com/bretamyers/FabricTools/main/FabricDWQueryCostAnalyzer/src/NB_Query_Cost_Analyzer.ipynb\"\n",
    "response = requests.get(url)\n",
    "\n",
    "b = base64.b64encode(bytes(json.dumps(response.json()), 'utf-8')) # convert to bytes\n",
    "base64_str = b.decode('utf-8') # convert bytes to string\n",
    "notebookDefDict = json.loads(base64.b64decode(base64_str).decode('utf-8')) # convert base64 bytes to string and then to dictionary\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse'] = LH_QueryResults.get('id')\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_name'] = LH_QueryResults.get('displayName')\n",
    "notebookDefDict['metadata']['dependencies']['lakehouse']['default_lakehouse_workspace_id'] = LH_QueryResults.get('workspaceId')\n",
    "\n",
    "for cell in notebookDefDict['cells']:\n",
    "    if cell['id'] == \"20be0399-9430-4b52-a3ab-1c911fc0d69a\":\n",
    "        cell['source'] = [\n",
    "                f\"FabricDWWorkspaceName = '{workspaceName}'\\n\",\n",
    "                \"FabricDWName = 'WH_SampleData'\\n\",\n",
    "                \"ConcurrencyNum = 1 # The number of workers that will be executing queries at once. Every worker will execute each query defined in the query list.\\n\",\n",
    "                f\"CapacityMetricsWorkspace = '{CapacityMetricsWorkspace}'\\n\",\n",
    "                f\"CapacityMetricsDataset = '{CapacityMetricsDataset}'\\n\",\n",
    "                f\"QueryRepeatCount = 4 # Number of times a query will run eg. queryRepeatCount = 4, queryList = [query1, query2] will become [query1, query1, query1, query1, query2, query2, query2]\\n\"\n",
    "                f\"StoreQueryResults = False\\n\",\n",
    "                f\"RunName = '' # The name of the run. If not specified, one will be generated with the following format. 'Run_{{yyyyMMdd}}_{{hhmmss}}'\",\n",
    "                f\"QueryList = '' # A list of queries to be executed. Eg. \\\"['SELECT COUNT(*) FROM tblA', 'SELECT 1 AS a']\\\"\",\n",
    "            ]\n",
    "notebookDefBase64 = base64.b64encode(json.dumps(notebookDefDict).encode('utf-8')).decode('utf-8') # convert dictionary back to base64 string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878a8e71-4e79-418b-83d5-f2859658fcca",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create or update notebook from the definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259741b-87d2-4a81-a336-6d766b592860",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time, base64\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n",
    "\n",
    "notebookFound = False\n",
    "if response.status_code == 200:\n",
    "    # Check if notebook already exists\n",
    "    for notebook in response.json().get('value'):\n",
    "        if notebook.get('displayName') == 'NB_Query_Cost_Analyzer':\n",
    "            notebookFound = True\n",
    "            body = {\"definition\": {\n",
    "                \"format\": \"ipynb\",\n",
    "                \"parts\": [\n",
    "                    {\n",
    "                        \"path\": \"notebook-content.py\"\n",
    "                        ,\"payload\": notebookDefBase64\n",
    "                        ,\"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "                }}\n",
    "            response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks/{notebook.get(\"id\")}/updateDefinition',  headers=header, data=json.dumps(body))\n",
    "\n",
    "            if response.status_code == 202:\n",
    "                responseStatus = response\n",
    "                for retry in range(5):\n",
    "                    time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "                    responseStatus = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n",
    "                    if responseStatus.json().get('status') == 'Succeeded':\n",
    "                        break\n",
    "                \n",
    "if not notebookFound:\n",
    "    print('Not found')\n",
    "    body = {\n",
    "        \"displayName\": \"NB_Query_Cost_Analyzer\",\n",
    "        \"description\": \"A notebook to run data warehouse queries and capture the duration and cost of those queries.\",\n",
    "        \"definition\": {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\": [\n",
    "                {\n",
    "                    \"path\": \"notebook-content.py\"\n",
    "                    ,\"payload\": notebookDefBase64\n",
    "                    ,\"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header, data=json.dumps(body))\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        time.sleep(int(response.headers.get('Retry-After')))\n",
    "        for retry in range(5):\n",
    "            responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n",
    "            print(responseStatus.text)\n",
    "            print(responseStatus.headers)\n",
    "            if responseStatus.headers.get('Location') is not None:\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "    else:\n",
    "        print(f\"Error - {response.json()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d046260-0103-4d5b-995b-163dc2973680",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Get the notebook id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f377b78-e1e2-4332-a7c0-a0d2cfff5096",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "          \n",
    "# Need to implement for pagination\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/notebooks', headers=header)\n",
    "\n",
    "for notebook in response.json().get('value'):\n",
    "    if notebook.get('displayName') == 'NB_Query_Cost_Analyzer':\n",
    "        notebookId = notebook.get('id')\n",
    "        print(f'{notebookId=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ca523-eec3-4fb8-ae9f-3b3492d2411a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Create Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a07eef0-25d6-4ba6-99c7-8d73ad2f89b6",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "body = {\"displayName\": \"WH_SampleData\"}\n",
    "\n",
    "response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header, data=json.dumps(body))\n",
    "\n",
    "if response.status_code == 202:\n",
    "    print(f'Creating Warehouse {body.get(\"displayName\")}')\n",
    "    time.sleep(int(response.headers.get('Retry-After')))\n",
    "    for retry in range(5):\n",
    "        responseStatus = requests.request(method='get', url=response.headers.get('Location'), headers=header)\n",
    "        print(responseStatus.text)\n",
    "        print(responseStatus.headers)\n",
    "        if responseStatus.json().get('status') != 'Succeeded':\n",
    "            time.sleep(int(responseStatus.headers.get('Retry-After')))\n",
    "        else:\n",
    "            if responseStatus.headers.get('Location') is None: # no result was return but operation completed\n",
    "                break\n",
    "            else:\n",
    "                responseResult = requests.request(method='get', url=responseStatus.headers.get('Location'), headers=header)\n",
    "                print(f\"Succeeded {responseResult.json()}\")\n",
    "                break\n",
    "else:\n",
    "    print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061eb768-5f22-4ba2-b8e3-14f786ef3ded",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header)\n",
    "\n",
    "for warehouse in response.json().get('value'):\n",
    "    if warehouse.get('displayName') == 'WH_SampleData':\n",
    "        WH_SampleData = warehouse\n",
    "        break\n",
    "print(WH_SampleData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee346c67-7d7b-4f7b-ac19-ecf93d16aae7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load data to Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60b5c3-4354-4442-946d-e5988ffe91d2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import azure.storage.blob\n",
    "import pandas as pd\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "spark.conf.set('spark.sql.parquet.int96RebaseModeInWrite', 'LEGACY')\n",
    "spark.conf.set('spark.sql.parquet.datetimeRebaseModeInWrite', 'LEGACY')\n",
    "\n",
    "account_url = \"https://fabrictutorialdata.blob.core.windows.net\"\n",
    "blob_service_client = azure.storage.blob.BlobServiceClient(account_url)\n",
    "container_client = blob_service_client.get_container_client('sampledata')\n",
    "\n",
    "tableList = list()\n",
    "for blob in container_client.walk_blobs(name_starts_with='WideWorldImportersDW/parquet/tables/', delimiter='/'):\n",
    "    tableName = blob.name.split('/')[-1].split('.parquet')[0]\n",
    "    tableList.append(tableName)\n",
    "\n",
    "for table in tableList:\n",
    "    print(f'Loading Table: {table}')\n",
    "    if mssparkutils.fs.exists(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\"):\n",
    "        mssparkutils.fs.rm(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\", recurse=True)\n",
    "\n",
    "    for blob in container_client.walk_blobs(name_starts_with=f\"WideWorldImportersDW/parquet/tables/\", delimiter='/'):\n",
    "        if f'{table}.parquet' == blob.name.split('/')[-1]:\n",
    "            dfPandas = pd.read_parquet(f'{account_url}/sampledata/{blob.name}', engine='pyarrow', storage_options={'anon': True})\n",
    "\n",
    "            if not dfPandas.empty:\n",
    "                dfSpark = spark.createDataFrame(dfPandas)\n",
    "                dfSpark.write.format('delta').mode('append').option('mergeSchema', \"true\").save(f\"{LH_SampleData.get('properties').get('abfsPath')}/Tables/{table}\")\n",
    "            else:\n",
    "                print('Bad file', blob.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9bd431",
   "metadata": {},
   "source": [
    "##### Trigger the sql endpoint metadata sync process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd934bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import time\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "\n",
    "workspaceId = spark.conf.get(\"trident.workspace.id\")\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses', headers=header)\n",
    "\n",
    "lakehouseId = [lakehouse.get('id') for lakehouse in response.json().get('value') if lakehouse.get('displayName') == 'LH_SampleData'][0]\n",
    "sqlendpointId = requests.request(method='get', url=f\"https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses/{lakehouseId}\", headers=header).json()['properties']['sqlEndpointProperties']['id']\n",
    "\n",
    "payload = {\"commands\":[{\"$type\":\"MetadataRefreshExternalCommand\"}]}\n",
    "response = requests.request(method='post', url=f\"https://api.fabric.microsoft.com/v1.0/myorg/lhdatamarts/{sqlendpointId}\", data=json.dumps(payload), headers=header)\n",
    "\n",
    "batchId = response.json()[\"batchId\"]\n",
    "progressState = response.json()[\"progressState\"]\n",
    "\n",
    "iterationCnt = 0\n",
    "while progressState == 'inProgress':\n",
    "    time.sleep(5)\n",
    "    statusresponsedata = requests.request(method='get', url=f\"https://api.fabric.microsoft.com/v1.0/myorg/lhdatamarts/{sqlendpointId}/batches/{batchId}\", headers=header)\n",
    "    progressState = statusresponsedata.json()[\"progressState\"]\n",
    "    print(f\"Sync state: {progressState}\")\n",
    "    if iterationCnt > 24:\n",
    "        break # A sync should not take more than 2 minutes. If it does, then break our the loop\n",
    "\n",
    "table_details = [\n",
    "        {\n",
    "        'tableName': table['tableName'],\n",
    "         'sqlSyncState':  table['sqlSyncState']\n",
    "        }\n",
    "        for table in statusresponsedata.json()['operationInformation'][0]['progressDetail']['tablesSyncStatus']\n",
    "    ]\n",
    "\n",
    "print(json.dumps(table_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484d10d-c021-409a-9016-f977d4c27732",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Load data to Warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43f623-71ed-4333-b93c-8b680d51a62e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = columns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "for attempt in range(20):\n",
    "    try:\n",
    "        with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                for retry in range(5):\n",
    "                    cursor.execute(f\"\"\"SELECT TABLE_NAME FROM LH_SampleData.INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = 'dbo'\"\"\")\n",
    "                    resultSet = get_result_set(cursor)\n",
    "                    if len(resultSet) == 14:\n",
    "                        break\n",
    "                    else:\n",
    "                        time.sleep(30) # wait 30 seconds for the schema sync to occur\n",
    "                cursor.commit()\n",
    "\n",
    "            for table in resultSet:\n",
    "                with conn.cursor() as cursor:\n",
    "                    query = f\"\"\"IF OBJECT_ID('dbo.{table.get(\"TABLE_NAME\")}', 'U') IS NOT NULL DROP TABLE dbo.{table.get(\"TABLE_NAME\")}; CREATE TABLE dbo.{table.get(\"TABLE_NAME\")} AS SELECT * FROM LH_SampleData.dbo.{table.get(\"TABLE_NAME\")}\"\"\"\n",
    "                    print(f\"{table.get('TABLE_NAME')} - {query}\")\n",
    "                    cursor.execute(query)\n",
    "                    cursor.commit()\n",
    "        break\n",
    "    except:\n",
    "        if attempt == (20 - 1):\n",
    "            raise ValueError('Data could not be loaded into Warehouse')\n",
    "        time.sleep(5)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d90134-f7c8-4a55-a79a-e32271afcda0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database={WH_SampleData.get(\"displayName\")}'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''IF OBJECT_ID('dbo.sp_Query', 'P') IS NOT NULL\n",
    "\tDROP PROCEDURE dbo.sp_Query\n",
    ";''')\n",
    "        cursor.execute('''CREATE PROCEDURE dbo.sp_Query AS\n",
    "\tSELECT COUNT(*) AS cnt_FactOrder FROM FactOrder\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactMovement FROM FactMovement\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactPurchase FROM FactPurchase\n",
    "\n",
    ";''')\n",
    "        cursor.execute('''IF OBJECT_ID('dbo.sp_Ingest', 'P') IS NOT NULL\n",
    "\tDROP PROCEDURE dbo.sp_Ingest\n",
    ";''')\n",
    "        cursor.execute('''CREATE PROCEDURE dbo.sp_Ingest AS\n",
    "        IF OBJECT_ID('dbo.DimCity', 'U') IS NOT NULL DROP TABLE dbo.DimCity; CREATE TABLE dbo.DimCity AS SELECT * FROM LH_SampleData.dbo.DimCity;\n",
    "\n",
    "        SELECT COUNT(*) FROM DimCity\n",
    ";''')\n",
    "        cursor.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fb10b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database=\"LH_SampleData\"'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''IF OBJECT_ID('dbo.sp_Query', 'P') IS NOT NULL\n",
    "\tDROP PROCEDURE dbo.sp_Query\n",
    ";''')\n",
    "        cursor.execute('''CREATE PROCEDURE dbo.sp_Query AS\n",
    "\tSELECT COUNT(*) AS cnt_FactOrder FROM FactOrder\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactMovement FROM FactMovement\n",
    "\n",
    "    SELECT COUNT(*) AS cnt_FactPurchase FROM FactPurchase\n",
    "\n",
    ";''')\n",
    "        cursor.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e10df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils  \n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={WH_SampleData.get(\"properties\").get(\"connectionString\")};Database=\"LH_QueryResults\"'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute('''CREATE OR ALTER VIEW vwColdQueryPerformancePerWorker AS \n",
    "/*\n",
    "Get the last execution of each unique query that was executed per worker.\n",
    "This will give us the cold query performance for each run.\n",
    "*/\n",
    "SELECT *\n",
    "FROM\n",
    "(\n",
    "\tSELECT\t*\n",
    "\t\t\t,ROW_NUMBER() OVER (PARTITION BY RunId, WorkerNum, QueryUniqueNum ORDER BY QueryRepeatNum) AS RN\n",
    "\tFROM LH_QueryResults.dbo.queryresults\n",
    ") AS a\n",
    "WHERE RN = 1\n",
    ";''')\n",
    "\n",
    "        cursor.execute('''CREATE OR ALTER VIEW vwWarmQueryPerformancePerWorker AS \n",
    "/*\n",
    "Get the last execution of each unique query that was executed per worker.\n",
    "This will give us the warm query performance for each run.\n",
    "*/\n",
    "SELECT *\n",
    "FROM\n",
    "(\n",
    "\tSELECT\t*\n",
    "\t\t\t,ROW_NUMBER() OVER (PARTITION BY RunId, WorkerNum, QueryUniqueNum ORDER BY QueryRepeatNum DESC) AS RN\n",
    "\tFROM LH_QueryResults.dbo.queryresults\n",
    ") AS a\n",
    "WHERE RN = 1\n",
    ";''')\n",
    "\n",
    "        cursor.execute('''CREATE OR ALTER VIEW vwColdQueryPerformancePerRun AS \n",
    "/*\n",
    "Get the last execution of each unique query that was executed per worker.\n",
    "This will give us the warm query performance for each run.\n",
    "*/\n",
    "SELECT RunId, RunName\n",
    "\t\t,SUM(QueryDurationMS) AS QueryDurationMS\n",
    "\t\t,SUM(QueryCUSeconds) AS QueryCUSeconds\n",
    "\t\t,SUM(QueryCostPayGo) AS QueryCostPayGo\n",
    "\t\t,SUM(QueryCostReserved) AS QueryCostReserved\n",
    "\t\t,SUM(DataScannedDiskMB) AS DataScannedDiskMB\n",
    "\t\t,SUM(DataScannedMemoryMB) AS DataScannedMemoryMB\n",
    "\t\t,SUM(DataScannedRemoteStorageMB) AS DataScannedRemoteStorageMB\n",
    "\t\t,SUM(AllocatedCpuTimeMS) AS AllocatedCpuTimeMS\n",
    "FROM\n",
    "(\n",
    "\tSELECT\tRunId, RunName, QueryDurationMS, QueryCUSeconds, QueryCostPayGo, QueryCostReserved, DataScannedDiskMB\n",
    "\t\t    ,DataScannedMemoryMB, DataScannedRemoteStorageMB, AllocatedCpuTimeMS\n",
    "\t\t\t,ROW_NUMBER() OVER (PARTITION BY RunId, WorkerNum, QueryUniqueNum ORDER BY QueryRepeatNum) AS RN\n",
    "\tFROM LH_QueryResults.dbo.queryresults\n",
    ") AS a\n",
    "WHERE RN = 1\n",
    "GROUP BY RunId, RunName\n",
    ";''')\n",
    "        cursor.execute('''CREATE OR ALTER VIEW vwWarmQueryPerformancePerRun AS \n",
    "/*\n",
    "Get the last execution of each unique query that was executed per worker.\n",
    "This will give us the warm query performance for each run.\n",
    "*/\n",
    "SELECT RunId, RunName\n",
    "\t\t,SUM(QueryDurationMS) AS QueryDurationMS\n",
    "\t\t,SUM(QueryCUSeconds) AS QueryCUSeconds\n",
    "\t\t,SUM(QueryCostPayGo) AS QueryCostPayGo\n",
    "\t\t,SUM(QueryCostReserved) AS QueryCostReserved\n",
    "\t\t,SUM(DataScannedDiskMB) AS DataScannedDiskMB\n",
    "\t\t,SUM(DataScannedMemoryMB) AS DataScannedMemoryMB\n",
    "\t\t,SUM(DataScannedRemoteStorageMB) AS DataScannedRemoteStorageMB\n",
    "\t\t,SUM(AllocatedCpuTimeMS) AS AllocatedCpuTimeMS\n",
    "FROM\n",
    "(\n",
    "\tSELECT\tRunId, RunName, QueryDurationMS, QueryCUSeconds, QueryCostPayGo, QueryCostReserved, DataScannedDiskMB\n",
    "\t\t    ,DataScannedMemoryMB, DataScannedRemoteStorageMB, AllocatedCpuTimeMS\n",
    "\t\t\t,ROW_NUMBER() OVER (PARTITION BY RunId, WorkerNum, QueryUniqueNum ORDER BY QueryRepeatNum DESC) AS RN\n",
    "\tFROM LH_QueryResults.dbo.queryresults\n",
    ") AS a\n",
    "WHERE RN = 1\n",
    "GROUP BY RunId, RunName\n",
    ";''')\n",
    "        cursor.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2906366c-fc4d-465c-9a60-2e782ef4ac15",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Refresh Fabric Capacity Metrics App so that the latest workspace artifacts are in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280025e5-283a-45aa-bb74-bd8ea4e77b39",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "def model_refresh():\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "    response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "    capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "    print(f'{capacityWorkspaceId = }')\n",
    "\n",
    "    response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "    datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "    print(f'{datasetId = }')\n",
    "\n",
    "    response = requests.post(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes\", headers=header)\n",
    "\n",
    "    refreshId = response.headers.get('RequestId')\n",
    "    print(f'{refreshId = } | {response.status_code = }')\n",
    "\n",
    "    if response.status_code == 202:\n",
    "        for attempt in range(12): \n",
    "            # https://learn.microsoft.com/en-us/power-bi/connect-data/asynchronous-refresh#get-refreshes\n",
    "            response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets/{datasetId}/refreshes?$top=1\", headers=header)\n",
    "            if response.status_code == 200:\n",
    "                if response.json().get('value')[0].get('status') != 'Unknown':\n",
    "                    print(f'Refresh Complete')\n",
    "                    break\n",
    "                else:\n",
    "                    print(f'Refreshing tables ...')\n",
    "                    time.sleep(20)\n",
    "            else:\n",
    "                time.sleep(10)\n",
    "    else:\n",
    "        print(f'Refreshed failed - {response.text}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e97c3e-1263-4760-ac21-d95755077b29",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import requests, math\n",
    "\n",
    "def check_if_workspace_exists():\n",
    "  header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "            ,\"Content-Type\": \"application/json\"\n",
    "            }\n",
    "\n",
    "  response = requests.get('https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "\n",
    "  capacityWorkspaceId = [workspace.get('id') for workspace in response.json().get('value') if workspace.get('displayName') == CapacityMetricsWorkspace][0]\n",
    "  print(f'{capacityWorkspaceId = }')\n",
    "\n",
    "  response = requests.get(f\"https://api.powerbi.com/v1.0/myorg/groups/{capacityWorkspaceId}/datasets\", headers=header)\n",
    "\n",
    "  datasetId = [dataset.get('id') for dataset in response.json().get('value') if dataset.get('name') == CapacityMetricsDataset][0]\n",
    "  print(f'{datasetId = }')\n",
    "\n",
    "  body = {\n",
    "    \"queries\": [\n",
    "      {\n",
    "        \"query\": f\"\"\"\n",
    "            DEFINE\n",
    "              VAR __DS0Core = \n",
    "                DISTINCT('Items'[WorkspaceName])\n",
    "                \n",
    "            EVALUATE\n",
    "              __DS0Core\n",
    "      \"\"\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "\n",
    "  response = requests.post(f'https://api.powerbi.com/v1.0/myorg/datasets/{datasetId}/executeQueries', headers=header, json=body )\n",
    "\n",
    "  return response\n",
    "\n",
    "workspaceFound = False\n",
    "response = check_if_workspace_exists()\n",
    "for results in response.json().get('results'):\n",
    "  for table in results.get('tables'):\n",
    "    for row in table.get('rows'):\n",
    "      if row.get('Items[WorkspaceName]') == workspaceName:\n",
    "        print(f'Workspace \"{workspaceName}\" was found in the capacity metrics app model')\n",
    "        workspaceFound = True\n",
    "\n",
    "if not workspaceFound:\n",
    "  for retry in range(10):\n",
    "      model_refresh()\n",
    "      response = check_if_workspace_exists()\n",
    "      for results in response.json().get('results'):\n",
    "          for table in results.get('tables'):\n",
    "              for row in table.get('rows'):\n",
    "                  if row.get('Items[WorkspaceName]') == workspaceName:\n",
    "                      workspaceFound = True\n",
    "      if workspaceFound:\n",
    "          print(f'Workspace \"{workspaceName}\" was found in the capacity metrics app model after doing a model refresh.')\n",
    "          break\n",
    "      else:\n",
    "          print(f'Workspace \"{workspaceName}\" was not found. Waiting 2 minutes and will refresh the model and try again...')\n",
    "          time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b9a783-80e9-4026-bb5c-9d5169b9271f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run the *NB_Query_Cost_Analyzer* notebook. This will execute a sample set of queries against the WH_SampleData and LH_SampleData to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c84e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, time\n",
    "\n",
    "queryList = [\n",
    "        # Transaction\n",
    "        'SELECT COUNT(*) FROM FactTransaction'\n",
    "        ,'''SELECT\tCOUNT(*)\n",
    "        FROM\tFactTransaction AS ft\n",
    "        JOIN\tDimDate AS d\n",
    "        ON\t\td.Date = ft.DateKey\n",
    "        JOIN\tDimPaymentMethod AS pm\n",
    "        ON\t\tpm.PaymentMethodKey= ft.PaymentMethodKey\n",
    "        JOIN\tDimTransactionType AS tt\n",
    "        ON\t\ttt.TransactionTypeKey = ft.TransactionTypeKey\n",
    "        JOIN\tDimSupplier AS s\n",
    "        ON\t\ts.SupplierKey = ft.SupplierKey\n",
    "        JOIN\tDimCustomer AS cu\n",
    "        ON\t\tcu.CustomerKey = ft.CustomerKey\n",
    "        JOIN\tDimCustomer AS cuBill\n",
    "        ON\t\tcuBill.CustomerKey = ft.BillToCustomerKey'''\n",
    "        # Order\n",
    "        ,'SELECT COUNT(*) cnt_FactOrder FROM FactOrder'\n",
    "        ,'''SELECT\tCOUNT(*)\n",
    "        FROM\tFactOrder AS fo\n",
    "        JOIN\tDimDate AS dOrder\n",
    "        ON\t\tdOrder.Date = fo.OrderDateKey\n",
    "        LEFT JOIN\tDimDate AS dPicked\n",
    "        ON\t\tdPicked.Date = fo.PickedDateKey\n",
    "        JOIN\tDimStockItem AS si\n",
    "        ON\t\tsi.StockItemKey = fo.StockItemKey\n",
    "        JOIN\tDimCity AS c\n",
    "        ON\t\tc.CityKey = fo.CityKey\n",
    "        JOIN\tDimEmployee AS e\n",
    "        ON\t\te.EmployeeKey = fo.SalespersonKey\n",
    "        JOIN\tDimEmployee AS ePicker\n",
    "        ON\t\tePicker.EmployeeKey = fo.PickerKey\n",
    "        JOIN\tDimCustomer AS cu\n",
    "        ON\t\tcu.CustomerKey = fo.CustomerKey'''\n",
    "        # FactMovement\n",
    "        ,'SELECT COUNT(*) cnt_FactMovement FROM FactMovement'\n",
    "        ,'''SELECT\tCOUNT(*)\n",
    "        FROM\tFactMovement AS fm\n",
    "        JOIN\tDimDate AS d\n",
    "        ON\t\td.Date = fm.DateKey\n",
    "        JOIN\tDimStockItem AS si\n",
    "        ON\t\tsi.StockItemKey = fm.StockItemKey\n",
    "        JOIN\tDimTransactionType AS tt\n",
    "        ON\t\ttt.TransactionTypeKey = fm.TransactionTypeKey\n",
    "        JOIN\tDimSupplier AS s\n",
    "        ON\t\ts.SupplierKey = fm.SupplierKey\n",
    "        JOIN\tDimCustomer AS c\n",
    "        ON\t\tc.CustomerKey = fm.CustomerKey'''\n",
    "        # FactPurchase\n",
    "        ,'SELECT COUNT(*) cnt_FactPurchase FROM FactPurchase'\n",
    "        ,'''SELECT\tCOUNT(*)\n",
    "        FROM\tFactPurchase AS fp\n",
    "        JOIN\tDimDate AS d\n",
    "        ON\t\td.Date = fp.DateKey\n",
    "        JOIN\tDimStockItem AS si\n",
    "        ON\t\tsi.StockItemKey = fp.StockItemKey\n",
    "        JOIN\tDimSupplier AS s\n",
    "        ON\t\ts.SupplierKey = fp.SupplierKey'''\n",
    "        # StockHolding\n",
    "        ,'SELECT COUNT(*) AS cnt_FactStockHolding FROM FactStockHolding'\n",
    "        ,'''SELECT\tCOUNT(*)\n",
    "        FROM\tFactStockHolding AS fsh\n",
    "        JOIN\tDimStockItem AS si\n",
    "        ON\t\tsi.StockItemKey = fsh.StockItemKey'''\n",
    "            ,'''SELECT\tTOP 100 fsh.*\n",
    "        FROM\tFactStockHolding AS fsh\n",
    "        JOIN\tDimStockItem AS si\n",
    "        ON\t\tsi.StockItemKey = fsh.StockItemKey'''\n",
    "        ,'''EXEC sp_Query'''\n",
    "    ]\n",
    "\n",
    "for artifactType in ['LH_SampleData', 'WH_SampleData']:\n",
    "    print(f'Running queries for {artifactType}')\n",
    "\n",
    "    header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "          }\n",
    "    \n",
    "    body = {\n",
    "        \"executionData\": {\n",
    "            \"parameters\": {\n",
    "                \"FabricDWWorkspaceName\": {\"value\": workspaceName, \"type\": \"string\"}\n",
    "                ,\"FabricDWName\": {\"value\": artifactType, \"type\": \"string\"}\n",
    "                ,\"ConcurrencyNum\": {\"value\": \"1\", \"type\": \"int\"}\n",
    "                ,\"CapacityMetricsWorkspace\": {\"value\": CapacityMetricsWorkspace, \"type\": \"string\"}\n",
    "                ,\"CapacityMetricsDataset\": {\"value\": CapacityMetricsDataset, \"type\": \"string\"}\n",
    "                ,\"QueryList\": {\"value\": str(queryList), \"type\": \"string\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/items/{notebookId}/jobs/instances?jobType=RunNotebook', headers=header, data=json.dumps(body))\n",
    "\n",
    "    startTime= time.time()\n",
    "    if response.status_code == 202:\n",
    "        print('Started Execution of Notebook')\n",
    "        time.sleep(10) # Sleep for 10 seconds to wait for job to get started.\n",
    "        while True:\n",
    "            responseStatus = requests.request(method=\"get\", url=response.headers.get('Location'), headers=header)\n",
    "            print(f\"Status: {responseStatus.json().get('status')} | {int(time.time()-startTime)} seconds {'|' + str(responseStatus.json().get('failureReason')) if responseStatus.json().get('failureReason') != None else ''}\", end='\\r')\n",
    "            if responseStatus.json().get('status') in ['Succeeded', 'Failed', 'Completed']:\n",
    "                print(f\"Status: {responseStatus.json().get('status')} | {int(time.time()-startTime)} seconds {'|' + str(responseStatus.json().get('failureReason')) if responseStatus.json().get('failureReason') != None else ''}\")\n",
    "                break\n",
    "            else:\n",
    "                time.sleep(60) # Wait 1 minute to check the status of the run\n",
    "    else:\n",
    "        print('Error trying to execute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load(f'{LH_QueryResults.properties.get(\"abfsPath\")}/Tables/runresults'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b724fa98",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load(f'{LH_QueryResults.properties.get(\"abfsPath\")}/Tables/queryresults'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2047e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.read.format('delta').load(f'{LH_QueryResults.properties.get(\"abfsPath\")}/Tables/queryinsightsresults'))"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default"
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
