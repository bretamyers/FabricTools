{"cells":[{"cell_type":"markdown","source":["### Parameters to update\n","##### Make sure notebook is attached to a lakehouse to log results\n","##### Two tables will be created. QueryResults and RunResults"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d94e7d5b-db72-4777-b700-297f8baaaf13"},{"cell_type":"code","source":["fabricDWWorkspaceName = 'WS_Temp' # '4d0c3d99-d190-42d4-b844-973ca02d1b7c'\n","fabricDWName = 'WH_Test'\n","concurrencyNum = 24 # This should be equal or greater than the length of the dataframe with the queryies defined below\n","capacity_metrics_workspace = 'Microsoft Fabric Capacity Metrics'\n","capacity_metrics_dataset = 'Fabric Capacity Metrics'"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":62,"statement_ids":[62],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.1159782Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:45.5023226Z","execution_finish_time":"2024-09-19T13:01:45.7784933Z","parent_msg_id":"75146105-e45e-4e89-b647-892851956983"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 62, Finished, Available, Finished)"},"metadata":{}}],"execution_count":60,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"20be0399-9430-4b52-a3ab-1c911fc0d69a"},{"cell_type":"code","source":["queryList = [\n","    'SELECT\tCOUNT_BIG(*) FROM\ttpch1tb.lineitem AS l JOIN\ttpch1tb.orders AS o ON\t\to.o_orderkey = l.l_orderkey'\n","    ,'SELECT\tCOUNT_BIG(*) FROM\ttpch1tb.lineitem AS l JOIN\ttpch1tb.orders AS o ON\t\to.o_orderkey = l.l_orderkey'\n","    ,'SELECT\tCOUNT_BIG(*) FROM\ttpch1tb.lineitem AS l JOIN\ttpch1tb.orders AS o ON\t\to.o_orderkey = l.l_orderkey'\n","    ,'SELECT\tCOUNT_BIG(*) FROM\ttpch1tb.lineitem AS l JOIN\ttpch1tb.orders AS o ON\t\to.o_orderkey = l.l_orderkey'\n","    ,'SELECT\tCOUNT_BIG(*) FROM\ttpch1tb.lineitem AS l JOIN\ttpch1tb.orders AS o ON\t\to.o_orderkey = l.l_orderkey'\n","]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":63,"statement_ids":[63],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.1880228Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:47.8302928Z","execution_finish_time":"2024-09-19T13:01:48.06828Z","parent_msg_id":"626ae391-220f-4e66-9ffc-8b40c8db1c95"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 63, Finished, Available, Finished)"},"metadata":{}}],"execution_count":61,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd4b5a5a-dfa7-43b0-b8a7-b4b3849a7559"},{"cell_type":"code","source":["rddQueries = sc.parallelize(queryList, maxConcurrency)\n","rddQueriesWithId = rddQueries.zipWithUniqueId().map(lambda x: [x[1], (x[1], x[0])] )#, tokenstruct, runId]) # zipWithUniqueId is faster than ZipWithIndex but could create gaps in the Ids generated\n","rddQueriesWithId = rddQueriesWithId.partitionBy(maxConcurrency, lambda k: k ) # is this even needed?\n","print(rddQueriesWithId.glom().map(len).collect())  # Get length of each partition to check for even distribution of rows in the partitions. This will tell us if the number of queries are evenly distributed\n","\n","executorCoreCnt = int(spark.conf.get('spark.executor.cores'))\n","executorInstances = int(spark.conf.get('spark.executor.instances'))\n","maxConcurrency = concurrencyNum if concurrencyNum < (executorCoreCnt * executorInstances) else (executorCoreCnt * executorInstances)\n","\n","# TODO adjust verbage of this\n","# print(f'Max currency of spark session is {(executorCoreCnt * executorInstances)}\\nDefined concurrency is {concurrencyNum}\\nCan only run {maxConcurrency} queries concurrently for this spark session')\n","displayHTML(f\"\"\"\n","<p><span style=\"font-size:20px;\"><strong>Max currency of spark session is </strong><i><strong>{maxConcurrency}</strong></i></span></p>\n","<p><span style=\"font-size:20px;\"><strong>Defined concurrency is </strong><i><strong>{concurrencyNum}</strong></i></span></p>\n","<p><span style=\"font-size:20px;\"><strong>Can only run </strong><i><strong>{maxConcurrency}</strong></i><strong> queries concurrently for this spark session</strong></span></p>\n","\"\"\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":64,"statement_ids":[64],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.2790604Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:48.4597829Z","execution_finish_time":"2024-09-19T13:01:49.2635206Z","parent_msg_id":"9d1c733d-762e-4246-bb09-0adde8077305"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 64, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1]\n"]},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<p><span style=\"font-size:20px;\"><strong>Max currency of spark session is </strong><i><strong>24</strong></i></span></p>\n<p><span style=\"font-size:20px;\"><strong>Defined concurrency is </strong><i><strong>24</strong></i></span></p>\n<p><span style=\"font-size:20px;\"><strong>Can only run </strong><i><strong>24</strong></i><strong> queries concurrently for this spark session</strong></span></p>\n"},"metadata":{}}],"execution_count":62,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de6ed022-d969-45cd-8fd2-93b34357f16f"},{"cell_type":"code","source":["import requests\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n","\n","while True:\n","    workspaceFound = False\n","    for workspace in response.json().get('value'):\n","        if workspace.get('displayName') == fabricDWWorkspaceName:\n","            fabricDWWorkspaceId = workspace.get('id')\n","            workspaceFound = True\n","            break\n","\n","    if workspaceFound == False and response.get('continuationToken'):\n","        response = requests.request(method='get', url=response.get('continuationUri'), headers=header)\n","    else:\n","        break\n","\n","print(f'{fabricDWWorkspaceId = }\\n{fabricDWWorkspaceName = }')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":65,"statement_ids":[65],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.3818571Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:49.6923366Z","execution_finish_time":"2024-09-19T13:01:50.5845842Z","parent_msg_id":"ead85237-dbd9-4a87-bc30-4241eb59072e"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 65, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["fabricDWWorkspaceId = '4d0c3d99-d190-42d4-b844-973ca02d1b7c'\nfabricDWWorkspaceName = 'WS_Temp'\n"]}],"execution_count":63,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f726386-0439-4ceb-9f96-b6f562e01135"},{"cell_type":"code","source":["import requests\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n","workspaceName = response.json().get('displayName')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n","capacityId = response.json().get('capacityId')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/capacities', headers=header)\n","for capacity in response.json().get('value'):\n","    if capacity.get('id') == capacityId:\n","        capacityRegion = capacity.get('region')\n","        capacityName = capacity.get('displayName')\n","        capacitySku = capacity.get('sku')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/warehouses', headers=header)\n","warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == fabricDWName][0]\n","fabricDWServer = warehouse.get('properties').get('connectionString')\n","warehouseId = warehouse.get('id')\n","\n","print(f'{warehouseId = }\\n{fabricDWServer = }\\n{workspaceName = }\\n{capacityId = }\\n{capacityRegion = }\\n{capacityName = }\\n{capacitySku = }')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":66,"statement_ids":[66],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.5138877Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:50.9579167Z","execution_finish_time":"2024-09-19T13:01:57.3415251Z","parent_msg_id":"ce6bdd2d-2089-4ff2-b98f-26da1f29b137"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 66, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["warehouseId = 'd422c452-c440-4bc6-8740-cf23ddc5d007'\nfabricDWServer = 'n52dzjnhphme5jslxpytuo62ni-te6qytmq2hkefoces46kali3pq.datawarehouse.fabric.microsoft.com'\nworkspaceName = 'WS_Temp'\ncapacityId = 'e36151a3-66ab-4bc5-b4f9-8a6e39f79d95'\ncapacityRegion = 'East US 2'\ncapacityName = 'fabricdemobam'\ncapacitySku = 'F32'\n"]}],"execution_count":64,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f1fd34b-c182-47ff-8d87-d8fdd550ad3e"},{"cell_type":"code","source":["response = requests.request(method='get', url=\"https://prices.azure.com/api/retail/prices?$filter=skuName eq 'Fabric Capacity'\", headers=header)\n","for capacity in response.json().get('Items'):\n","    if capacity.get('armRegionName') == capacityRegion.replace(' ', '').lower():\n","        costReserved = capacity.get('retailPrice') / 12 / 730 / 60 / 60 # get the amount per CU second\n","        costPayGo = costReserved / (156.334/262.80) # constant saving of ~41%. 156.334 is the resevered price of a region. 262.80 is the paygo price of a region\n","print(f'{costReserved = :.10f}\\n{costPayGo = :.10f}') # per CU"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.6945197Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:57.7229365Z","execution_finish_time":"2024-09-19T13:01:58.5142409Z","parent_msg_id":"cd001c23-2fc9-435e-a493-f417ee268db2"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 67, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["costReserved = 0.0000297438\ncostPayGo = 0.0000499998\n"]}],"execution_count":65,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6509449a-48a5-4da1-a7c7-43a75ecd7325"},{"cell_type":"markdown","source":["##### Define the queries to be executed. These are single line queries so use /* */ for commenting out code vs --"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e5bcecd-ac6c-45bf-9ebb-66fe8949ab54"},{"cell_type":"code","source":["from notebookutils import mssparkutils  \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n","import pyodbc, struct, itertools, time, datetime, re, uuid, json\n","\n","connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={fabricDWServer};Database={fabricDWName}'\n","\n","# Use the credentials of the user executing the notebook\n","token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n","encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n","tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n","\n","runId = str(uuid.uuid4())\n","runStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","runStartTimeEpoch = int(runStartDateTimeUTC.timestamp()*1000)\n","\n","print(f'{runId = }\\n{runStartDateTimeUTC = }\\n{runStartTimeEpoch = }')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":68,"statement_ids":[68],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:45.8608956Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:58.9066062Z","execution_finish_time":"2024-09-19T13:01:59.1449559Z","parent_msg_id":"784b9883-bd9f-448d-837b-6afa4b45165e"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 68, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["runId = 'b40ba9ea-05c5-462e-90c1-2763ac05989c'\nrunStartDateTimeUTC = datetime.datetime(2024, 9, 19, 13, 1, 58, 850264, tzinfo=datetime.timezone.utc)\nrunStartTimeEpoch = 1726750918850\n"]}],"execution_count":66,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44a4596a-e954-4ccc-aa49-0d32c1500013"},{"cell_type":"code","source":["from delta.tables import *\n","\n","with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","    with conn.cursor() as cursor:\n","        cursor.execute('''SELECT @@VERSION AS DWVersion\n","                            ,@@SERVERNAME AS ServerGuid\n","                            ,DB_NAME() AS DWName\n","        ''')\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","        cursor.commit()\n","        resultSet = [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","        cursor.execute(f'''SELECT [is_vorder_enabled] AS IsVOrderEnabled, [data_lake_log_publishing_desc] AS DataLakeLogPublishingDesc\n","                            ,[data_lake_log_publishing] AS DataLakeLogPublishing, [create_date] AS DWCreateDate, [compatibility_level] AS CompatibilityLevel\n","                            FROM sys.databases \n","                            WHERE [name] = '{fabricDWName}'\n","            ''')\n","\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","        cursor.commit()\n","        resultSet.extend([dict(zip(resultColumns, [str(col) for col in row])) for row in resultList])\n","\n","        df = spark.createDataFrame([dict(zip(resultColumns, [str(col) for col in row])) for row in resultList])\n","\n","dfRun = (df.withColumn('RunStartDateTimeUTC', F.lit(runStartDateTimeUTC).cast(TimestampType()))\n","            .withColumn('RunStartTimeEpoch', F.lit(runStartTimeEpoch).cast(LongType()))\n","            .withColumn('RunId', F.lit(runId).cast(StringType()))\n","            .withColumn('DWConnectionString', F.lit(fabricDWServer).cast(StringType()))\n","            .withColumn('QueriesExecutedCnt', F.lit(len(queryList)).cast(IntegerType()))\n","            .withColumn('RunConcurrency', F.lit(maxConcurrency).cast(IntegerType()))\n","\n","            .withColumn('DWGuid', F.lit(warehouseId).cast(StringType())) # TODO\n","            .withColumn('WorkspaceName', F.lit(workspaceName).cast(StringType())) # TODO\n","            .withColumn('WorkspaceGuid', F.lit(fabricDWWorkspaceId).cast(StringType())) # TODO\n","            .withColumn('CapacityName', F.lit(capacityName).cast(StringType())) # TODO\n","            .withColumn('CapacityGuid', F.lit(capacityId).cast(StringType())) # TODO\n","            .withColumn('CapacitySKU', F.lit(capacitySku).cast(StringType())) # TODO\n","            .withColumn('CapacityRegion', F.lit(capacityRegion).cast(StringType())) # TODO\n","\n","            .withColumn('RunEndDatetimeUTC', F.lit(None).cast(TimestampType()))\n","            .withColumn('RunEndTimeEpochMS', F.lit(None).cast(LongType()))\n","            .withColumn('RunDurationMS', F.lit(None).cast(LongType()))\n","            .withColumn('RunCUSeconds', F.lit(None).cast(DoubleType()))\n","            .withColumn('RunCostPayGo', F.lit(None).cast(DoubleType()))\n","            .withColumn('RunCostReserved', F.lit(None).cast(DoubleType()))\n","            \n","            .withColumn('CapacityDailyCUSeconds', F.lit(60*60*24*int(capacitySku.split('F')[1])))\n","            .withColumn('CapacityDailyCostPayGo', F.lit(costPayGo * 60*60*24*int(capacitySku.split('F')[1])))\n","            .withColumn('CapacityDailyCostReserved', F.lit(costReserved * 60*60*24*int(capacitySku.split('F')[1])))\n","        )\n","\n","\n","# # TODO add a merge so that if you need to rerun from this step, it doesn't insert a new record\n","dfRun.select(\"RunId\"\n","        ,\"DWConnectionString\"\n","        ,\"QueriesExecutedCnt\"\n","        ,\"RunConcurrency\"\n","        ,\"DWGuid\"\n","        ,\"WorkspaceName\"\n","        ,\"WorkspaceGuid\"\n","        ,\"CapacityName\"\n","        ,\"CapacityGuid\"\n","        ,\"CapacitySKU\"\n","        ,\"CapacityRegion\"\n","        ,\"CompatibilityLevel\"\n","        ,\"DWCreateDate\"\n","        ,\"DataLakeLogPublishing\"\n","        ,\"DataLakeLogPublishingDesc\"\n","        ,\"IsVOrderEnabled\"\n","        ,\"RunStartDateTimeUTC\"\n","        ,\"RunStartTimeEpoch\"\n","        ,\"RunEndDatetimeUTC\"\n","        ,\"RunEndTimeEpochMS\"\n","        ,\"RunDurationMS\"\n","        ,\"RunCUSeconds\"\n","        ,\"RunCostPayGo\"\n","        ,\"RunCostReserved\"\n","        ,\"CapacityDailyCUSeconds\"\n","        ,\"CapacityDailyCostPayGo\"\n","        ,\"CapacityDailyCostReserved\"\n","    )\n","\n","if spark.catalog.tableExists(\"RunResults\"):\n","\n","    dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","\n","    (dtRunResults.alias('t')\n","        .merge(dfRun.alias('s')\n","            ,f't.runId = s.RunId'\n","            )\n","        .whenNotMatchedInsertAll()\n","    ).execute() \n","else:\n","    dfRun.write.format('delta').mode('append').saveAsTable('RunResults')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":69,"statement_ids":[69],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.0026689Z","session_start_time":null,"execution_start_time":"2024-09-19T13:01:59.5505936Z","execution_finish_time":"2024-09-19T13:02:07.7162996Z","parent_msg_id":"b6dad566-f0d8-43ed-b539-f2f0a1439eab"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 69, Finished, Available, Finished)"},"metadata":{}}],"execution_count":67,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f1633b8f-665d-4393-a63c-174effbb1ba9"},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\n","import pyodbc \n","\n","def get_result_set(cursor):\n","    if cursor.description:\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","    else:\n","        resultList = []\n","        resultColumns = []\n","    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","def execute_query(iterator):\n","    queryMetrics = []\n","    for queryInfo in iterator:\n","        queryId = queryInfo[1][0]\n","        queryStatement = queryInfo[1][1]\n","        with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","            with conn.cursor() as cursor:\n","                queryStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","                startTime = int(round(time.time() * 1000))\n","\n","                cursor.execute(queryStatement)\n","                endTime = int(round(time.time() * 1000))\n","                queryEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","                \n","                queryMessage = str(cursor.messages) if cursor.messages else \"\"\n","                resultSet = get_result_set(cursor)\n","\n","                while cursor.nextset():\n","                    queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n","                    resultSet.append(get_result_set(cursor))\n","                \n","                statementId = ','.join(re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n","                queryHash = ','.join(re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage)) if re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage) else \"\"\n","                distributionRequestId = ','.join(re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n","                resultSetJsonString = json.dumps(resultSet)\n","\n","                cursor.commit()\n","\n","                queryId = str(uuid.uuid4())\n","                queryMetrics.append([runId, queryId, queryStatement, queryStartDateTimeUTC, queryEndDateTimeUTC\n","                        ,queryMessage, startTime, endTime, endTime - startTime\n","                        ,statementId, queryHash, distributionRequestId, resultSetJsonString\n","                        ])\n","    return queryMetrics\n","\n","queriesExecuted = rddQueriesWithId.mapPartitions(execute_query)\n","\n","dfQueriesExecuted = queriesExecuted.toDF(schema=StructType([\n","    StructField(\"RunId\", StringType(), False),\n","    StructField(\"QueryId\", StringType(), False),\n","    StructField(\"QueryStatement\", StringType(), False),\n","    StructField(\"QueryStartDateTimeUTC\", TimestampType(), False),\n","    StructField(\"QueryEndDateTimeUTC\", TimestampType(), False),\n","    StructField(\"ReturnMessage\", StringType(), False),\n","    StructField(\"QueryStartTimeEpochMS\", LongType(), False),\n","    StructField(\"QueryEndTimeEpochMS\", LongType(), False),\n","    StructField(\"QueryDurationMS\", LongType(), False),\n","    StructField(\"StatementId\", StringType(), False),\n","    StructField(\"QueryHash\", StringType(), False),\n","    StructField(\"DistributionRequestId\", StringType(), False),\n","    StructField(\"ResultSet\", StringType(), False)\n","    ]))\n","\n","dfFinal = dfQueriesExecuted.withColumn('QueryCUSeconds', F.lit(None).cast(DoubleType())).withColumn('QueryCostPayGo', F.lit(None).cast(DoubleType())).withColumn('QueryCostReserved', F.lit(None).cast(DoubleType()))\n","dfFinal.write.format('delta').mode('append').saveAsTable('QueryResults')\n","\n","runEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","runEndTimeEpoch = int(runEndDateTimeUTC.timestamp()*1000)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":70,"statement_ids":[70],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.1275598Z","session_start_time":null,"execution_start_time":"2024-09-19T13:02:08.1401869Z","execution_finish_time":"2024-09-19T13:05:17.5955243Z","parent_msg_id":"86474b0b-f096-478b-a67f-6269eb1aa2e6"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 70, Finished, Available, Finished)"},"metadata":{}}],"execution_count":68,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"advisor":{"adviceMetadata":"{\"artifactId\":\"b80b83a7-fee0-4a1d-9694-b7a020bd5977\",\"activityId\":\"e469517b-9eab-489b-8060-1288568c8db5\",\"applicationId\":\"application_1726748724620_0001\",\"jobGroupId\":\"70\",\"advices\":{\"warn\":1}}"}},"id":"a7a25284-d19a-4120-b2bf-9326efd16033"},{"cell_type":"code","source":["statementList = spark.sql(f'SELECT ARRAY_JOIN(COLLECT_SET(CONCAT(\"\\\\\"\", StatementId, \"\\\\\"\")), \", \") AS Statements FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('Statements')\n","# We have to explode by statement ids since a sql query may have multiple queries within it\n","queriesExecutedCnt = spark.sql(f'SELECT COUNT(StatementId) AS QueryCnt FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('QueryCnt') \n","print(f'{runId = }\\n{statementList = }\\n{queriesExecutedCnt = }')"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":71,"statement_ids":[71],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.2629282Z","session_start_time":null,"execution_start_time":"2024-09-19T13:05:18.0701176Z","execution_finish_time":"2024-09-19T13:05:20.4815105Z","parent_msg_id":"392c0b61-6fb6-47bf-bf57-db2abb66f0a2"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 71, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["runId = 'b40ba9ea-05c5-462e-90c1-2763ac05989c'\nstatementList = '\"075E623B-2BFD-4281-A7C9-FE2FED347ED8\", \"CE0A35FB-C568-4B73-9752-091675144B17\", \"F12AE7BF-04C2-4015-AAA1-E4E0FE8A51F4\", \"0CD1D140-8FD4-4309-A941-B73228AE2782\", \"E6CB6918-0184-44AB-BE7F-C900C699D49F\"'\nqueriesExecutedCnt = 5\n"]}],"execution_count":69,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"fdf916df-8027-4011-a8ec-dd3fedc1c2b2"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","\n","df_final = spark.createDataFrame(data=[(runEndDateTimeUTC, runEndTimeEpoch, )], schema=['RunEndDateTimeUTC', 'RunEndTimeEpochMS'])\n","\n","(dtRunResults.alias('t')\n","    .merge(df_final.alias('s')\n","        ,f't.RunId = \"{runId}\"'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'RunEndDatetimeUTC': 's.RunEndDateTimeUTC'\n","        ,'RunEndTimeEpochMS': 's.RunEndTimeEpochMS'\n","        ,'RunDurationMS': 's.RunEndTimeEpochMS - t.RunStartTimeEpoch'\n","        }\n","        )\n",").execute() "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":72,"statement_ids":[72],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.3732175Z","session_start_time":null,"execution_start_time":"2024-09-19T13:05:20.9077093Z","execution_finish_time":"2024-09-19T13:05:27.2863226Z","parent_msg_id":"72ac3edd-14fe-497d-9d9c-cef6692c96a0"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 72, Finished, Available, Finished)"},"metadata":{}}],"execution_count":70,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d44baaf-dc59-433f-8b7e-c5a2a124f3fe"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from datetime import timedelta\n","from pyspark.sql.functions import col, lit, sum, min, max\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n","\n","def get_capacity_metrics_usage(time_point, operation_id_list):\n","\t\n","\tschema = StructType([\n","\t\tStructField(\"Items[ItemId]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"Items[ItemKind]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"Items[ItemName]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationStartTime]\", \tTimestampType(), \tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationEndTime]\", \t\tTimestampType(), \tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationId]\", \t\t\tStringType(), \t\tTrue),\n","\t\tStructField(\"Sum_CUs\", \t\tDoubleType(), \t\tTrue),\n","\t\tStructField(\"Sum_Duration\", IntegerType(), \t\tTrue)\n","\t])\n","\n","\tdax_command = f\"\"\"\n","\tDEFINE\n","\t\tMPARAMETER 'CapacityID' \t= \"{capacityId}\"\n","\t\tMPARAMETER 'TimePoint' \t\t= (DATE({time_point.year}, {time_point.month}, {time_point.day}) + TIME({time_point.hour}, {time_point.minute}, {time_point.second}))\n","\n","\t\tVAR __Var_CapacityId\t= {{\"{capacityId}\"}}\n","\t\tVAR __Var_OperationId\t= {{{statementList}}}\n","\n","\t\tVAR __Filter_OperationId \t= TREATAS(__Var_OperationId, 'TimePointBackgroundDetail'[OperationId])\n","\t\tVAR __Filter_CapacityId \t= TREATAS(__Var_CapacityId, 'Capacities'[capacityId])\n","\n","\t\tVAR OperationCUs = \n","\t\t\tSUMMARIZECOLUMNS(\n","\t\t\t\t'Items'[ItemId],\n","\t\t\t\t'Items'[ItemKind],\n","\t\t\t\t'Items'[ItemName],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationStartTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationEndTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationId],\n","\t\t\t\t__Filter_OperationId,\n","\t\t\t\t__Filter_CapacityId,\n","\t\t\t\t\"Sum_CUs\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n","\t\t\t\t\"Sum_Duration\", CALCULATE(SUM('TimePointBackgroundDetail'[Duration (s)]))\n","\t\t\t)\n","\n","\tEVALUATE\n","\t\tOperationCUs\n","\t\"\"\"\n","\n","\tdf_dax = fabric.evaluate_dax(dax_string = dax_command, dataset = capacity_metrics_dataset, workspace = capacity_metrics_workspace)\n","\n","\tdf = (spark.createDataFrame(data=df_dax, schema=schema).withColumn(\"TimePoint\", lit(time_point)).select(\n","\t\t\tcol(\"TimePoint\")\n","\t\t\t,col(\"Items[ItemId]\").alias(\"ItemId\")\n","\t\t\t,col(\"Items[ItemKind]\").alias(\"ItemKind\")\n","\t\t\t,col(\"Items[ItemName]\").alias(\"ItemName\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationStartTime]\").alias(\"StartTime\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationEndTime]\").alias(\"EndTime\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationId]\").alias(\"OperationId\")\n","\t\t\t,col(\"Sum_CUs\").cast(DoubleType()).alias(\"Sum_CUs\")\n","\t\t\t,col(\"Sum_Duration\").cast(IntegerType()).alias(\"Sum_Duration\"))\n","\t\t)\n","\n","\treturn df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":73,"statement_ids":[73],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.5087044Z","session_start_time":null,"execution_start_time":"2024-09-19T13:05:27.7880254Z","execution_finish_time":"2024-09-19T13:05:28.0736771Z","parent_msg_id":"49c514ce-610b-4c2d-bef6-69d9cab49b78"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 73, Finished, Available, Finished)"},"metadata":{}}],"execution_count":71,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b09fd036-6768-4fe9-b1f9-fea79ed5e7b0"},{"cell_type":"code","source":["'''\n","Get the capacity usage for the queries that were executed.\n","This ensures that the background operations are captured no matter what time they are run as they are smoothed over a 24 hour time period.\n","Next, filter that down to the distinct records. This is necessary because a record may show up in the today and tomorrow datasets depending on the time it was run.\n","Nest, aggregate the records into a single record. This is necessary becuase some operations will have two entries, one under the executing user and one under the user \"System\".\n","'''\n","\n","# # Continues to query the metrica app to get the data. Data can delayed by a few minutes.\n","# # We retry every minute until 15 minutes has passed.\n","for retryCnt in range(15):\n","    df_today    = get_capacity_metrics_usage(runStartDateTimeUTC, statementList)\n","    df_tomorrow = get_capacity_metrics_usage(runStartDateTimeUTC + timedelta(hours = 24), statementList)\n","    df_all_days = df_today.unionAll(df_tomorrow)\n","    df_distinct = df_all_days.select('ItemId', 'ItemKind', 'ItemName', 'StartTime', 'EndTime', 'OperationId', 'Sum_CUs', 'Sum_Duration').distinct()\n","    df_final    = df_distinct.groupBy('ItemId', 'ItemKind', 'ItemName', 'OperationId').agg(min(\"StartTime\").alias(\"StartTime\"), max(\"EndTime\").alias(\"EndTime\"), sum(\"Sum_CUs\").alias(\"QueryCUSeconds\"), sum(\"Sum_Duration\").alias(\"SumDuration\"))\n","\n","    print(f'{df_final.count()} queries of the {queriesExecutedCnt} that have been found in the capacity metrics model. ', end='')\n","    if df_final.count() == queriesExecutedCnt:\n","        break\n","    else:\n","        print('Sleeping for a minute...')\n","        time.sleep(60)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":74,"statement_ids":[74],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:46.5932334Z","session_start_time":null,"execution_start_time":"2024-09-19T13:05:28.5047676Z","execution_finish_time":"2024-09-19T13:09:11.2179958Z","parent_msg_id":"cf65aea8-c7a7-4bfb-890d-ba5551bbad99"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 74, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0 queries of the 5 that have been found in the capacity metrics model. Sleeping for a minute...\n0 queries of the 5 that have been found in the capacity metrics model. Sleeping for a minute...\n0 queries of the 5 that have been found in the capacity metrics model. Sleeping for a minute...\n5 queries of the 5 that have been found in the capacity metrics model. "]}],"execution_count":72,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d68bfa2-1f97-47af-a21c-230459507da7"},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n","\n","dfQueryResults = spark.table(\"QueryResults\")\n","\n","dfQueryResultsCleansed = (dfQueryResults.join(df_final, dfQueryResults.StatementId.contains(df_final.OperationId)) \n","    .filter(dfQueryResults.RunId == runId) \n","    .groupBy(dfQueryResults.RunId, dfQueryResults.StatementId) \n","    .agg(sum(df_final.QueryCUSeconds).alias(\"QueryCUSeconds\"), sum(df_final.SumDuration).alias(\"SumDuration\")\n","    ))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":80,"statement_ids":[80],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:10:12.346969Z","session_start_time":null,"execution_start_time":"2024-09-19T13:10:12.753257Z","execution_finish_time":"2024-09-19T13:10:13.5204509Z","parent_msg_id":"c76246cb-a4b9-486b-b6ef-20f8d85d9b10"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 80, Finished, Available, Finished)"},"metadata":{}}],"execution_count":78,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f9ffd41b-12e4-4c5a-aeae-0dccf30add3e"},{"cell_type":"markdown","source":["##### Update the QueryResults table with the CUSeconds and QueryCost derived from the Capacity Metrics App"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13aa53bc-c865-491d-881a-3206e8cfbfad"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtQueryResults = DeltaTable.forName(spark, \"QueryResults\")\n","\n","(dtQueryResults.alias('t')\n","    .merge(dfQueryResultsCleansed.alias('s')\n","        ,f't.runId = s.RunId AND t.StatementId = s.StatementId'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'QueryCUSeconds': 's.QueryCUSeconds'\n","        ,'QueryCostPayGo': f's.QueryCUSeconds * {costPayGo}'\n","        ,'QueryCostReserved': f's.QueryCUSeconds * {costReserved}'\n","        }\n","        )\n",").execute() "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":81,"statement_ids":[81],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:10:16.5431835Z","session_start_time":null,"execution_start_time":"2024-09-19T13:10:16.9841516Z","execution_finish_time":"2024-09-19T13:10:23.5206205Z","parent_msg_id":"26abe375-a5c0-4c6b-b5ce-7dbdcdc4acd8"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 81, Finished, Available, Finished)"},"metadata":{}}],"execution_count":79,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b93c6bdf-7984-489a-b3bf-c3de6080aa60"},{"cell_type":"markdown","source":["##### Update the RunResults table with cost of run"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8343c8b6-52c9-414d-8bf7-962982409336"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","dtRunResultsCleansed = spark.sql(f'SELECT SUM(COALESCE(QueryCUSeconds, 0)) AS RunCUSeconds, SUM(COALESCE(QueryCostPayGo, 0)) AS RunCostPayGo, SUM(COALESCE(QueryCostReserved, 0)) AS RunCostReserved FROM QueryResults WHERE RunId = \"{runId}\"')\n","\n","(dtRunResults.alias('t')\n","    .merge(dtRunResultsCleansed.alias('s')\n","        ,f't.RunId = \"{runId}\"'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'RunCUSeconds': 's.RunCUSeconds'\n","        ,'RunCostPayGo': f's.RunCUSeconds * {costPayGo}'  # This could be different looking at the tables separately due to rounding\n","        ,'RunCostReserved': f's.RunCUSeconds * {costReserved}'  # This could be different looking at the tables separately due to rounding\n","        }\n","        )\n",").execute()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":82,"statement_ids":[82],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:10:38.3714114Z","session_start_time":null,"execution_start_time":"2024-09-19T13:10:38.7908053Z","execution_finish_time":"2024-09-19T13:10:47.0125403Z","parent_msg_id":"e4694b44-044c-45fb-9307-7f131fad4303"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 82, Finished, Available, Finished)"},"metadata":{}}],"execution_count":80,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":true},"id":"12730f91-3044-4f4c-908c-87d657f3ace5"},{"cell_type":"code","source":["displayHTML(f\"\"\"<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n","<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n","<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n","\t,s.login_name\n","\t,r.[session_id]\n","\t,r.start_time\n","\t,r.STATUS\n","\t,r.total_elapsed_time\n","\t,r.command\n","\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n","\t\tWHEN r.[statement_start_offset] > 0\n","\t\t\tTHEN\n","\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n","\t\t\t\tCASE r.[statement_end_offset]\n","\t\t\t\t\tWHEN - 1\n","\t\t\t\t\t\tTHEN\n","\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n","\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n","\t\t\t\t\tELSE\n","\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n","\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n","\t\t\t\t\tEND\n","\t\tELSE\n","\t\t\t/* 1st part of full command is running */\n","\t\t\tCASE r.[statement_end_offset]\n","\t\t\t\tWHEN - 1\n","\t\t\t\t\tTHEN\n","\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n","\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n","\t\t\t\tELSE\n","\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n","\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n","\t\t\t\tEND\n","\t\tEND AS [executing_statement]\n","\t,t.[text] AS [parent_batch]\n","\t,s.[program_name]\n","\t,r.query_hash\n","\t,r.query_plan_hash\n","\t,r.dist_statement_id\n","\t,r.[label]\n","\t,s.client_interface_name\n","\t,r.[sql_handle]\n","\t,c.client_net_address\n","\t,c.connection_id\n","FROM\tsys.dm_exec_requests r\n","CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n","JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n","JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n","JOIN\tsys.databases d ON d.database_id = r.database_id\n","WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n","AND\tr.session_id <> @@SPID\n","AND\ts.program_name NOT IN ('QueryInsights','DMS')\n","</pre></p>\n","\"\"\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":78,"statement_ids":[78],"state":"finished","livy_statement_state":"available","session_id":"e469517b-9eab-489b-8060-1288568c8db5","normalized_state":"finished","queued_time":"2024-09-19T13:01:47.1515387Z","session_start_time":null,"execution_start_time":"2024-09-19T13:09:28.2903637Z","execution_finish_time":"2024-09-19T13:09:28.5330852Z","parent_msg_id":"2f446f92-578e-4c58-acf6-f2e29a31c15e"},"text/plain":"StatementMeta(, e469517b-9eab-489b-8060-1288568c8db5, 78, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n\t,s.login_name\n\t,r.[session_id]\n\t,r.start_time\n\t,r.STATUS\n\t,r.total_elapsed_time\n\t,r.command\n\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n\t\tWHEN r.[statement_start_offset] > 0\n\t\t\tTHEN\n\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n\t\t\t\tCASE r.[statement_end_offset]\n\t\t\t\t\tWHEN - 1\n\t\t\t\t\t\tTHEN\n\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n\t\t\t\t\tELSE\n\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n\t\t\t\t\tEND\n\t\tELSE\n\t\t\t/* 1st part of full command is running */\n\t\t\tCASE r.[statement_end_offset]\n\t\t\t\tWHEN - 1\n\t\t\t\t\tTHEN\n\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n\t\t\t\tELSE\n\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n\t\t\t\tEND\n\t\tEND AS [executing_statement]\n\t,t.[text] AS [parent_batch]\n\t,s.[program_name]\n\t,r.query_hash\n\t,r.query_plan_hash\n\t,r.dist_statement_id\n\t,r.[label]\n\t,s.client_interface_name\n\t,r.[sql_handle]\n\t,c.client_net_address\n\t,c.connection_id\nFROM\tsys.dm_exec_requests r\nCROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\nJOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\nJOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\nJOIN\tsys.databases d ON d.database_id = r.database_id\nWHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\nAND\tr.session_id <> @@SPID\nAND\ts.program_name NOT IN ('QueryInsights','DMS')\n</pre></p>\n"},"metadata":{}}],"execution_count":76,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false},"collapsed":false},"id":"487dfa72-4492-47c5-b5c3-b1fbc595f985"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ea5b5fe-d61b-4356-a234-44112e32e5cc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"cefd0e28-0119-4648-adc8-e8f798807de2","default_lakehouse_name":"LH_QueryResults","default_lakehouse_workspace_id":"93d2465a-1e27-4ac4-8dd7-cf80b0b88331"}}},"nbformat":4,"nbformat_minor":5}