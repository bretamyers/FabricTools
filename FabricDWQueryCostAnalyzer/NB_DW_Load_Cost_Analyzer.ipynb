{"cells":[{"cell_type":"markdown","source":["### Parameters to update\n","##### Make sure notebook is attached to a lakehouse to log results\n","##### Two tables will be created. QueryResults and RunResults"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d94e7d5b-db72-4777-b700-297f8baaaf13"},{"cell_type":"code","source":["FabricDWWorkspaceName = ''\n","FabricDWName = 'WH_SampleData'\n","ConcurrencyNum = 1 # This should be equal or greater than the length of the dataframe with the queryies defined below\n","CapacityMetricsWorkspace = 'Microsoft Fabric Capacity Metrics'\n","CapacityMetricsDataset = 'Fabric Capacity Metrics'"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"20be0399-9430-4b52-a3ab-1c911fc0d69a"},{"cell_type":"code","source":["print(f\"{FabricDWWorkspaceName=}\")\n","print(f\"{FabricDWName=}\")\n","print(f\"{ConcurrencyNum=}\")\n","print(f\"{CapacityMetricsWorkspace=}\")\n","print(f\"{CapacityMetricsDataset=}\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f493545a-4048-4721-a9ef-eeed8e883a28"},{"cell_type":"code","source":["queryList = [\n","    'SELECT COUNT(*) AS Cnt FROM dbo.DimCity'\n","    ,'SELECT COUNT(*) AS Cnt FROM dbo.DimCity'\n","    ,'SELECT COUNT(*) AS Cnt FROM dbo.DimCity'\n","    ,'SELECT COUNT(*) AS Cnt FROM dbo.DimCity'\n","    ,'SELECT COUNT(*) AS Cnt FROM dbo.DimCity'\n","]"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dd4b5a5a-dfa7-43b0-b8a7-b4b3849a7559"},{"cell_type":"code","source":["executorCoreCnt = int(spark.conf.get('spark.executor.cores'))\n","executorInstances = int(spark.conf.get('spark.executor.instances'))\n","maxConcurrency = ConcurrencyNum if ConcurrencyNum < (executorCoreCnt * executorInstances) else (executorCoreCnt * executorInstances)\n","\n","rddQueries = sc.parallelize(queryList, maxConcurrency)\n","rddQueriesWithId = rddQueries.zipWithUniqueId().map(lambda x: [x[1], (x[1], x[0])] )#, tokenstruct, runId]) # zipWithUniqueId is faster than ZipWithIndex but could create gaps in the Ids generated\n","rddQueriesWithId = rddQueriesWithId.partitionBy(maxConcurrency, lambda k: k ) # is this even needed?\n","print(rddQueriesWithId.glom().map(len).collect())  # Get length of each partition to check for even distribution of rows in the partitions. This will tell us if the number of queries are evenly distributed\n","\n","# TODO adjust verbage of this\n","# print(f'Max currency of spark session is {(executorCoreCnt * executorInstances)}\\nDefined concurrency is {concurrencyNum}\\nCan only run {maxConcurrency} queries concurrently for this spark session')\n","displayHTML(f\"\"\"\n","<p><span style=\"font-size:20px;\"><strong>Max currency of spark session is </strong><i><strong>{maxConcurrency}</strong></i></span></p>\n","<p><span style=\"font-size:20px;\"><strong>Defined concurrency is </strong><i><strong>{ConcurrencyNum}</strong></i></span></p>\n","<p><span style=\"font-size:20px;\"><strong>Can only run </strong><i><strong>{maxConcurrency}</strong></i><strong> queries concurrently for this spark session</strong></span></p>\n","\"\"\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de6ed022-d969-45cd-8fd2-93b34357f16f"},{"cell_type":"code","source":["import requests\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n","\n","while True:\n","    workspaceFound = False\n","    for workspace in response.json().get('value'):\n","        if workspace.get('displayName') == FabricDWWorkspaceName:\n","            fabricDWWorkspaceId = workspace.get('id')\n","            workspaceFound = True\n","            break\n","\n","    if workspaceFound == False and response.json().get('continuationToken'):\n","        responseStatus = requests.request(method='get', url=response.json().get('continuationUri'), headers=header)\n","    else:\n","        print(f\"Workspace was not found and no contination token found - {response.json()}\")\n","        break\n","\n","print(f'{fabricDWWorkspaceId = }\\n{FabricDWWorkspaceName = }')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2f726386-0439-4ceb-9f96-b6f562e01135"},{"cell_type":"code","source":["import requests\n","\n","header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n","          ,\"Content-Type\": \"application/json\"\n","          }\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n","workspaceName = response.json().get('displayName')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}', headers=header)\n","capacityId = response.json().get('capacityId')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/capacities', headers=header)\n","for capacity in response.json().get('value'):\n","    if capacity.get('id') == capacityId:\n","        capacityRegion = capacity.get('region')\n","        capacityName = capacity.get('displayName')\n","        capacitySku = capacity.get('sku')\n","\n","response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{fabricDWWorkspaceId}/warehouses', headers=header)\n","warehouse = [warehouse for warehouse in response.json().get('value') if warehouse.get('displayName') == FabricDWName][0]\n","fabricDWServer = warehouse.get('properties').get('connectionString')\n","warehouseId = warehouse.get('id')\n","\n","print(f'{warehouseId = }\\n{fabricDWServer = }\\n{workspaceName = }\\n{capacityId = }\\n{capacityRegion = }\\n{capacityName = }\\n{capacitySku = }')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f1fd34b-c182-47ff-8d87-d8fdd550ad3e"},{"cell_type":"code","source":["response = requests.request(method='get', url=\"https://prices.azure.com/api/retail/prices?$filter=skuName eq 'Fabric Capacity'\", headers=header)\n","for capacity in response.json().get('Items'):\n","    if capacity.get('armRegionName') == capacityRegion.replace(' ', '').lower():\n","        costReserved = capacity.get('retailPrice') / 12 / 730 / 60 / 60 # get the amount per CU second\n","        costPayGo = costReserved / (156.334/262.80) # constant saving of ~41%. 156.334 is the resevered price of a region. 262.80 is the paygo price of a region\n","print(f'{costReserved = :.10f}\\n{costPayGo = :.10f}') # per CU"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6509449a-48a5-4da1-a7c7-43a75ecd7325"},{"cell_type":"markdown","source":["##### Define the queries to be executed. These are single line queries so use /* */ for commenting out code vs --"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7e5bcecd-ac6c-45bf-9ebb-66fe8949ab54"},{"cell_type":"code","source":["from notebookutils import mssparkutils  \n","from pyspark.sql import functions as F\n","from pyspark.sql import Row\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n","import pyodbc, struct, itertools, time, datetime, re, uuid, json\n","\n","connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={fabricDWServer};Database={FabricDWName}'\n","\n","# Use the credentials of the user executing the notebook\n","token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n","encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n","tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n","\n","runId = str(uuid.uuid4())\n","runStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","runStartTimeEpoch = int(runStartDateTimeUTC.timestamp()*1000)\n","\n","print(f'{runId = }\\n{runStartDateTimeUTC = }\\n{runStartTimeEpoch = }')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44a4596a-e954-4ccc-aa49-0d32c1500013"},{"cell_type":"code","source":["from delta.tables import *\n","\n","with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","    with conn.cursor() as cursor:\n","        cursor.execute('''SELECT @@VERSION AS DWVersion\n","                            ,@@SERVERNAME AS ServerGuid\n","                            ,DB_NAME() AS DWName\n","        ''')\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","        cursor.commit()\n","        resultSet = [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","        cursor.execute(f'''SELECT [is_vorder_enabled] AS IsVOrderEnabled, [data_lake_log_publishing_desc] AS DataLakeLogPublishingDesc\n","                            ,[data_lake_log_publishing] AS DataLakeLogPublishing, [create_date] AS DWCreateDate, [compatibility_level] AS CompatibilityLevel\n","                            FROM sys.databases \n","                            WHERE [name] = '{FabricDWName}'\n","            ''')\n","\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","        cursor.commit()\n","        resultSet.extend([dict(zip(resultColumns, [str(col) for col in row])) for row in resultList])\n","\n","        df = spark.createDataFrame([dict(zip(resultColumns, [str(col) for col in row])) for row in resultList])\n","\n","dfRun = (df.withColumn('RunStartDateTimeUTC', F.lit(runStartDateTimeUTC).cast(TimestampType()))\n","            .withColumn('RunStartTimeEpoch', F.lit(runStartTimeEpoch).cast(LongType()))\n","            .withColumn('RunId', F.lit(runId).cast(StringType()))\n","            .withColumn('DWConnectionString', F.lit(fabricDWServer).cast(StringType()))\n","            .withColumn('QueriesExecutedCnt', F.lit(len(queryList)).cast(IntegerType()))\n","            .withColumn('RunConcurrency', F.lit(maxConcurrency).cast(IntegerType()))\n","\n","            .withColumn('DWGuid', F.lit(warehouseId).cast(StringType())) # TODO\n","            .withColumn('WorkspaceName', F.lit(workspaceName).cast(StringType())) # TODO\n","            .withColumn('WorkspaceGuid', F.lit(fabricDWWorkspaceId).cast(StringType())) # TODO\n","            .withColumn('CapacityName', F.lit(capacityName).cast(StringType())) # TODO\n","            .withColumn('CapacityGuid', F.lit(capacityId).cast(StringType())) # TODO\n","            .withColumn('CapacitySKU', F.lit(capacitySku).cast(StringType())) # TODO\n","            .withColumn('CapacityRegion', F.lit(capacityRegion).cast(StringType())) # TODO\n","\n","            .withColumn('RunEndDatetimeUTC', F.lit(None).cast(TimestampType()))\n","            .withColumn('RunEndTimeEpochMS', F.lit(None).cast(LongType()))\n","            .withColumn('RunDurationMS', F.lit(None).cast(LongType()))\n","            .withColumn('RunCUSeconds', F.lit(None).cast(DoubleType()))\n","            .withColumn('RunCostPayGo', F.lit(None).cast(DoubleType()))\n","            .withColumn('RunCostReserved', F.lit(None).cast(DoubleType()))\n","            \n","            .withColumn('CapacityDailyCUSeconds', F.lit(60*60*24*int(capacitySku.split('F')[1])))\n","            .withColumn('CapacityDailyCostPayGo', F.lit(costPayGo * 60*60*24*int(capacitySku.split('F')[1])))\n","            .withColumn('CapacityDailyCostReserved', F.lit(costReserved * 60*60*24*int(capacitySku.split('F')[1])))\n","        )\n","\n","\n","# # TODO add a merge so that if you need to rerun from this step, it doesn't insert a new record\n","dfRun.select(\"RunId\"\n","        ,\"DWConnectionString\"\n","        ,\"QueriesExecutedCnt\"\n","        ,\"RunConcurrency\"\n","        ,\"DWGuid\"\n","        ,\"WorkspaceName\"\n","        ,\"WorkspaceGuid\"\n","        ,\"CapacityName\"\n","        ,\"CapacityGuid\"\n","        ,\"CapacitySKU\"\n","        ,\"CapacityRegion\"\n","        ,\"CompatibilityLevel\"\n","        ,\"DWCreateDate\"\n","        ,\"DataLakeLogPublishing\"\n","        ,\"DataLakeLogPublishingDesc\"\n","        ,\"IsVOrderEnabled\"\n","        ,\"RunStartDateTimeUTC\"\n","        ,\"RunStartTimeEpoch\"\n","        ,\"RunEndDatetimeUTC\"\n","        ,\"RunEndTimeEpochMS\"\n","        ,\"RunDurationMS\"\n","        ,\"RunCUSeconds\"\n","        ,\"RunCostPayGo\"\n","        ,\"RunCostReserved\"\n","        ,\"CapacityDailyCUSeconds\"\n","        ,\"CapacityDailyCostPayGo\"\n","        ,\"CapacityDailyCostReserved\"\n","    )\n","\n","if spark.catalog.tableExists(\"RunResults\"):\n","\n","    dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","\n","    (dtRunResults.alias('t')\n","        .merge(dfRun.alias('s')\n","            ,f't.runId = s.RunId'\n","            )\n","        .whenNotMatchedInsertAll()\n","    ).execute() \n","else:\n","    dfRun.write.format('delta').mode('append').saveAsTable('RunResults')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f1633b8f-665d-4393-a63c-174effbb1ba9"},{"cell_type":"code","source":["from pyspark import SparkContext, SparkConf\n","import pyodbc \n","\n","def get_result_set(cursor):\n","    if cursor.description:\n","        resultList = cursor.fetchall()\n","        resultColumns = columns = [column[0] for column in cursor.description]\n","    else:\n","        resultList = []\n","        resultColumns = []\n","    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n","\n","def execute_query(iterator):\n","    queryMetrics = []\n","    for queryInfo in iterator:\n","        queryId = queryInfo[1][0]\n","        queryStatement = queryInfo[1][1]\n","        with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n","            with conn.cursor() as cursor:\n","                queryStartDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","                startTime = int(round(time.time() * 1000))\n","\n","                cursor.execute(queryStatement)\n","                endTime = int(round(time.time() * 1000))\n","                queryEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","                \n","                queryMessage = str(cursor.messages) if cursor.messages else \"\"\n","                resultSet = get_result_set(cursor)\n","\n","                while cursor.nextset():\n","                    queryMessage += \",\".join([str(cursor.messages) if cursor.messages else \"\"])\n","                    resultSet.append(get_result_set(cursor))\n","                \n","                statementId = ','.join(re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Statement ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n","                queryHash = ','.join(re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage)) if re.findall(r\"Query hash: (0x[A-F0-9]+)\", queryMessage) else \"\"\n","                distributionRequestId = ','.join(re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage)) if re.findall(r\"Distributed request ID: \\{([A-F0-9\\-]+)\\}\", queryMessage) else \"\"\n","                resultSetJsonString = json.dumps(resultSet)\n","\n","                cursor.commit()\n","\n","                queryId = str(uuid.uuid4())\n","                queryMetrics.append([runId, queryId, queryStatement, queryStartDateTimeUTC, queryEndDateTimeUTC\n","                        ,queryMessage, startTime, endTime, endTime - startTime\n","                        ,statementId, queryHash, distributionRequestId, resultSetJsonString\n","                        ])\n","    return queryMetrics\n","\n","queriesExecuted = rddQueriesWithId.mapPartitions(execute_query)\n","\n","dfQueriesExecuted = queriesExecuted.toDF(schema=StructType([\n","    StructField(\"RunId\", StringType(), False),\n","    StructField(\"QueryId\", StringType(), False),\n","    StructField(\"QueryStatement\", StringType(), False),\n","    StructField(\"QueryStartDateTimeUTC\", TimestampType(), False),\n","    StructField(\"QueryEndDateTimeUTC\", TimestampType(), False),\n","    StructField(\"ReturnMessage\", StringType(), False),\n","    StructField(\"QueryStartTimeEpochMS\", LongType(), False),\n","    StructField(\"QueryEndTimeEpochMS\", LongType(), False),\n","    StructField(\"QueryDurationMS\", LongType(), False),\n","    StructField(\"StatementId\", StringType(), False),\n","    StructField(\"QueryHash\", StringType(), False),\n","    StructField(\"DistributionRequestId\", StringType(), False),\n","    StructField(\"ResultSet\", StringType(), False)\n","    ]))\n","\n","dfFinal = dfQueriesExecuted.withColumn('QueryCUSeconds', F.lit(None).cast(DoubleType())).withColumn('QueryCostPayGo', F.lit(None).cast(DoubleType())).withColumn('QueryCostReserved', F.lit(None).cast(DoubleType()))\n","dfFinal.write.format('delta').mode('append').saveAsTable('QueryResults')\n","\n","runEndDateTimeUTC = datetime.datetime.now(datetime.timezone.utc)\n","runEndTimeEpoch = int(runEndDateTimeUTC.timestamp()*1000)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"a7a25284-d19a-4120-b2bf-9326efd16033"},{"cell_type":"code","source":["statementList = spark.sql(f'SELECT ARRAY_JOIN(COLLECT_SET(CONCAT(\"\\\\\"\", StatementId, \"\\\\\"\")), \", \") AS Statements FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('Statements')\n","# We have to explode by statement ids since a sql query may have multiple queries within it\n","queriesExecutedCnt = spark.sql(f'SELECT COUNT(StatementId) AS QueryCnt FROM (SELECT EXPLODE(SPLIT(StatementId, \",\")) AS StatementId FROM QueryResults WHERE runId = \"{runId}\") AS a ').collect()[0].asDict().get('QueryCnt') \n","print(f'{runId = }\\n{statementList = }\\n{queriesExecutedCnt = }')"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"fdf916df-8027-4011-a8ec-dd3fedc1c2b2"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","\n","df_final = spark.createDataFrame(data=[(runEndDateTimeUTC, runEndTimeEpoch, )], schema=['RunEndDateTimeUTC', 'RunEndTimeEpochMS'])\n","\n","(dtRunResults.alias('t')\n","    .merge(df_final.alias('s')\n","        ,f't.RunId = \"{runId}\"'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'RunEndDatetimeUTC': 's.RunEndDateTimeUTC'\n","        ,'RunEndTimeEpochMS': 's.RunEndTimeEpochMS'\n","        ,'RunDurationMS': 's.RunEndTimeEpochMS - t.RunStartTimeEpoch'\n","        }\n","        )\n",").execute() "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d44baaf-dc59-433f-8b7e-c5a2a124f3fe"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from datetime import timedelta\n","from pyspark.sql.functions import col, lit, sum, min, max\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, DoubleType\n","\n","def get_capacity_metrics_usage(time_point, operation_id_list):\n","\t\n","\tschema = StructType([\n","\t\tStructField(\"Items[ItemId]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"Items[ItemKind]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"Items[ItemName]\", \tStringType(), \t\tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationStartTime]\", \tTimestampType(), \tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationEndTime]\", \t\tTimestampType(), \tTrue),\n","\t\tStructField(\"TimePointBackgroundDetail[OperationId]\", \t\t\tStringType(), \t\tTrue),\n","\t\tStructField(\"Sum_CUs\", \t\tDoubleType(), \t\tTrue),\n","\t\tStructField(\"Sum_Duration\", IntegerType(), \t\tTrue)\n","\t])\n","\n","\tdax_command = f\"\"\"\n","\tDEFINE\n","\t\tMPARAMETER 'CapacityID' \t= \"{capacityId}\"\n","\t\tMPARAMETER 'TimePoint' \t\t= (DATE({time_point.year}, {time_point.month}, {time_point.day}) + TIME({time_point.hour}, {time_point.minute}, {time_point.second}))\n","\n","\t\tVAR __Var_CapacityId\t= {{\"{capacityId}\"}}\n","\t\tVAR __Var_OperationId\t= {{{statementList}}}\n","\n","\t\tVAR __Filter_OperationId \t= TREATAS(__Var_OperationId, 'TimePointBackgroundDetail'[OperationId])\n","\t\tVAR __Filter_CapacityId \t= TREATAS(__Var_CapacityId, 'Capacities'[capacityId])\n","\n","\t\tVAR OperationCUs = \n","\t\t\tSUMMARIZECOLUMNS(\n","\t\t\t\t'Items'[ItemId],\n","\t\t\t\t'Items'[ItemKind],\n","\t\t\t\t'Items'[ItemName],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationStartTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationEndTime],\n","\t\t\t\t'TimePointBackgroundDetail'[OperationId],\n","\t\t\t\t__Filter_OperationId,\n","\t\t\t\t__Filter_CapacityId,\n","\t\t\t\t\"Sum_CUs\", CALCULATE(SUM('TimePointBackgroundDetail'[Total CU (s)])),\n","\t\t\t\t\"Sum_Duration\", CALCULATE(SUM('TimePointBackgroundDetail'[Duration (s)]))\n","\t\t\t)\n","\n","\tEVALUATE\n","\t\tOperationCUs\n","\t\"\"\"\n","\n","\tdf_dax = fabric.evaluate_dax(dax_string = dax_command, dataset = CapacityMetricsDataset, workspace = CapacityMetricsWorkspace)\n","\n","\tdf = (spark.createDataFrame(data=df_dax, schema=schema).withColumn(\"TimePoint\", lit(time_point)).select(\n","\t\t\tcol(\"TimePoint\")\n","\t\t\t,col(\"Items[ItemId]\").alias(\"ItemId\")\n","\t\t\t,col(\"Items[ItemKind]\").alias(\"ItemKind\")\n","\t\t\t,col(\"Items[ItemName]\").alias(\"ItemName\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationStartTime]\").alias(\"StartTime\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationEndTime]\").alias(\"EndTime\")\n","\t\t\t,col(\"TimePointBackgroundDetail[OperationId]\").alias(\"OperationId\")\n","\t\t\t,col(\"Sum_CUs\").cast(DoubleType()).alias(\"Sum_CUs\")\n","\t\t\t,col(\"Sum_Duration\").cast(IntegerType()).alias(\"Sum_Duration\"))\n","\t\t)\n","\n","\treturn df"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b09fd036-6768-4fe9-b1f9-fea79ed5e7b0"},{"cell_type":"code","source":["'''\n","Get the capacity usage for the queries that were executed.\n","This ensures that the background operations are captured no matter what time they are run as they are smoothed over a 24 hour time period.\n","Next, filter that down to the distinct records. This is necessary because a record may show up in the today and tomorrow datasets depending on the time it was run.\n","Nest, aggregate the records into a single record. This is necessary becuase some operations will have two entries, one under the executing user and one under the user \"System\".\n","'''\n","\n","# # Continues to query the metrica app to get the data. Data can delayed by a few minutes.\n","# # We retry every minute until 15 minutes has passed.\n","for retryCnt in range(15):\n","    df_today    = get_capacity_metrics_usage(runStartDateTimeUTC, statementList)\n","    df_tomorrow = get_capacity_metrics_usage(runStartDateTimeUTC + timedelta(hours = 24), statementList)\n","    df_all_days = df_today.unionAll(df_tomorrow)\n","    df_distinct = df_all_days.select('ItemId', 'ItemKind', 'ItemName', 'StartTime', 'EndTime', 'OperationId', 'Sum_CUs', 'Sum_Duration').distinct()\n","    df_final    = df_distinct.groupBy('ItemId', 'ItemKind', 'ItemName', 'OperationId').agg(min(\"StartTime\").alias(\"StartTime\"), max(\"EndTime\").alias(\"EndTime\"), sum(\"Sum_CUs\").alias(\"QueryCUSeconds\"), sum(\"Sum_Duration\").alias(\"SumDuration\"))\n","\n","    print(f'{df_final.count()} queries of the {queriesExecutedCnt} that have been found in the capacity metrics model. ', end='')\n","    if df_final.count() == queriesExecutedCnt:\n","        break\n","    else:\n","        print('Sleeping for a minute...')\n","        time.sleep(60)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"9d68bfa2-1f97-47af-a21c-230459507da7"},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n","\n","dfQueryResults = spark.table(\"QueryResults\")\n","\n","dfQueryResultsCleansed = (dfQueryResults.join(df_final, dfQueryResults.StatementId.contains(df_final.OperationId)) \n","    .filter(dfQueryResults.RunId == runId) \n","    .groupBy(dfQueryResults.RunId, dfQueryResults.StatementId) \n","    .agg(sum(df_final.QueryCUSeconds).alias(\"QueryCUSeconds\"), sum(df_final.SumDuration).alias(\"SumDuration\")\n","    ))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"f9ffd41b-12e4-4c5a-aeae-0dccf30add3e"},{"cell_type":"markdown","source":["##### Update the QueryResults table with the CUSeconds and QueryCost derived from the Capacity Metrics App"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"13aa53bc-c865-491d-881a-3206e8cfbfad"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtQueryResults = DeltaTable.forName(spark, \"QueryResults\")\n","\n","(dtQueryResults.alias('t')\n","    .merge(dfQueryResultsCleansed.alias('s')\n","        ,f't.runId = s.RunId AND t.StatementId = s.StatementId'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'QueryCUSeconds': 's.QueryCUSeconds'\n","        ,'QueryCostPayGo': f's.QueryCUSeconds * {costPayGo}'\n","        ,'QueryCostReserved': f's.QueryCUSeconds * {costReserved}'\n","        }\n","        )\n",").execute() "],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b93c6bdf-7984-489a-b3bf-c3de6080aa60"},{"cell_type":"markdown","source":["##### Update the RunResults table with cost of run"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8343c8b6-52c9-414d-8bf7-962982409336"},{"cell_type":"code","source":["from delta.tables import *\n","\n","dtRunResults = DeltaTable.forName(spark, \"RunResults\")\n","dtRunResultsCleansed = spark.sql(f'SELECT SUM(COALESCE(QueryCUSeconds, 0)) AS RunCUSeconds, SUM(COALESCE(QueryCostPayGo, 0)) AS RunCostPayGo, SUM(COALESCE(QueryCostReserved, 0)) AS RunCostReserved FROM QueryResults WHERE RunId = \"{runId}\"')\n","\n","(dtRunResults.alias('t')\n","    .merge(dtRunResultsCleansed.alias('s')\n","        ,f't.RunId = \"{runId}\"'\n","        )\n","    .whenMatchedUpdate(set=\n","        {'RunCUSeconds': 's.RunCUSeconds'\n","        ,'RunCostPayGo': f's.RunCUSeconds * {costPayGo}'  # This could be different looking at the tables separately due to rounding\n","        ,'RunCostReserved': f's.RunCUSeconds * {costReserved}'  # This could be different looking at the tables separately due to rounding\n","        }\n","        )\n",").execute()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":true},"id":"12730f91-3044-4f4c-908c-87d657f3ace5"},{"cell_type":"code","source":["displayHTML(f\"\"\"<script src=\"https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js\"></script>\n","<p style=\"margin-bottom:0\"><span style=\"font-size:20px;\"><strong>/*<br>Reference T-SQL - </strong></span><span style=\"font-size:20px;\"><strong>See running sql statements on the DW. Used to verify query(s) are executing and that the concurrency is working correctly.<br>*/</strong></span></p>\n","<pre class=\"prettyprint\"><p style=\"margin-top:0;\">SELECT\td.name AS 'database_name'\n","\t,s.login_name\n","\t,r.[session_id]\n","\t,r.start_time\n","\t,r.STATUS\n","\t,r.total_elapsed_time\n","\t,r.command\n","\t,CASE /* Uses statement start and end offset to figure out what statement is running */\n","\t\tWHEN r.[statement_start_offset] > 0\n","\t\t\tTHEN\n","\t\t\t\t/* The start of the active command is not at the beginning of the full command text */\n","\t\t\t\tCASE r.[statement_end_offset]\n","\t\t\t\t\tWHEN - 1\n","\t\t\t\t\t\tTHEN\n","\t\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n","\t\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, 2147483647)\n","\t\t\t\t\tELSE\n","\t\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n","\t\t\t\t\t\tSUBSTRING(t.TEXT, (r.[statement_start_offset] / 2) + 1, (r.[statement_end_offset] - r.[statement_start_offset]) / 2)\n","\t\t\t\t\tEND\n","\t\tELSE\n","\t\t\t/* 1st part of full command is running */\n","\t\t\tCASE r.[statement_end_offset]\n","\t\t\t\tWHEN - 1\n","\t\t\t\t\tTHEN\n","\t\t\t\t\t\t/* The end of the full command is also the end of the active statement */\n","\t\t\t\t\t\tRTRIM(LTRIM(t.[text]))\n","\t\t\t\tELSE\n","\t\t\t\t\t/* The end of the active statement is not at the end of the full command */\n","\t\t\t\t\tLEFT(t.TEXT, (r.[statement_end_offset] / 2) + 1)\n","\t\t\t\tEND\n","\t\tEND AS [executing_statement]\n","\t,t.[text] AS [parent_batch]\n","\t,s.[program_name]\n","\t,r.query_hash\n","\t,r.query_plan_hash\n","\t,r.dist_statement_id\n","\t,r.[label]\n","\t,s.client_interface_name\n","\t,r.[sql_handle]\n","\t,c.client_net_address\n","\t,c.connection_id\n","FROM\tsys.dm_exec_requests r\n","CROSS APPLY sys.[dm_exec_sql_text](r.[sql_handle]) t\n","JOIN\tsys.dm_exec_sessions s ON r.session_id = s.session_id\n","JOIN\tsys.dm_exec_connections c ON s.session_id = c.session_id\n","JOIN\tsys.databases d ON d.database_id = r.database_id\n","WHERE\tr.dist_statement_id != '00000000-0000-0000-0000-000000000000'\n","AND\tr.session_id <> @@SPID\n","AND\ts.program_name NOT IN ('QueryInsights','DMS')\n","</pre></p>\n","\"\"\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":true,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false},"collapsed":false},"id":"487dfa72-4492-47c5-b5c3-b1fbc595f985"},{"cell_type":"code","source":[],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3ea5b5fe-d61b-4356-a234-44112e32e5cc"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}