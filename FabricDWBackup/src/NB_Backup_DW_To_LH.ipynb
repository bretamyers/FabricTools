{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33a5c338-f911-4a80-8561-522cc4d81c82",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Parameters\n",
    "- workspaceName - The name of the workspace where the source warehouse exists.\n",
    "- warehouseName - The name of the warehouse that is to be backed up.\n",
    "- workspaceBackupName - The name of the workspace that the lakehouse for backups exists. This defaults to the workspace where this notebook exists.\n",
    "- lakehouseBackupName - The name of the lakehouse used for back ups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97583691-a2b7-4e6a-8fcb-2223b1f1e888",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "workspaceName = 'WS_Demo_InternetSales'\n",
    "warehouseName = 'WH_InternetSales'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f0a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaceBackupName = spark.conf.get('trident.workspace.id')\n",
    "lakehouseBackupsName = 'LH_DW_Backups'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa226348-68ad-41f9-bd59-37b09898dd90",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Copy the data from the Warehouse delta folders a separate Lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba470f-b040-4823-ae64-c4110069baee",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from notebookutils import mssparkutils\n",
    "\n",
    "def deep_ls(path: str, max_depth=1):\n",
    "    \"\"\"\n",
    "    https://www.rakirahman.me/directory-recursion-synapse/\n",
    "\n",
    "    List all files and folders in specified path and\n",
    "    subfolders within maximum recursion depth.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in path\n",
    "    li = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Return all files\n",
    "    for x in li:\n",
    "        if x.size != 0:\n",
    "            yield x\n",
    "\n",
    "    # If the max_depth has not been reached, start\n",
    "    # listing files and folders in subdirectories\n",
    "    if max_depth > 1:\n",
    "        for x in li:\n",
    "            if x.size != 0:\n",
    "                continue\n",
    "            for y in deep_ls(x.path, max_depth - 1):\n",
    "                yield y\n",
    "\n",
    "    # If max_depth has been reached,\n",
    "    # return the folders\n",
    "    else:\n",
    "        for x in li:\n",
    "            if x.size == 0:\n",
    "                yield x\n",
    "\n",
    "\n",
    "fileList = deep_ls(f'abfss://{workspaceName}@onelake.dfs.fabric.microsoft.com/{warehouseName}.datawarehouse/Tables', max_depth=2)\n",
    "\n",
    "nowDatetime = datetime.datetime.now(datetime.timezone.utc)\n",
    "backupDatetime = nowDatetime.strftime(\"%Y%m%d_%H%M\")\n",
    "print(f'{workspaceName = }')\n",
    "print(f'{backupDatetime = }')\n",
    "\n",
    "# https://blog.fabric.microsoft.com/en-us/blog/optimizing-spark-compute-for-medallion-architectures-in-microsoft-fabric?ft=Santhosh%20Kumar%20Ravindran:author\n",
    "spark.conf.set(\"spark.sql.parquet.vorder.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\",\"false\")\n",
    "spark.conf.set(\"spark.databricks.delta.collect.stats\",\"false\")\n",
    "\n",
    "for file in fileList:\n",
    "    schema = file.path.split('/')[-2]\n",
    "    table = file.name\n",
    "    print(f'{schema = }, {table = }')\n",
    "    df = spark.read.format('delta').load(file.path)\n",
    "    df.write.mode('overwrite').format('delta').save(f'abfss://{workspaceBackupName}@onelake.dfs.fabric.microsoft.com/{lakehouseBackupsName}.Lakehouse/Files/{workspaceName}/{warehouseName}/{backupDatetime}/{schema}/{table}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2e5e39-7d1e-4f58-8d97-1bb6b4397129",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Drop old backups - backupRetentionCnt is set to 5 by default meaning it will keep the last 5 backups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e0028a-fe93-4dba-aa85-437525c21a18",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "backupList = mssparkutils.fs.ls(f'abfss://{workspaceBackupName}@onelake.dfs.fabric.microsoft.com/{lakehouseBackupsName}.Lakehouse/Files/{workspaceName}/{warehouseName}')\n",
    "backupRetentionCnt = 5\n",
    "\n",
    "for i, file in enumerate(backupList):\n",
    "    if len(backupList) - i > backupRetentionCnt:\n",
    "        print(f'Removing directory \"{file.path}\"')\n",
    "        mssparkutils.fs.rm(file.path, recurse=True)\n",
    "    else:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "5f07e922-7e5c-41d0-8174-c7694efae0b1",
    "default_lakehouse_name": "LH_DW_Backups",
    "default_lakehouse_workspace_id": "59664bd4-4ae0-4217-a188-bcb3d6abf770",
    "known_lakehouses": [
     {
      "id": "5f07e922-7e5c-41d0-8174-c7694efae0b1"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
