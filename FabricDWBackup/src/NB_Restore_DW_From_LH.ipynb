{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa8f81a1-aa1a-45b6-8749-56f987d66899",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Parameters\n",
    "- workspaceName - The name of the workspace where the source warehouse exists.\n",
    "- warehouseName - The name of the warehouse that is to be backed up.\n",
    "- workspaceBackupName - The name of the workspace that the lakehouse for backups exists. This defaults to the workspace where this notebook exists.\n",
    "- lakehouseBackupName - The name of the lakehouse used for back ups.\n",
    "- backupDateTime - The datetime that the backup took place. Get this value from the backup lakehouse folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f0370d-59c9-469e-85b9-4f4ca9bebc79",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "workspaceName = 'WS_Demo_InternetSales'\n",
    "warehouseName = 'WH_InternetSales'\n",
    "\n",
    "backupDatetime = '20241210_1450'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "workspaceBackupName = spark.conf.get('trident.artifact.workspace.id')\n",
    "lakehouseBackupName = 'LH_DW_Backups'\n",
    "print(f'{workspaceBackupId =}\\n{lakehouseBackupName =}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e8a545",
   "metadata": {},
   "source": [
    "##### Get the workspace name for backups name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eababfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceBackupId}', headers=header)\n",
    "workspaceBackupName = response.json().get('displayName')\n",
    "print(f'{workspaceBackupName =}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e9aac9",
   "metadata": {},
   "source": [
    "##### Create a list of all the tables that need to be restored based on what was backed up for the given backup datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ba470f-b040-4823-ae64-c4110069baee",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparksession\n",
    "\n",
    "def deep_ls(path: str, max_depth=1):\n",
    "    \"\"\"\n",
    "    https://www.rakirahman.me/directory-recursion-synapse/\n",
    "\n",
    "    List all files and folders in specified path and\n",
    "    subfolders within maximum recursion depth.\n",
    "    \"\"\"\n",
    "\n",
    "    # List all files in path\n",
    "    li = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Return all files\n",
    "    for x in li:\n",
    "        if x.size != 0:\n",
    "            yield x\n",
    "\n",
    "    # If the max_depth has not been reached, start\n",
    "    # listing files and folders in subdirectories\n",
    "    if max_depth > 1:\n",
    "        for x in li:\n",
    "            if x.size != 0:\n",
    "                continue\n",
    "            for y in deep_ls(x.path, max_depth - 1):\n",
    "                yield y\n",
    "\n",
    "    # If max_depth has been reached,\n",
    "    # return the folders\n",
    "    else:\n",
    "        for x in li:\n",
    "            if x.size == 0:\n",
    "                yield x\n",
    "\n",
    "fileList = deep_ls(f'abfss://{workspaceBackupName}@onelake.dfs.fabric.microsoft.com/{lakehouseBackupName}.Lakehouse/Files/{workspaceName}/{warehouseName}/{backupDatetime}', max_depth=2)\n",
    "\n",
    "tableList = []\n",
    "for file in fileList:\n",
    "    schema = file.path.split('/')[-2]\n",
    "    table = file.name\n",
    "    print(f'{schema = }, {table = }')\n",
    "    tableList.append({\"schema\": schema, \"table\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c17642",
   "metadata": {},
   "source": [
    "##### Get the workapce id that the warehouse exists in and get the workspace id of the lakehouse that contains the backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf0f66-0fe1-48f5-8c91-0a4b1ee04be2",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces', headers=header)\n",
    "for workspace in response.json().get('value'):\n",
    "  if workspace.get('displayName') == workspaceName:\n",
    "    workspaceId = workspace.get('id')\n",
    "  elif workspace.get('displayName') == workspaceBackupName:\n",
    "    workspaceBackupId = workspace.get('id')\n",
    "\n",
    "print(f'{workspaceId = }\\n{workspaceBackupId = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6147b2-ce0f-44fe-86d2-cb971ffd71c1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Create temporary restore lakehouse in the workspace where the warehouse exists. \"LH_temp_restore_{warehouseName}_{backupDateTime}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9435abd2-c388-4bbc-9ca1-0b2b56c109c7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "lakehouseTempRestoreName = f\"LH_temp_restore_{warehouseName}_{backupDatetime}\"\n",
    "\n",
    "body = {\n",
    "    \"displayName\": lakehouseTempRestoreName\n",
    "}\n",
    "\n",
    "response = requests.request(method='post', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses', headers=header, data=json.dumps(body))\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8480615-7f66-4d53-9660-6c6d68be28fb",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Get the lakehouse ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a63db3-a1a3-442d-84f8-8625fa932345",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import requests, json\n",
    "\n",
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceBackupId}/lakehouses', headers=header)\n",
    "for item in response.json().get('value'):\n",
    "  if item.get('displayName') == lakehouseBackupName:\n",
    "    lakehouseBackupId = item.get('id')\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses', headers=header)\n",
    "for item in response.json().get('value'):\n",
    "    lakehouseTempRestoreId = item.get('id')\n",
    "\n",
    "print(f'{lakehouseBackupId = }\\n{lakehouseTempRestoreId = }')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26c795",
   "metadata": {},
   "source": [
    "##### Creates a OneLake table shortcut for each warehouse table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8bbd05-5657-4b5b-8141-ceb88e3ad366",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils\n",
    "import requests, json\n",
    "\n",
    "\n",
    "workspaceIdTarget = workspaceId\n",
    "itemIdTarget = lakehouseTempRestoreId\n",
    "\n",
    "workspaceIdSource = workspaceBackupId\n",
    "itemIdSource = lakehouseBackupId\n",
    "\n",
    "\n",
    "url = f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceIdTarget}/items/{itemIdTarget}/shortcuts' #?shortcutConflictPolicy=GenerateUniqueName'\n",
    "\n",
    "for table in tableList:\n",
    "    body = {\n",
    "        \"name\": f\"{table.get('schema')}_{table.get('table')}\"\n",
    "        ,\"path\": \"Tables\"\n",
    "        ,\"target\": {\n",
    "            \"oneLake\": {\n",
    "                \"itemId\": itemIdSource\n",
    "                ,\"path\": f\"Files/{workspaceName}/{warehouseName}/{backupDatetime}/{table.get('schema')}/{table.get('table')}\"\n",
    "                ,\"workspaceId\": workspaceIdSource\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.request(method='post', url=url, headers=header, data=json.dumps(body))\n",
    "    print(response.status_code)\n",
    "    print(f'{response.status_code} - {response.json()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a8f0f5",
   "metadata": {},
   "source": [
    "##### Get the warehouse id and sql endpoint connection string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735c0ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "response = requests.request(method='get', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/warehouses', headers=header)\n",
    "\n",
    "warehouseId, sqlEndpoint = [[warehouse.get('id'), warehouse.get('properties').get('connectionString')] for warehouse in response.json().get('value') if warehouse.get('displayName') == warehouseName][0]\n",
    "print(warehouseId, sqlEndpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4734c91f",
   "metadata": {},
   "source": [
    "##### Executes a sql query on the warehouse that truncates and inserts data for each table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a792a49-eb5e-4f00-95b8-70d1ae25c8ce",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pyodbc, struct, itertools, time, datetime, re, uuid, json\n",
    "\n",
    "fabricDWServer = sqlEndpoint\n",
    "\n",
    "connectionString = f'DRIVER={{ODBC Driver 18 for SQL Server}};SERVER={fabricDWServer};Database={workspaceName}'\n",
    "\n",
    "# Use the credentials of the user executing the notebook\n",
    "token = bytes(mssparkutils.credentials.getToken('pbi'), \"UTF-8\")\n",
    "encoded_bytes = bytes(itertools.chain.from_iterable(zip(token, itertools.repeat(0))))\n",
    "tokenstruct = struct.pack(\"<i\", len(encoded_bytes)) + encoded_bytes\n",
    "\n",
    "def get_result_set(cursor):\n",
    "    if cursor.description:\n",
    "        resultList = cursor.fetchall()\n",
    "        resultColumns = columns = [column[0] for column in cursor.description]\n",
    "    else:\n",
    "        resultList = []\n",
    "        resultColumns = []\n",
    "    return [dict(zip(resultColumns, [str(col) for col in row])) for row in resultList]\n",
    "\n",
    "with pyodbc.connect(connectionString, attrs_before = { 1256:tokenstruct }) as conn:\n",
    "    for table in tableList:\n",
    "        with conn.cursor() as cursor:\n",
    "            queryStatement = f'TRUNCATE TABLE {warehouseName}.{table.get(\"schema\")}.{table.get(\"table\")}; INSERT INTO {warehouseName}.{table.get(\"schema\")}.{table.get(\"table\")} SELECT * FROM {lakehouseTempRestoreName}.dbo.{table.get(\"schema\")}_{table.get(\"table\")}'\n",
    "            print(queryStatement)\n",
    "\n",
    "            cursor.execute(queryStatement)\n",
    "            \n",
    "            queryMessage = str(cursor.messages) if cursor.messages else \"\"\n",
    "            print(queryMessage)\n",
    "            \n",
    "            resultSet = get_result_set(cursor)\n",
    "            print(resultSet)\n",
    "\n",
    "            cursor.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d960af-e016-4c90-8210-9d5471a027d5",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Drop the temporary lakehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8309ef-656c-4b51-b505-45d2c9da43e9",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "header = {'Authorization': f'Bearer {mssparkutils.credentials.getToken(\"pbi\")}'\n",
    "          ,\"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "response = requests.request(method='delete', url=f'https://api.fabric.microsoft.com/v1/workspaces/{workspaceId}/lakehouses/{lakehouseTempRestoreId}', headers=header)\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43b6242-876f-479f-8caa-4716ebafc4be",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {}
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
